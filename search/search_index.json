{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p> Try out Evaluations - Read Docs - Slack Community - Feature Request </p> <p>UpTrain is an open-source tool to evaluate LLM applications. UpTrain provides pre-built metrics to check LLM responses on aspects such as correctness, hallucination, toxicity, etc. as well as provides an easy-to-use framework to configure custom checks.</p>"},{"location":"#pre-built-evaluations-we-offer","title":"Pre-built Evaluations We Offer \ud83d\udcdd","text":""},{"location":"#evaluate-the-quality-of-your-responses","title":"Evaluate the quality of your responses:","text":"<ol> <li>Response Completeness: Grades how if the response completely resolves the given user query.</li> <li>Response Relevance: Grades how relevant the generated response is for the given question.</li> <li>Response Conciseness: Grades how concise the generated response is i.e. the extent of additional irrelevant information in the response.</li> <li>Response Matching: Operator to compare the llm-generated text with the gold (ideal) response using the defined score metric.</li> <li>Response Consistency: Grades how consistent the response is with the question asked as well as with the context provided.</li> </ol>"},{"location":"#evaluate-the-quality-of-retrieved-context-and-response-groundedness","title":"Evaluate the quality of retrieved context and response groundedness:","text":"<ol> <li>Factual Accuracy: Checks if the facts present in the response can be verified by the retrieved context</li> <li>Response Completeness wrt Context: Grades how complete the response was for the question specified with respect to the information present in the context</li> <li>Context Relevance: Evaluates if the retrieved context contain sufficient information to answer the given question</li> </ol>"},{"location":"#evaluations-to-safeguard-system-prompts-and-avoid-llm-mis-use","title":"Evaluations to safeguard system prompts and avoid LLM mis-use:","text":"<ol> <li>Prompt Injection: Identifys prompt leakage attacks</li> </ol>"},{"location":"#evaluate-the-language-quality-of-the-response","title":"Evaluate the language quality of the response:","text":"<ol> <li>Tone Critique: Assesses if the tone of machine-generated responses matches with the desired persona.</li> <li>Language Critique: Scores machine generated responses on multiple aspects - fluence, politeness, grammar, and coherence.</li> </ol>"},{"location":"#defining-custom-evaluations-and-others","title":"Defining custom evaluations and others:","text":"<ol> <li>Guideline Adherence: Grades how well the LLM adheres to a given custom guideline.</li> <li>Custom Prompt Evaluation: Evaluate by defining your custom grading prompt.</li> <li>Cosine Similarity: Calculate cosine similarity between embeddings of two texts.</li> </ol>"},{"location":"#evaluate-the-conversation-as-a-whole","title":"Evaluate the conversation as a whole:","text":"<ol> <li>Conversation Satisfaction: Measures the user\u2019s satisfaction with the conversation with the LLM/AI assistant based on completeness and user\u2019s acceptance.</li> </ol>"},{"location":"#get-started","title":"Get started \ud83d\ude4c","text":""},{"location":"#install-the-package-through-pip","title":"Install the package through pip:","text":"<pre><code>pip install uptrain\n</code></pre>"},{"location":"#how-to-use-uptrain","title":"How to use UpTrain:","text":"<p>There are two ways to use UpTrain: 1. Open-source framework: You can evaluate your responses via the open-source version by providing your OpenAI API key to run evaluations. UpTrain leverages a pipeline comprising GPT-3.5 calls for the same. Note that the evaluation pipeline runs on UpTrain's server but none of the data is logged.</p> <ol> <li>UpTrain API: You can use UpTrain's managed service to log and evaluate your LLM responses. Just provide your UpTrain API key (no need for OpenAI keys) and UpTrain manages running evaluations for you with real-time dashboards and deep insights.</li> </ol>"},{"location":"#open-source-framework","title":"Open-source framework:","text":"<p>Follow the code snippet below to get started with UpTrain.</p> <pre><code>from uptrain import EvalLLM, Evals\nimport json\n\nOPENAI_API_KEY = \"sk-***************\"\n\ndata = [{\n    'question': 'Which is the most popular global sport?',\n    'context': \"The popularity of sports can be measured in various ways, including TV viewership, social media presence, number of participants, and economic impact. Football is undoubtedly the world's most popular sport with major events like the FIFA World Cup and sports personalities like Ronaldo and Messi, drawing a followership of more than 4 billion people. Cricket is particularly popular in countries like India, Pakistan, Australia, and England. The ICC Cricket World Cup and Indian Premier League (IPL) have substantial viewership. The NBA has made basketball popular worldwide, especially in countries like the USA, Canada, China, and the Philippines. Major tennis tournaments like Wimbledon, the US Open, French Open, and Australian Open have large global audiences. Players like Roger Federer, Serena Williams, and Rafael Nadal have boosted the sport's popularity. Field Hockey is very popular in countries like India, Netherlands, and Australia. It has a considerable following in many parts of the world.\",\n    'response': 'Football is the most popular sport with around 4 billion followers worldwide'\n}]\n\neval_llm = EvalLLM(openai_api_key=OPENAI_API_KEY)\n\nresults = eval_llm.evaluate(\n    data=data,\n    checks=[Evals.CONTEXT_RELEVANCE, Evals.FACTUAL_ACCURACY, Evals.RESPONSE_COMPLETENESS]\n)\n\nprint(json.dumps(results, indent=3))\n</code></pre> <p>If you have any questions, please join our Slack community</p>"},{"location":"#uptrain-api","title":"UpTrain API:","text":"<ol> <li> <p>Get your free UpTrain API Key here.</p> </li> <li> <p>Follow the code snippets below to get started with UpTrain.</p> </li> </ol> <pre><code>from uptrain import APIClient, Evals\nimport json\n\nUPTRAIN_API_KEY = \"up-***************\" \n\ndata = [{\n    'question': 'Which is the most popular global sport?',\n    'context': \"The popularity of sports can be measured in various ways, including TV viewership, social media presence, number of participants, and economic impact. Football is undoubtedly the world's most popular sport with major events like the FIFA World Cup and sports personalities like Ronaldo and Messi, drawing a followership of more than 4 billion people. Cricket is particularly popular in countries like India, Pakistan, Australia, and England. The ICC Cricket World Cup and Indian Premier League (IPL) have substantial viewership. The NBA has made basketball popular worldwide, especially in countries like the USA, Canada, China, and the Philippines. Major tennis tournaments like Wimbledon, the US Open, French Open, and Australian Open have large global audiences. Players like Roger Federer, Serena Williams, and Rafael Nadal have boosted the sport's popularity. Field Hockey is very popular in countries like India, Netherlands, and Australia. It has a considerable following in many parts of the world.\",\n    'response': 'Football is the most popular sport with around 4 billion followers worldwide'\n}]\n\nclient = APIClient(uptrain_api_key=UPTRAIN_API_KEY)\n\nresults = client.log_and_evaluate(\n    project_name=\"Sample-Project\",\n    data=data,\n    checks=[Evals.CONTEXT_RELEVANCE, Evals.FACTUAL_ACCURACY, Evals.RESPONSE_COMPLETENESS]\n)\n\nprint(json.dumps(results, indent=3))\n</code></pre> <p>To have a customized onboarding, please book a demo call here.</p>"},{"location":"#performing-experiments-with-uptrain","title":"Performing experiments with UpTrain:","text":"<p>Experiments help you perform A/B testing with prompts, so you can compare and choose the options most suitable for you. </p> <pre><code>from uptrain import APIClient, Evals\nimport json\n\nUPTRAIN_API_KEY = \"up-***************\" \n\ndata = [\n    {\n        \"question\": \"How can you ensure that a designed prompt elicits the desired response from a language model?\",\n        \"context\": \"nudge the model to generate a desired result. Prompt design can be an efficient\\nway to experiment with adapting a language model for a specific use case.\",\n        \"response\": \"To ensure that a designed prompt elicits the desired response from a language model, you can experiment with different prompt designs that nudge the model towards generating the desired result. This can be an efficient way to adapt the language model for a specific use case.\",\n        \"chunk_size\": 200\n    },\n    {\n        \"question\": \"How can you ensure that a designed prompt elicits the desired response from a language model?\",\n        \"context\": \"Design and save your own prompts\\nPrompt design is the process of manually creating prompts that elicit the\\ndesired response from a language model. By carefully crafting prompts, you can\\nnudge the model to generate a desired result. Prompt design can be an efficient\\nway to experiment with adapting a language model for a specific use case.\\nYou can create and save your own prompts in Vertex AI Studio. When\\ncreating a new prompt, you enter the prompt text, specify the model to use,\\nconfigure parameter values, and test the prompt by generating a response. You\\ncan iterate on the prompt and its configurations until you get the desired\\nresults. When you are done designing the prompt, you can save it in\\nVertex AI Studio.\\nResponse citations\\nIf you are using a text model in Vertex AI Studio like text-bison, you\\nreceive text responses based on your input. Our features are intended to produce\\noriginal content and not replicate existing content at length. If\",\n        \"response\": \"To ensure that a designed prompt elicits the desired response from a language model, you can manually create prompts that carefully craft the desired result. By iterating on the prompt and its configurations in Vertex AI Studio, you can experiment and adjust until you achieve the desired results. Additionally, it is important to note that the text models in Vertex AI Studio are designed to produce original content and not replicate existing content at length.\",\n        \"chunk_size\": 1000\n    }\n]\n\nclient = APIClient(uptrain_api_key=UPTRAIN_API_KEY)\n\nresults = client.evaluate_experiments(\n    project_name=\"Chunk-Size-Experiment\",\n    data=data,\n    checks=[Evals.CONTEXT_RELEVANCE, Evals.RESPONSE_RELEVANCE, Evals.FACTUAL_ACCURACY],\n    exp_columns=[\"chunk_size\"]\n)\n\nprint(json.dumps(results, indent=3))\n</code></pre>"},{"location":"#key-features","title":"Key Features \ud83d\udca1","text":"<ul> <li>Custom Grading Checks - Write your custom grading prompts to use LLM as an evaluator.</li> <li>Embeddings Similarity Check - Compute cosine similarity between prompt-response embeddings</li> <li>UMAP Visualization and Clustering - Visualize your embedding space using tools like UMAP and t-SNE.</li> <li>Feature Slicing - Built-in pivoting functionalities for data dice and slice to pinpoint low-performing cohorts.</li> <li>Realtime Dashboards - Monitor your model's performance in realtime.</li> </ul>"},{"location":"#dimensions-of-llm-evaluations","title":"Dimensions of LLM Evaluations \ud83d\udca1","text":"<p>We recently wrote about different criteria to evaluate LLM applications and explored grouping them into categories. Read more about it.</p>"},{"location":"#integrations","title":"Integrations","text":"Eval Frameworks LLM Providers LLM Packages Serving frameworks OpenAI Evals \u2705 GPT-3.5-turbo \u2705 Langchain \ud83d\udd1c HuggingFace \ud83d\udd1c EleutherAI LM Eval \ud83d\udd1c GPT-4 \u2705 Llama Index \ud83d\udd1c Replicate \ud83d\udd1c BIG-Bench \ud83d\udd1c Claude \u2705 AutoGPT \ud83d\udd1c Cohere \u2705"},{"location":"#why-uptrain","title":"Why UpTrain \ud83e\udd14?","text":"<p>Large language models are trained over billions of data points and perform really well over a wide variety of tasks. But one thing these models are not good at is being deterministic. Even with the most well-crafted prompts, the model can misbehave for certain inputs, be it hallucinations, wrong output structure, toxic or biased response, irrelevant response, and error modes can be immense. </p> <p>To ensure your LLM applications work reliably and correctly, UpTrain makes it easy for developers to evaluate the responses of their applications on multiple criteria. UpTrain's evaluation framework can be used to:</p> <p>1) Improve performance by 20% - You can\u2019t improve what you can\u2019t measure. UpTrain continuously monitors your application's performance on multiple evaluation criterions and alerts you in case of any regressions with automatic root cause analysis.</p> <p>1) Iterate 3x faster - UpTrain enables fast and robust experimentation across multiple prompts, model providers, and custom configurations, by calculating quantitative scores for direct comparison and optimal prompt selection.</p> <p>1) Mitigate LLM Hallucinations - Hallucinations have plagued LLMs since their inception. By quantifying degree of hallucination and quality of retrieved context, UpTrain helps to detect responses with low factual accuracy and prevent them before serving to the end-users.</p>"},{"location":"#what-does-uptrain-have-to-offer","title":"What does UpTrain have to offer? \ud83d\ude80","text":"<p>To make it easy for you to evaluate your LLM applications, UpTrain offers:</p> <p>1) Diverse LLM Evaluations - UpTrain provides a diverse set of pre-built metrics like response relevance, context quality, factual accuracy, language quality, etc. to evaluate your LLM applications upon.</p> <p>1) Single-line Integration - With UpTrain's wide array of pre-built metrics, you can run LLM evaluations in less than two minutes.</p> <p>1) Customization - UpTrain is built with customization at its core, allowing you to configure custom grading prompts and operators with just a python function.</p> <p>We are constantly working to make UpTrain better. Want a new feature or need any integrations? Feel free to create an issue or contribute directly to the repository.</p>"},{"location":"#license","title":"License \ud83d\udcbb","text":"<p>This repo is published under Apache 2.0 license and we are committed to adding more functionalities to the UpTrain open-source repo. Upon popular demand, we have also rolled out a no-code self-serve console. For customized onboarding, please book a demo call here.</p>"},{"location":"#stay-updated","title":"Stay Updated \u260e\ufe0f","text":"<p>We are continuously adding tons of features and use cases. Please support us by giving the project a star \u2b50!</p>"},{"location":"#provide-feedback-harsher-the-better","title":"Provide feedback (Harsher the better \ud83d\ude09)","text":"<p>We are building UpTrain in public. Help us improve by giving your feedback here.</p>"},{"location":"#contributors","title":"Contributors \ud83d\udda5\ufe0f","text":"<p>We welcome contributions to UpTrain. Please see our contribution guide for details.</p> <p> </p>"},{"location":"framework/Check/","title":"Check","text":"<p>A simple check that runs the given list of table operators in sequence.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Name of the check.</p> <code>operators</code> <code>list[TableOp]</code> <p>A list of operators to run in sequence on the input data. The output of each operator is passed as input to the next operator.</p> <code>plots</code> <code>list[Operator]</code> <p>How to plot the output of the check.</p> Source code in <code>uptrain/framework/checks.py</code> <pre><code>class Check:\n    \"\"\"A simple check that runs the given list of table operators in sequence.\n\n    Attributes:\n        name (str): Name of the check.\n        operators (list[TableOp]): A list of operators to run in sequence on the input data. The output of each\n            operator is passed as input to the next operator.\n        plots (list[Operator]): How to plot the output of the check.\n    \"\"\"\n\n    name: str\n    operators: list[Operator]\n    plots: list[Operator]\n\n    def __init__(\n        self,\n        name: str,\n        operators: list[Operator],\n        plots: t.Union[list[Operator], None] = None,\n    ):\n        self.name = name\n        self.operators = operators\n        self.plots = plots if plots is not None else []\n\n    def setup(self, settings: \"Settings\"):\n        self._settings = settings\n\n        # no need to add the plot operator to the dag, since it's run later\n        self._op_dag = OperatorDAG(name=self.name)\n        for i, op in enumerate(self.operators):\n            if i == 0:\n                deps = []\n            else:\n                deps = [f\"operator_{i-1}\"]\n            self._op_dag.add_step(f\"operator_{i}\", op, deps=deps)\n        self._op_dag.setup(settings)\n\n        return self\n\n    def run(\n        self, data: t.Union[pl.DataFrame, None] = None\n    ) -&gt; t.Union[pl.DataFrame, None]:\n        \"\"\"Run this check on the given data.\"\"\"\n        node_inputs = {\"operator_0\": data}\n\n        if len(self.operators):\n            # pick output from the last op in the sequence\n            name_final_node = f\"operator_{len(self.operators) - 1}\"\n            node_outputs = self._op_dag.run(\n                node_inputs=node_inputs,\n                output_nodes=[name_final_node],\n            )\n            return node_outputs[name_final_node]\n        else:\n            return data\n\n    def dict(self) -&gt; dict:\n        \"\"\"Serialize this check to a dict.\"\"\"\n        return {\n            \"name\": self.name,\n            \"operators\": [to_py_types(op) for op in self.operators],\n            \"plots\": [to_py_types(op) for op in self.plots],\n        }  # serializes only the attributes of the class, like pydantic models\n\n    @classmethod\n    def from_dict(cls, data: dict) -&gt; \"Check\":\n        \"\"\"Deserialize a check from a dict of its parameters.\"\"\"\n        operators = [deserialize_operator(op) for op in data[\"operators\"]]\n        plots = [deserialize_operator(op) for op in data[\"plots\"]]\n        return cls(name=data[\"name\"], operators=operators, plots=plots)  # type: ignore\n</code></pre>"},{"location":"framework/Check/#uptrain.framework.Check.dict","title":"<code>dict()</code>","text":"<p>Serialize this check to a dict.</p> Source code in <code>uptrain/framework/checks.py</code> <pre><code>def dict(self) -&gt; dict:\n    \"\"\"Serialize this check to a dict.\"\"\"\n    return {\n        \"name\": self.name,\n        \"operators\": [to_py_types(op) for op in self.operators],\n        \"plots\": [to_py_types(op) for op in self.plots],\n    }  # serializes only the attributes of the class, like pydantic models\n</code></pre>"},{"location":"framework/Check/#uptrain.framework.Check.from_dict","title":"<code>from_dict(data)</code>  <code>classmethod</code>","text":"<p>Deserialize a check from a dict of its parameters.</p> Source code in <code>uptrain/framework/checks.py</code> <pre><code>@classmethod\ndef from_dict(cls, data: dict) -&gt; \"Check\":\n    \"\"\"Deserialize a check from a dict of its parameters.\"\"\"\n    operators = [deserialize_operator(op) for op in data[\"operators\"]]\n    plots = [deserialize_operator(op) for op in data[\"plots\"]]\n    return cls(name=data[\"name\"], operators=operators, plots=plots)  # type: ignore\n</code></pre>"},{"location":"framework/Check/#uptrain.framework.Check.run","title":"<code>run(data=None)</code>","text":"<p>Run this check on the given data.</p> Source code in <code>uptrain/framework/checks.py</code> <pre><code>def run(\n    self, data: t.Union[pl.DataFrame, None] = None\n) -&gt; t.Union[pl.DataFrame, None]:\n    \"\"\"Run this check on the given data.\"\"\"\n    node_inputs = {\"operator_0\": data}\n\n    if len(self.operators):\n        # pick output from the last op in the sequence\n        name_final_node = f\"operator_{len(self.operators) - 1}\"\n        node_outputs = self._op_dag.run(\n            node_inputs=node_inputs,\n            output_nodes=[name_final_node],\n        )\n        return node_outputs[name_final_node]\n    else:\n        return data\n</code></pre>"},{"location":"framework/CheckSet/","title":"CheckSet","text":"<p>Container for a set of checks to run together. This is the entrypoint to Uptrain for users.</p> <p>Attributes:</p> Name Type Description <code>source</code> <code>Operator</code> <p>The source operator to run. Specifies where to get the data from.</p> <code>preprocessors</code> <code>list[TableOp]</code> <p>A list of operators to run on the input data before running the checks.</p> <code>checks</code> <code>list[Check]</code> <p>The set of checks to run on the input data.</p> Source code in <code>uptrain/framework/checks.py</code> <pre><code>class CheckSet:\n    \"\"\"Container for a set of checks to run together. This is the entrypoint to Uptrain for users.\n\n    Attributes:\n        source (Operator): The source operator to run. Specifies where to get the data from.\n        preprocessors (list[TableOp]): A list of operators to run on the input data before running the checks.\n        checks (list[Check]): The set of checks to run on the input data.\n    \"\"\"\n\n    source: Operator\n    checks: list[Check]\n    preprocessors: list[TransformOp]\n    postprocessors: list[Operator]\n\n    def __init__(\n        self,\n        source: Operator,\n        checks: list[t.Any],\n        preprocessors: t.Union[list[TransformOp], None] = None,\n        postprocessors: t.Union[list[Operator], None] = None,\n    ):\n        self.source = source\n        self.checks = checks\n        self.preprocessors = preprocessors if preprocessors is not None else []\n        self.postprocessors = postprocessors if postprocessors is not None else []\n\n        # verify all checks have different names\n        check_names = [check.name for check in checks]\n        assert len(set(check_names)) == len(check_names), \"Duplicate check names\"\n        for check in checks:\n            assert isinstance(check, Check), \"Each check must be an instance of Check\"\n\n    def setup(self, settings: Settings):\n        \"\"\"Create the logs directory, or clear it if it already exists. Also, persist the\n        evaluation config.\n        \"\"\"\n        self._settings = settings\n        logs_dir = self._settings.logs_folder\n        if not os.path.exists(logs_dir):\n            os.makedirs(logs_dir)\n        else:\n            clear_directory(logs_dir)\n\n        logger.info(f\"Uptrain Logs directory: {logs_dir}\")\n\n        # persist the check-set as well as the corresponding settings\n        self.serialize(os.path.join(logs_dir, \"config.json\"))\n        self._settings.serialize(os.path.join(logs_dir, \"settings.json\"))\n\n        self.source.setup(self._settings)\n        for preprocessor in self.preprocessors:\n            preprocessor.setup(self._settings)\n        for check in self.checks:\n            check.setup(self._settings)\n        for postprocessor in self.postprocessors:\n            postprocessor.setup(self._settings)\n        return self\n\n    def run(self):\n        \"\"\"Run all checks in this set.\"\"\"\n        from uptrain.operators import JsonWriter\n\n        logger.info(\"CheckSet Status: Starting checkset\")\n\n        source_output = self.source.run()[\"output\"]\n        if source_output is None:\n            raise RuntimeError(\"Dataset read from the source is: None\")\n        if len(source_output) == 0:\n            raise RuntimeError(\"Dataset read from the source is: empty\")\n        logger.info(\"CheckSet Status: Dataset loaded from source\")\n\n        if len(self.preprocessors) &gt; 0:\n            for preprocessor in self.preprocessors:\n                source_output = preprocessor.run(source_output)[\"output\"]\n                assert source_output is not None, \"Output of preprocessor is None\"\n\n            # persist the preprocessed input for debugging\n            JsonWriter(\n                fpath=os.path.join(\n                    self._settings.logs_folder, \"preprocessed_input.jsonl\"\n                )\n            ).setup(self._settings).run(source_output)\n\n        logger.info(\"CheckSet Status: Preprocessing Done\")\n\n        consolidated_output = {}\n        for check in self.checks:\n            logger.info(f\"CheckSet Status: Check {check.name} Started\")\n            check_output = check.run(source_output)\n            assert check_output is not None, f\"Output of check {check.name} is None\"\n            self._get_sink_for_check(self._settings, check).run(check_output)\n            logger.info(f\"CheckSet Status: Check {check.name} Completed\")\n\n            if len(self.postprocessors):\n                if not all(isinstance(op, ColumnOp) for op in check.operators):\n                    continue\n                for col in check_output.columns:\n                    consolidated_output[col] = check_output[col]\n        logger.info(\"CheckSet Status: All Checks Completed\")\n\n        if len(self.postprocessors):\n            consolidated_output = pl.DataFrame(consolidated_output)\n            for postprocessor in self.postprocessors:\n                consolidated_output = postprocessor.run(consolidated_output)[\"output\"]\n                assert (\n                    consolidated_output is not None\n                ), \"Output of postprocessor is None\"\n\n            # persist the postprocessed input for debugging\n            JsonWriter(\n                fpath=os.path.join(\n                    self._settings.logs_folder, \"postprocessed_input.jsonl\"\n                )\n            ).setup(self._settings).run(consolidated_output)\n        logger.info(\"CheckSet Status: Postprocessing Done\")\n\n    @staticmethod\n    def _get_sink_for_check(settings: Settings, check: Check):\n        \"\"\"Get the sink operator for this check.\"\"\"\n        from uptrain.operators import JsonWriter\n\n        fname = check.name.replace(\" \", \"_\") + \".jsonl\"\n        return JsonWriter(fpath=os.path.join(settings.logs_folder, fname))\n\n    @classmethod\n    def from_dict(cls, data: dict) -&gt; \"CheckSet\":\n        checks = [Check.from_dict(check) for check in data.get(\"checks\", [])]\n        return cls(\n            source=deserialize_operator(data[\"source\"]),\n            preprocessors=[\n                deserialize_operator(op) for op in data.get(\"preprocessors\", [])\n            ],  # type: ignore\n            checks=checks,\n            postprocessors=[\n                deserialize_operator(op) for op in data.get(\"postprocessors\", [])\n            ],  # type: ignore\n        )\n\n    def dict(self) -&gt; dict:\n        return {\n            \"source\": to_py_types(self.source),\n            \"preprocessors\": [to_py_types(op) for op in self.preprocessors],\n            \"checks\": [check.dict() for check in self.checks],\n            \"postprocessors\": [to_py_types(op) for op in self.postprocessors],\n        }\n\n    @classmethod\n    def deserialize(cls, fpath: str) -&gt; \"CheckSet\":\n        with open(fpath, \"r\") as f:\n            return cls.from_dict(jsonload(f))\n\n    def serialize(self, fpath: t.Optional[str] = None):\n        \"\"\"Serialize this check set along with the run settings to a JSON file.\"\"\"\n        if fpath is None:\n            fpath = os.path.join(self._settings.logs_folder, \"config.json\")\n\n        with open(fpath, \"w\") as f:\n            jsondump(self.dict(), f)\n</code></pre>"},{"location":"framework/CheckSet/#uptrain.framework.CheckSet.run","title":"<code>run()</code>","text":"<p>Run all checks in this set.</p> Source code in <code>uptrain/framework/checks.py</code> <pre><code>def run(self):\n    \"\"\"Run all checks in this set.\"\"\"\n    from uptrain.operators import JsonWriter\n\n    logger.info(\"CheckSet Status: Starting checkset\")\n\n    source_output = self.source.run()[\"output\"]\n    if source_output is None:\n        raise RuntimeError(\"Dataset read from the source is: None\")\n    if len(source_output) == 0:\n        raise RuntimeError(\"Dataset read from the source is: empty\")\n    logger.info(\"CheckSet Status: Dataset loaded from source\")\n\n    if len(self.preprocessors) &gt; 0:\n        for preprocessor in self.preprocessors:\n            source_output = preprocessor.run(source_output)[\"output\"]\n            assert source_output is not None, \"Output of preprocessor is None\"\n\n        # persist the preprocessed input for debugging\n        JsonWriter(\n            fpath=os.path.join(\n                self._settings.logs_folder, \"preprocessed_input.jsonl\"\n            )\n        ).setup(self._settings).run(source_output)\n\n    logger.info(\"CheckSet Status: Preprocessing Done\")\n\n    consolidated_output = {}\n    for check in self.checks:\n        logger.info(f\"CheckSet Status: Check {check.name} Started\")\n        check_output = check.run(source_output)\n        assert check_output is not None, f\"Output of check {check.name} is None\"\n        self._get_sink_for_check(self._settings, check).run(check_output)\n        logger.info(f\"CheckSet Status: Check {check.name} Completed\")\n\n        if len(self.postprocessors):\n            if not all(isinstance(op, ColumnOp) for op in check.operators):\n                continue\n            for col in check_output.columns:\n                consolidated_output[col] = check_output[col]\n    logger.info(\"CheckSet Status: All Checks Completed\")\n\n    if len(self.postprocessors):\n        consolidated_output = pl.DataFrame(consolidated_output)\n        for postprocessor in self.postprocessors:\n            consolidated_output = postprocessor.run(consolidated_output)[\"output\"]\n            assert (\n                consolidated_output is not None\n            ), \"Output of postprocessor is None\"\n\n        # persist the postprocessed input for debugging\n        JsonWriter(\n            fpath=os.path.join(\n                self._settings.logs_folder, \"postprocessed_input.jsonl\"\n            )\n        ).setup(self._settings).run(consolidated_output)\n    logger.info(\"CheckSet Status: Postprocessing Done\")\n</code></pre>"},{"location":"framework/CheckSet/#uptrain.framework.CheckSet.serialize","title":"<code>serialize(fpath=None)</code>","text":"<p>Serialize this check set along with the run settings to a JSON file.</p> Source code in <code>uptrain/framework/checks.py</code> <pre><code>def serialize(self, fpath: t.Optional[str] = None):\n    \"\"\"Serialize this check set along with the run settings to a JSON file.\"\"\"\n    if fpath is None:\n        fpath = os.path.join(self._settings.logs_folder, \"config.json\")\n\n    with open(fpath, \"w\") as f:\n        jsondump(self.dict(), f)\n</code></pre>"},{"location":"framework/CheckSet/#uptrain.framework.CheckSet.setup","title":"<code>setup(settings)</code>","text":"<p>Create the logs directory, or clear it if it already exists. Also, persist the evaluation config.</p> Source code in <code>uptrain/framework/checks.py</code> <pre><code>def setup(self, settings: Settings):\n    \"\"\"Create the logs directory, or clear it if it already exists. Also, persist the\n    evaluation config.\n    \"\"\"\n    self._settings = settings\n    logs_dir = self._settings.logs_folder\n    if not os.path.exists(logs_dir):\n        os.makedirs(logs_dir)\n    else:\n        clear_directory(logs_dir)\n\n    logger.info(f\"Uptrain Logs directory: {logs_dir}\")\n\n    # persist the check-set as well as the corresponding settings\n    self.serialize(os.path.join(logs_dir, \"config.json\"))\n    self._settings.serialize(os.path.join(logs_dir, \"settings.json\"))\n\n    self.source.setup(self._settings)\n    for preprocessor in self.preprocessors:\n        preprocessor.setup(self._settings)\n    for check in self.checks:\n        check.setup(self._settings)\n    for postprocessor in self.postprocessors:\n        postprocessor.setup(self._settings)\n    return self\n</code></pre>"},{"location":"framework/Remote/","title":"Remote","text":"<p>This module implements a simple client that can be used to schedule unit-tests/evaluations  on the UpTrain server.</p>"},{"location":"framework/Remote/#uptrain.framework.remote.APIClient","title":"<code>APIClient</code>","text":"Source code in <code>uptrain/framework/remote.py</code> <pre><code>class APIClient:\n    base_url: str\n    client: httpx.Client\n\n    def __init__(self, settings: Settings = None, uptrain_api_key: str = None) -&gt; None:\n        if (uptrain_api_key is None) and (settings is None):\n            raise Exception(\"Please provide UpTrain API Key\")\n\n        if settings is None:\n            settings = Settings(uptrain_access_token=uptrain_api_key)\n\n        server_url = settings.check_and_get(\"uptrain_server_url\")\n        api_key = settings.check_and_get(\"uptrain_access_token\")\n        self.settings = settings\n        self.base_url = server_url.rstrip(\"/\") + \"/api/public\"\n        self.client = httpx.Client(\n            headers={\"uptrain-access-token\": api_key},\n            timeout=httpx.Timeout(7200, connect=5),\n        )\n\n    def check_auth(self):\n        \"\"\"Ping the server to check if the client is authenticated.\"\"\"\n        url = f\"{self.base_url}/auth\"\n        try:\n            response = self.client.get(url)\n            return raise_or_return(response)\n        except httpx.ConnectError as e:\n            raise RuntimeError(\n                f\"Failed to connect to the Uptrain server at {self.base_url}\"\n            ) from e\n\n    def add_dataset(self, name: str, fpath: str):\n        url = f\"{self.base_url}/dataset\"\n        with open(fpath, \"rb\") as file:\n            files = {\"data_file\": (name, file, \"application/octet-stream\")}\n            response = self.client.post(url, data={\"name\": name}, files=files)\n        return raise_or_return(response)\n\n    def get_dataset(self, name: str, version: t.Optional[int] = None):\n        url = f\"{self.base_url}/dataset\"\n        params: dict = {\"name\": name}\n        if version is not None:\n            params[\"version\"] = version\n        response = self.client.get(url, params=params)\n        return raise_or_return(response)\n\n    def list_datasets(self, skip: int = 0, limit: int = 100):\n        url = f\"{self.base_url}/datasets\"\n        params = {\"skip\": skip, \"limit\": limit}\n        response = self.client.get(url, params=params)\n        return raise_or_return(response)\n\n    def download_dataset(\n        self, name: str, fpath: str, version: t.Optional[int] = None\n    ) -&gt; None:\n        \"\"\"\n        Download a dataset from the server.\n\n        Args:\n            name: name of the dataset to download\n            fpath: path to save the dataset to\n            version: version of the dataset to download. If None, the latest version is downloaded.\n        \"\"\"\n        url = f\"{self.base_url}/dataset/{name}/download\"\n        params: dict = {}\n        if version is not None:\n            params[\"version\"] = version\n        response = self.client.get(url, params=params)\n        if not response.is_success:\n            logger.error(response.text)\n            response.raise_for_status()\n        else:\n            with open(fpath, \"wb\") as download_file:\n                for chunk in response.iter_bytes():\n                    download_file.write(chunk)\n\n    def add_checkset(self, name: str, checkset: CheckSet, settings: Settings):\n        url = f\"{self.base_url}/checkset\"\n        response = self.client.post(\n            url,\n            json={\"name\": name, \"config\": checkset.dict(), \"settings\": settings.dict()},\n        )\n        return raise_or_return(response)\n\n    def get_checkset(self, name: str, version: t.Optional[int] = None):\n        url = f\"{self.base_url}/checkset\"\n        params: dict = {\"name\": name}\n        if version is not None:\n            params[\"version\"] = version\n        response = self.client.get(url, params=params)\n        return raise_or_return(response)\n\n    def list_checksets(self, skip: int = 0, limit: int = 10):\n        url = f\"{self.base_url}/checksets\"\n        params = {\"skip\": skip, \"limit\": limit}\n        response = self.client.get(url, params=params)\n        return raise_or_return(response)\n\n    def add_experiment(\n        self,\n        name: str,\n        checkset: CheckSet,\n        experiment_args: ExperimentArgs,\n        settings: Settings,\n    ):\n        preprocessors = experiment_args._get_preprocessors()\n        modified_checks = experiment_args._modify_checks(checkset.checks)\n        modified_checkset = CheckSet(\n            source=checkset.source, checks=modified_checks, preprocessors=preprocessors\n        )\n        url = f\"{self.base_url}/checkset\"\n        response = self.client.post(\n            url,\n            json={\n                \"name\": name,\n                \"config\": modified_checkset.dict(),\n                \"settings\": settings.dict(),\n            },\n        )\n        return raise_or_return(response)\n\n    def add_run(self, dataset: str, checkset: str):\n        \"\"\"Schedules an evaluation on the server. Specify the dataset and checkset to use.\n\n        Args:\n            dataset: name of the dataset to use\n            checkset: name of the checkset to use\n\n        Returns:\n            run: information about the run along with a unique identifier.\n        \"\"\"\n        url = f\"{self.base_url}/run\"\n        response = self.client.post(\n            url, json={\"dataset\": dataset, \"checkset\": checkset}\n        )\n        return raise_or_return(response)\n\n    def get_run(self, run_id: str):\n        \"\"\"Get the status of a run.\n\n        Args:\n            run_id: unique identifier for the run.\n\n        Returns:\n            run: information about the run along with a unique identifier.\n        \"\"\"\n        url = f\"{self.base_url}/run/{run_id}\"\n        response = self.client.get(url)\n        return raise_or_return(response)\n\n    def get_run_results(self, run_id: str, check_name: str) -&gt; list[dict]:\n        \"\"\"Get the results of a run.\n\n        Args:\n            run_id: unique identifier for the run.\n            check_name: name of the check to get results for.\n\n        Returns:\n            run: information about the run along with a unique identifier.\n        \"\"\"\n        url = f\"{self.base_url}/run/{run_id}/results\"\n        params: dict = {\"check_name\": check_name}\n        response = self.client.get(url, params=params)\n        if not response.is_success:\n            logger.error(response.text)\n            response.raise_for_status()\n        else:\n            return pl.DataFrame(\n                polars_to_pandas(pl.read_ndjson(response.content))\n            ).to_dicts()\n\n    def download_run_result(self, run_id: str, check_name: str, fpath: str) -&gt; None:\n        \"\"\"Download the results of a run.\n\n        Args:\n            run_id: unique identifier for the run.\n            check_name: name of the check to get results for.\n            fpath: path to save the results to.\n        \"\"\"\n        url = f\"{self.base_url}/run/{run_id}/results\"\n        params: dict = {\"check_name\": check_name}\n        with self.client.stream(\"GET\", url, params=params) as response:\n            if not response.is_success:\n                logger.error(response.text)\n                response.raise_for_status()\n            else:\n                with open(fpath, \"wb\") as download_file:\n                    for chunk in response.iter_bytes():\n                        download_file.write(chunk)\n\n    def list_runs(self, num: int = 10, only_completed: bool = False):\n        \"\"\"List all the runs on the server.\n\n        - filter by scheduled/completed/in-process?\n        \"\"\"\n        url = f\"{self.base_url}/runs\"\n        params: dict = {\"num\": num}\n        if only_completed:\n            params[\"status\"] = \"completed\"\n        response = self.client.get(url, params=params)\n        return raise_or_return(response)\n\n    def add_daily_schedule(\n        self,\n        checkset: str,\n        start_on: str,\n        assign_topics: int = 0,\n        assign_topics_args: t.Optional[dict] = None,\n        extra_args: t.Optional[dict] = None,\n    ):\n        \"\"\"Schedules a periodic evaluation on the server. Specify the checkset to run against it.\n\n        Args:\n            checkset: name of the checkset to use\n            start_on: date to start the schedule on\n\n        Returns:\n            run: information about the schedule along with a unique identifier.\n        \"\"\"\n        url = f\"{self.base_url}/schedule\"\n        response = self.client.post(\n            url,\n            json={\n                \"checkset\": checkset,\n                \"start_on\": start_on,\n                \"assign_topics\": assign_topics,\n                \"assign_topics_args\": assign_topics_args,\n                \"extra_args\": extra_args,\n            },\n        )\n        return raise_or_return(response)\n\n    def get_schedule(self, schedule_id: str):\n        \"\"\"Get the status of a schedule.\n\n        Args:\n            schedule_id: unique identifier for the schedule.\n\n        Returns:\n            run: information about the schedule along with a unique identifier.\n        \"\"\"\n        url = f\"{self.base_url}/schedule/{schedule_id}\"\n        response = self.client.get(url)\n        return raise_or_return(response)\n\n    def remove_schedule(self, schedule_id: str):\n        \"\"\"Remove a schedule.\n\n        Args:\n            schedule_id: unique identifier for the schedule.\n\n        Returns:\n            run: information about the schedule along with a unique identifier.\n        \"\"\"\n        url = f\"{self.base_url}/schedule/{schedule_id}\"\n        response = self.client.delete(url)\n        return raise_or_return(response)\n\n    def list_schedules(self, num: int = 10, active_only: bool = True):\n        \"\"\"List all the schedules on the server.\"\"\"\n        url = f\"{self.base_url}/schedules\"\n        params: dict = {\"num\": num, \"active_only\": active_only}\n        response = self.client.get(url, params=params)\n        return raise_or_return(response)\n\n    def rerun_schedule(\n        self, schedule_id: str, start_on: str, end_on: t.Optional[str] = None\n    ):\n        \"\"\"Rerun a schedule.\n        - New checks added to the checkset are run for all dates\n        - Existing checks are run only on the failed rows.\n\n        Args:\n            schedule_id: unique identifier for the run.\n            start_on: date to start the reruns on\n            end_on: date to end the reruns on\n\n        Returns:\n            run: information about the schedule along with a unique identifier.\n        \"\"\"\n        url = f\"{self.base_url}/schedule/{schedule_id}/rerun\"\n        params = {\"start_on\": start_on}\n        if end_on is not None:\n            params[\"end_on\"] = end_on\n        response = self.client.put(url, params=params)\n        return raise_or_return(response)\n\n    def get_schedule_results_all_dates(\n        self, schedule_id: str, check_name: str\n    ) -&gt; list[dict]:\n        \"\"\"Get the results of a schedule.\n\n        Args:\n            schedule_id: unique identifier for the schedule.\n            check_name: name of the check to get results for.\n\n        Returns:\n            run: information about the schedule along with a unique identifier.\n        \"\"\"\n        url = f\"{self.base_url}/schedule/{schedule_id}/results\"\n        params: dict = {\"check_name\": check_name}\n        response = self.client.get(url, params=params)\n        if not response.is_success:\n            logger.error(response.text)\n            response.raise_for_status()\n        else:\n            return pl.read_ndjson(response.content).to_dicts()\n\n    def evaluate(\n        self,\n        eval_name: str,\n        full_dataset: t.Union[list[dict], pl.DataFrame, pd.DataFrame],\n        params: t.Union[dict, None] = None,\n    ):\n        \"\"\"Run an evaluation on the server.\n\n        NOTE: Internal use only. Use regular uptrain operators to run evaluations,\n        \"\"\"\n        url = f\"{self.base_url}/evaluate\"\n        if isinstance(full_dataset, pl.DataFrame):\n            full_dataset = full_dataset.to_dicts()\n        elif isinstance(full_dataset, pd.DataFrame):\n            full_dataset = full_dataset.to_dict(orient=\"records\")\n\n        # send in chunks of 100, so the connection doesn't time out waiting for the server\n        results = []\n\n        if params is not None:\n            params[\"uptrain_settings\"] = self.settings.dict()\n        else:\n            params = {}\n            params[\"uptrain_settings\"] = self.settings.dict()\n\n        NUM_TRIES = 3\n        for i in range(0, len(full_dataset), 100):\n            response_json = None\n            for try_num in range(NUM_TRIES):\n                try:\n                    logger.info(\n                        f\"Sending evaluation request for rows {i} to &lt;{i+100} to the Uptrain server\"\n                    )\n                    response = self.client.post(\n                        url,\n                        json={\n                            \"eval_name\": eval_name,\n                            \"dataset\": full_dataset[i : i + 100],\n                            \"params\": params,\n                        },\n                    )\n                    response_json = raise_or_return(response)\n                    break\n                except Exception as e:\n                    logger.info(\"Retrying evaluation request\")\n                    if try_num == NUM_TRIES - 1:\n                        logger.error(f\"Evaluation failed with error: {e}\")\n                        raise e\n\n            if response_json is not None:\n                results.extend(response_json)\n\n        return results\n\n    def perform_root_cause_analysis(\n        self,\n        project_name: str,\n        data: t.Union[list[dict], pl.DataFrame, pd.DataFrame],\n        rca_template: RcaTemplate,\n        scenario_description: t.Optional[str] = None,\n        schema: t.Union[DataSchema, dict[str, str], None] = None,\n        metadata: t.Optional[dict[str, t.Any]] = None,\n    ):\n        \"\"\"Perform root cause analysis on the server.\n\n        Args:\n            project_name: Name of the project to run root cause analysis on.\n            data: Data to do rca on. Either a Pandas DataFrame or a list of dicts.\n            rca_template: rca template to run.\n            schema: Schema of the data. Only required if the data attributes aren't typical (question, response, context).\n            metadata: Attributes to attach to this dataset. Useful for filtering and grouping in the UI.\n\n        Returns:\n            results: List of dictionaries with each data point and corresponding error modes.\n        \"\"\"\n\n        url = f\"{self.base_url}/perform_root_cause_analysis\"\n\n        if isinstance(data, pl.DataFrame):\n            data = data.to_dicts()\n        elif isinstance(data, pd.DataFrame):\n            data = data.to_dict(orient=\"records\")\n\n        if schema is None:\n            schema = DataSchema()\n        elif isinstance(schema, dict):\n            schema = DataSchema(**schema)\n\n        if metadata is None:\n            metadata = {}\n        metadata[\"internal_call\"] = metadata.get(\"internal_call\", False)\n\n        req_attrs, ser_templates = set(), []\n        if rca_template == RcaTemplate.RAG_WITH_CITATION:\n            req_attrs.update(\n                [schema.question, schema.response, schema.context, schema.cited_context]\n            )\n        else:\n            raise Exception(\"RCA Template not supported yet\")\n\n        dictn = {\"scenario_description\": scenario_description}\n        ser_templates.append({\"rca_template_name\": rca_template.value, **dictn})\n\n        for idx, row in enumerate(data):\n            if not req_attrs.issubset(row.keys()):\n                raise ValueError(\n                    f\"Row {idx} is missing required all required attributes for evaluation: {req_attrs}\"\n                )\n\n        # send in chunks of 50, so the connection doesn't time out waiting for the server\n        results = []\n        NUM_TRIES, BATCH_SIZE = 3, 50\n        for i in range(0, len(data), BATCH_SIZE):\n            response_json = None\n            for try_num in range(NUM_TRIES):\n                try:\n                    logger.info(\n                        f\"Sending root cause analysis request for rows {i} to &lt;{i+BATCH_SIZE} to the Uptrain server\"\n                    )\n                    response = self.client.post(\n                        url,\n                        json={\n                            \"data\": data[i : i + BATCH_SIZE],\n                            \"rca_templates\": ser_templates,\n                            \"metadata\": {\n                                \"project\": project_name,\n                                \"schema\": schema.dict(),\n                                **metadata,\n                                \"uptrain_settings\": self.settings.dict(),\n                            },\n                        },\n                    )\n                    response_json = raise_or_return(response)\n                    break\n                except Exception as e:\n                    logger.info(\"Retrying root cause analysis request\")\n                    if try_num == NUM_TRIES - 1:\n                        logger.error(f\"Evaluation failed with error: {e}\")\n                        raise e\n\n            if response_json is not None:\n                results.extend(response_json)\n\n        return results\n\n    def log_and_evaluate(\n        self,\n        project_name: str,\n        data: t.Union[list[dict], pl.DataFrame, pd.DataFrame],\n        checks: list[t.Union[str, Evals, ParametricEval]],\n        scenario_description: t.Union[str, list[str], None] = None,\n        schema: t.Union[DataSchema, dict[str, str], None] = None,\n        metadata: t.Optional[dict[str, t.Any]] = None,\n    ):\n        \"\"\"Run an evaluation on the server and log the results.\n        NOTE: This api is a bit different than the regular `evaluate` call.\n\n        Args:\n            project_name: Name of the project to evaluate on.\n            data: Data to evaluate on. Either a Pandas DataFrame or a list of dicts.\n            checks: List of checks to evaluate on.\n            schema: Schema of the data. Only required if the data attributes aren't typical (question, response, context).\n            metadata: Attributes to attach to this dataset. Useful for filtering and grouping in the UI.\n\n        Returns:\n            results: List of dictionaries with each data point and corresponding evaluation results.\n        \"\"\"\n        url = f\"{self.base_url}/log_and_evaluate\"\n\n        if isinstance(data, pl.DataFrame):\n            data = data.to_dicts()\n        elif isinstance(data, pd.DataFrame):\n            data = data.to_dict(orient=\"records\")\n\n        if schema is None:\n            schema = DataSchema()\n        elif isinstance(schema, dict):\n            schema = DataSchema(**schema)\n\n        if metadata is None:\n            metadata = {}\n\n        checks = [Evals(m) if isinstance(m, str) else m for m in checks]\n        for m in checks:\n            assert isinstance(m, (Evals, ParametricEval))\n\n        req_attrs, ser_checks = set(), []\n        for m in checks:\n            if m in [Evals.SUB_QUERY_COMPLETENESS]:\n                req_attrs.update([schema.sub_questions, schema.question])\n            elif m in [Evals.CONTEXT_CONCISENESS]:\n                req_attrs.update(\n                    [schema.question, schema.context, schema.concise_context]\n                )\n            elif m in [Evals.CONTEXT_RERANKING]:\n                req_attrs.update(\n                    [schema.question, schema.context, schema.reranked_context]\n                )\n            elif m in [\n                Evals.FACTUAL_ACCURACY,\n                Evals.RESPONSE_COMPLETENESS_WRT_CONTEXT,\n                Evals.RESPONSE_CONSISTENCY,\n                Evals.CODE_HALLUCINATION,\n            ]:\n                req_attrs.update([schema.question, schema.context, schema.response])\n            elif m in [\n                Evals.VALID_RESPONSE,\n                Evals.RESPONSE_RELEVANCE,\n                Evals.RESPONSE_COMPLETENESS,\n                Evals.RESPONSE_CONCISENESS,\n                Evals.PROMPT_INJECTION,\n            ]:\n                req_attrs.update([schema.question, schema.response])\n            elif m in [Evals.CONTEXT_RELEVANCE]:\n                req_attrs.update([schema.question, schema.context])\n            elif (\n                m in [Evals.CRITIQUE_LANGUAGE]\n                or isinstance(m, CritiqueTone)\n                or isinstance(m, GuidelineAdherence)\n            ):\n                req_attrs.update([schema.response])\n            elif m in [\n                Evals.RESPONSE_ALIGNMENT_WITH_SCENARIO,\n                Evals.RESPONSE_SINCERITY_WITH_SCENARIO,\n            ]:\n                req_attrs.update(\n                    [\n                        schema.question,\n                        schema.response,\n                        schema.scenario,\n                        schema.objective,\n                    ]\n                )\n            elif isinstance(m, ResponseMatching):\n                req_attrs.update(\n                    [schema.question, schema.response, schema.ground_truth]\n                )\n            elif isinstance(m, ConversationSatisfaction):\n                req_attrs.update([schema.conversation])\n            elif isinstance(m, JailbreakDetection):\n                req_attrs.update([schema.question])\n\n            if isinstance(m, ParametricEval):\n                dictm = m.dict()\n                dictm.update({\"scenario_description\": scenario_description})\n                ser_checks.append({\"check_name\": m.__class__.__name__, **dictm})\n            elif isinstance(m, Evals):\n                dictm = {\"scenario_description\": scenario_description}\n                ser_checks.append({\"check_name\": m.value, **dictm})\n            else:\n                raise ValueError(f\"Invalid metric: {m}\")\n        for idx, row in enumerate(data):\n            if not req_attrs.issubset(row.keys()):\n                raise ValueError(\n                    f\"Row {idx} is missing required all required attributes for evaluation: {req_attrs}\"\n                )\n\n        # send in chunks of 50, so the connection doesn't time out waiting for the server\n        results = []\n        NUM_TRIES, BATCH_SIZE = 3, 50\n        for i in range(0, len(data), BATCH_SIZE):\n            response_json = None\n            for try_num in range(NUM_TRIES):\n                try:\n                    logger.info(\n                        f\"Sending evaluation request for rows {i} to &lt;{i+BATCH_SIZE} to the Uptrain server\"\n                    )\n                    response = self.client.post(\n                        url,\n                        json={\n                            \"data\": data[i : i + BATCH_SIZE],\n                            \"checks\": ser_checks,\n                            \"metadata\": {\n                                \"project\": project_name,\n                                \"schema\": schema.dict(),\n                                **metadata,\n                                \"uptrain_settings\": self.settings.dict(),\n                            },\n                        },\n                    )\n                    response_json = raise_or_return(response)\n                    break\n                except Exception as e:\n                    logger.info(\"Retrying evaluation request\")\n                    if try_num == NUM_TRIES - 1:\n                        logger.error(f\"Evaluation failed with error: {e}\")\n                        raise e\n\n            if response_json is not None:\n                results.extend(response_json)\n\n        return results\n\n    def evaluate_experiments(\n        self,\n        project_name: str,\n        data: t.Union[list[dict], pl.DataFrame],\n        checks: list[t.Union[str, Evals, ParametricEval]],\n        exp_columns: list[str],\n        schema: t.Union[DataSchema, dict[str, str], None] = None,\n        metadata: t.Optional[dict[str, t.Any]] = None,\n    ):\n        \"\"\"Evaluate experiments on the server and log the results.\n\n        Args:\n            project_name: Name of the experiment to evaluate on.\n            data: Data to evaluate on. Either a Pandas DataFrame or a list of dicts.\n            checks: List of checks to evaluate on.\n            exp_columns: List of columns/keys which denote different experiment configurations.\n            schema: Schema of the data. Only required if the data attributes aren't typical (question, response, context).\n            metadata: Attributes to attach to this dataset. Useful for filtering and grouping in the UI.\n\n        Returns:\n            results: List of dictionaries with each data point and corresponding evaluation results for all the experiments.\n        \"\"\"\n        if metadata is None:\n            metadata = {}\n\n        metadata.update({\"uptrain_experiment_columns\": exp_columns})\n\n        if schema is None:\n            schema = DataSchema()\n        elif isinstance(schema, dict):\n            schema = DataSchema(**schema)\n\n        results = self.log_and_evaluate(\n            project_name=project_name,\n            data=data,\n            checks=checks,\n            schema=schema,\n            metadata=metadata,\n        )\n\n        results = pl.DataFrame(results)\n        all_cols = set(results.columns)\n        value_cols = list(all_cols - set([schema.question] + exp_columns))\n        index_cols = metadata.get(\"uptrain_index_columns\", [schema.question])\n        exp_results = results.pivot(\n            values=value_cols, index=index_cols, columns=exp_columns\n        )\n        exp_results = exp_results.to_dicts()\n        return exp_results\n\n    def download_project_eval_results(self, project_name: str, fpath: str):\n        \"\"\"Fetch all the evaluation results for a project.\n\n        Args:\n            project_name: Name of the project to fetch results for.\n            fpath: Path to save the results to.\n        \"\"\"\n        url = f\"{self.base_url}/evaluation_results/{project_name}\"\n        with self.client.stream(\"GET\", url) as response:\n            if not response.is_success:\n                logger.error(response.text)\n                response.raise_for_status()\n            else:\n                with open(fpath, \"w\") as download_file:\n                    for chunk in response.iter_text():\n                        download_file.write(chunk)\n</code></pre>"},{"location":"framework/Remote/#uptrain.framework.remote.APIClient.add_daily_schedule","title":"<code>add_daily_schedule(checkset, start_on, assign_topics=0, assign_topics_args=None, extra_args=None)</code>","text":"<p>Schedules a periodic evaluation on the server. Specify the checkset to run against it.</p> <p>Parameters:</p> Name Type Description Default <code>checkset</code> <code>str</code> <p>name of the checkset to use</p> required <code>start_on</code> <code>str</code> <p>date to start the schedule on</p> required <p>Returns:</p> Name Type Description <code>run</code> <p>information about the schedule along with a unique identifier.</p> Source code in <code>uptrain/framework/remote.py</code> <pre><code>def add_daily_schedule(\n    self,\n    checkset: str,\n    start_on: str,\n    assign_topics: int = 0,\n    assign_topics_args: t.Optional[dict] = None,\n    extra_args: t.Optional[dict] = None,\n):\n    \"\"\"Schedules a periodic evaluation on the server. Specify the checkset to run against it.\n\n    Args:\n        checkset: name of the checkset to use\n        start_on: date to start the schedule on\n\n    Returns:\n        run: information about the schedule along with a unique identifier.\n    \"\"\"\n    url = f\"{self.base_url}/schedule\"\n    response = self.client.post(\n        url,\n        json={\n            \"checkset\": checkset,\n            \"start_on\": start_on,\n            \"assign_topics\": assign_topics,\n            \"assign_topics_args\": assign_topics_args,\n            \"extra_args\": extra_args,\n        },\n    )\n    return raise_or_return(response)\n</code></pre>"},{"location":"framework/Remote/#uptrain.framework.remote.APIClient.add_run","title":"<code>add_run(dataset, checkset)</code>","text":"<p>Schedules an evaluation on the server. Specify the dataset and checkset to use.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>str</code> <p>name of the dataset to use</p> required <code>checkset</code> <code>str</code> <p>name of the checkset to use</p> required <p>Returns:</p> Name Type Description <code>run</code> <p>information about the run along with a unique identifier.</p> Source code in <code>uptrain/framework/remote.py</code> <pre><code>def add_run(self, dataset: str, checkset: str):\n    \"\"\"Schedules an evaluation on the server. Specify the dataset and checkset to use.\n\n    Args:\n        dataset: name of the dataset to use\n        checkset: name of the checkset to use\n\n    Returns:\n        run: information about the run along with a unique identifier.\n    \"\"\"\n    url = f\"{self.base_url}/run\"\n    response = self.client.post(\n        url, json={\"dataset\": dataset, \"checkset\": checkset}\n    )\n    return raise_or_return(response)\n</code></pre>"},{"location":"framework/Remote/#uptrain.framework.remote.APIClient.check_auth","title":"<code>check_auth()</code>","text":"<p>Ping the server to check if the client is authenticated.</p> Source code in <code>uptrain/framework/remote.py</code> <pre><code>def check_auth(self):\n    \"\"\"Ping the server to check if the client is authenticated.\"\"\"\n    url = f\"{self.base_url}/auth\"\n    try:\n        response = self.client.get(url)\n        return raise_or_return(response)\n    except httpx.ConnectError as e:\n        raise RuntimeError(\n            f\"Failed to connect to the Uptrain server at {self.base_url}\"\n        ) from e\n</code></pre>"},{"location":"framework/Remote/#uptrain.framework.remote.APIClient.download_dataset","title":"<code>download_dataset(name, fpath, version=None)</code>","text":"<p>Download a dataset from the server.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>name of the dataset to download</p> required <code>fpath</code> <code>str</code> <p>path to save the dataset to</p> required <code>version</code> <code>Optional[int]</code> <p>version of the dataset to download. If None, the latest version is downloaded.</p> <code>None</code> Source code in <code>uptrain/framework/remote.py</code> <pre><code>def download_dataset(\n    self, name: str, fpath: str, version: t.Optional[int] = None\n) -&gt; None:\n    \"\"\"\n    Download a dataset from the server.\n\n    Args:\n        name: name of the dataset to download\n        fpath: path to save the dataset to\n        version: version of the dataset to download. If None, the latest version is downloaded.\n    \"\"\"\n    url = f\"{self.base_url}/dataset/{name}/download\"\n    params: dict = {}\n    if version is not None:\n        params[\"version\"] = version\n    response = self.client.get(url, params=params)\n    if not response.is_success:\n        logger.error(response.text)\n        response.raise_for_status()\n    else:\n        with open(fpath, \"wb\") as download_file:\n            for chunk in response.iter_bytes():\n                download_file.write(chunk)\n</code></pre>"},{"location":"framework/Remote/#uptrain.framework.remote.APIClient.download_project_eval_results","title":"<code>download_project_eval_results(project_name, fpath)</code>","text":"<p>Fetch all the evaluation results for a project.</p> <p>Parameters:</p> Name Type Description Default <code>project_name</code> <code>str</code> <p>Name of the project to fetch results for.</p> required <code>fpath</code> <code>str</code> <p>Path to save the results to.</p> required Source code in <code>uptrain/framework/remote.py</code> <pre><code>def download_project_eval_results(self, project_name: str, fpath: str):\n    \"\"\"Fetch all the evaluation results for a project.\n\n    Args:\n        project_name: Name of the project to fetch results for.\n        fpath: Path to save the results to.\n    \"\"\"\n    url = f\"{self.base_url}/evaluation_results/{project_name}\"\n    with self.client.stream(\"GET\", url) as response:\n        if not response.is_success:\n            logger.error(response.text)\n            response.raise_for_status()\n        else:\n            with open(fpath, \"w\") as download_file:\n                for chunk in response.iter_text():\n                    download_file.write(chunk)\n</code></pre>"},{"location":"framework/Remote/#uptrain.framework.remote.APIClient.download_run_result","title":"<code>download_run_result(run_id, check_name, fpath)</code>","text":"<p>Download the results of a run.</p> <p>Parameters:</p> Name Type Description Default <code>run_id</code> <code>str</code> <p>unique identifier for the run.</p> required <code>check_name</code> <code>str</code> <p>name of the check to get results for.</p> required <code>fpath</code> <code>str</code> <p>path to save the results to.</p> required Source code in <code>uptrain/framework/remote.py</code> <pre><code>def download_run_result(self, run_id: str, check_name: str, fpath: str) -&gt; None:\n    \"\"\"Download the results of a run.\n\n    Args:\n        run_id: unique identifier for the run.\n        check_name: name of the check to get results for.\n        fpath: path to save the results to.\n    \"\"\"\n    url = f\"{self.base_url}/run/{run_id}/results\"\n    params: dict = {\"check_name\": check_name}\n    with self.client.stream(\"GET\", url, params=params) as response:\n        if not response.is_success:\n            logger.error(response.text)\n            response.raise_for_status()\n        else:\n            with open(fpath, \"wb\") as download_file:\n                for chunk in response.iter_bytes():\n                    download_file.write(chunk)\n</code></pre>"},{"location":"framework/Remote/#uptrain.framework.remote.APIClient.evaluate","title":"<code>evaluate(eval_name, full_dataset, params=None)</code>","text":"<p>Run an evaluation on the server.</p> <p>NOTE: Internal use only. Use regular uptrain operators to run evaluations,</p> Source code in <code>uptrain/framework/remote.py</code> <pre><code>def evaluate(\n    self,\n    eval_name: str,\n    full_dataset: t.Union[list[dict], pl.DataFrame, pd.DataFrame],\n    params: t.Union[dict, None] = None,\n):\n    \"\"\"Run an evaluation on the server.\n\n    NOTE: Internal use only. Use regular uptrain operators to run evaluations,\n    \"\"\"\n    url = f\"{self.base_url}/evaluate\"\n    if isinstance(full_dataset, pl.DataFrame):\n        full_dataset = full_dataset.to_dicts()\n    elif isinstance(full_dataset, pd.DataFrame):\n        full_dataset = full_dataset.to_dict(orient=\"records\")\n\n    # send in chunks of 100, so the connection doesn't time out waiting for the server\n    results = []\n\n    if params is not None:\n        params[\"uptrain_settings\"] = self.settings.dict()\n    else:\n        params = {}\n        params[\"uptrain_settings\"] = self.settings.dict()\n\n    NUM_TRIES = 3\n    for i in range(0, len(full_dataset), 100):\n        response_json = None\n        for try_num in range(NUM_TRIES):\n            try:\n                logger.info(\n                    f\"Sending evaluation request for rows {i} to &lt;{i+100} to the Uptrain server\"\n                )\n                response = self.client.post(\n                    url,\n                    json={\n                        \"eval_name\": eval_name,\n                        \"dataset\": full_dataset[i : i + 100],\n                        \"params\": params,\n                    },\n                )\n                response_json = raise_or_return(response)\n                break\n            except Exception as e:\n                logger.info(\"Retrying evaluation request\")\n                if try_num == NUM_TRIES - 1:\n                    logger.error(f\"Evaluation failed with error: {e}\")\n                    raise e\n\n        if response_json is not None:\n            results.extend(response_json)\n\n    return results\n</code></pre>"},{"location":"framework/Remote/#uptrain.framework.remote.APIClient.evaluate_experiments","title":"<code>evaluate_experiments(project_name, data, checks, exp_columns, schema=None, metadata=None)</code>","text":"<p>Evaluate experiments on the server and log the results.</p> <p>Parameters:</p> Name Type Description Default <code>project_name</code> <code>str</code> <p>Name of the experiment to evaluate on.</p> required <code>data</code> <code>Union[list[dict], DataFrame]</code> <p>Data to evaluate on. Either a Pandas DataFrame or a list of dicts.</p> required <code>checks</code> <code>list[Union[str, Evals, ParametricEval]]</code> <p>List of checks to evaluate on.</p> required <code>exp_columns</code> <code>list[str]</code> <p>List of columns/keys which denote different experiment configurations.</p> required <code>schema</code> <code>Union[DataSchema, dict[str, str], None]</code> <p>Schema of the data. Only required if the data attributes aren't typical (question, response, context).</p> <code>None</code> <code>metadata</code> <code>Optional[dict[str, Any]]</code> <p>Attributes to attach to this dataset. Useful for filtering and grouping in the UI.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>results</code> <p>List of dictionaries with each data point and corresponding evaluation results for all the experiments.</p> Source code in <code>uptrain/framework/remote.py</code> <pre><code>def evaluate_experiments(\n    self,\n    project_name: str,\n    data: t.Union[list[dict], pl.DataFrame],\n    checks: list[t.Union[str, Evals, ParametricEval]],\n    exp_columns: list[str],\n    schema: t.Union[DataSchema, dict[str, str], None] = None,\n    metadata: t.Optional[dict[str, t.Any]] = None,\n):\n    \"\"\"Evaluate experiments on the server and log the results.\n\n    Args:\n        project_name: Name of the experiment to evaluate on.\n        data: Data to evaluate on. Either a Pandas DataFrame or a list of dicts.\n        checks: List of checks to evaluate on.\n        exp_columns: List of columns/keys which denote different experiment configurations.\n        schema: Schema of the data. Only required if the data attributes aren't typical (question, response, context).\n        metadata: Attributes to attach to this dataset. Useful for filtering and grouping in the UI.\n\n    Returns:\n        results: List of dictionaries with each data point and corresponding evaluation results for all the experiments.\n    \"\"\"\n    if metadata is None:\n        metadata = {}\n\n    metadata.update({\"uptrain_experiment_columns\": exp_columns})\n\n    if schema is None:\n        schema = DataSchema()\n    elif isinstance(schema, dict):\n        schema = DataSchema(**schema)\n\n    results = self.log_and_evaluate(\n        project_name=project_name,\n        data=data,\n        checks=checks,\n        schema=schema,\n        metadata=metadata,\n    )\n\n    results = pl.DataFrame(results)\n    all_cols = set(results.columns)\n    value_cols = list(all_cols - set([schema.question] + exp_columns))\n    index_cols = metadata.get(\"uptrain_index_columns\", [schema.question])\n    exp_results = results.pivot(\n        values=value_cols, index=index_cols, columns=exp_columns\n    )\n    exp_results = exp_results.to_dicts()\n    return exp_results\n</code></pre>"},{"location":"framework/Remote/#uptrain.framework.remote.APIClient.get_run","title":"<code>get_run(run_id)</code>","text":"<p>Get the status of a run.</p> <p>Parameters:</p> Name Type Description Default <code>run_id</code> <code>str</code> <p>unique identifier for the run.</p> required <p>Returns:</p> Name Type Description <code>run</code> <p>information about the run along with a unique identifier.</p> Source code in <code>uptrain/framework/remote.py</code> <pre><code>def get_run(self, run_id: str):\n    \"\"\"Get the status of a run.\n\n    Args:\n        run_id: unique identifier for the run.\n\n    Returns:\n        run: information about the run along with a unique identifier.\n    \"\"\"\n    url = f\"{self.base_url}/run/{run_id}\"\n    response = self.client.get(url)\n    return raise_or_return(response)\n</code></pre>"},{"location":"framework/Remote/#uptrain.framework.remote.APIClient.get_run_results","title":"<code>get_run_results(run_id, check_name)</code>","text":"<p>Get the results of a run.</p> <p>Parameters:</p> Name Type Description Default <code>run_id</code> <code>str</code> <p>unique identifier for the run.</p> required <code>check_name</code> <code>str</code> <p>name of the check to get results for.</p> required <p>Returns:</p> Name Type Description <code>run</code> <code>list[dict]</code> <p>information about the run along with a unique identifier.</p> Source code in <code>uptrain/framework/remote.py</code> <pre><code>def get_run_results(self, run_id: str, check_name: str) -&gt; list[dict]:\n    \"\"\"Get the results of a run.\n\n    Args:\n        run_id: unique identifier for the run.\n        check_name: name of the check to get results for.\n\n    Returns:\n        run: information about the run along with a unique identifier.\n    \"\"\"\n    url = f\"{self.base_url}/run/{run_id}/results\"\n    params: dict = {\"check_name\": check_name}\n    response = self.client.get(url, params=params)\n    if not response.is_success:\n        logger.error(response.text)\n        response.raise_for_status()\n    else:\n        return pl.DataFrame(\n            polars_to_pandas(pl.read_ndjson(response.content))\n        ).to_dicts()\n</code></pre>"},{"location":"framework/Remote/#uptrain.framework.remote.APIClient.get_schedule","title":"<code>get_schedule(schedule_id)</code>","text":"<p>Get the status of a schedule.</p> <p>Parameters:</p> Name Type Description Default <code>schedule_id</code> <code>str</code> <p>unique identifier for the schedule.</p> required <p>Returns:</p> Name Type Description <code>run</code> <p>information about the schedule along with a unique identifier.</p> Source code in <code>uptrain/framework/remote.py</code> <pre><code>def get_schedule(self, schedule_id: str):\n    \"\"\"Get the status of a schedule.\n\n    Args:\n        schedule_id: unique identifier for the schedule.\n\n    Returns:\n        run: information about the schedule along with a unique identifier.\n    \"\"\"\n    url = f\"{self.base_url}/schedule/{schedule_id}\"\n    response = self.client.get(url)\n    return raise_or_return(response)\n</code></pre>"},{"location":"framework/Remote/#uptrain.framework.remote.APIClient.get_schedule_results_all_dates","title":"<code>get_schedule_results_all_dates(schedule_id, check_name)</code>","text":"<p>Get the results of a schedule.</p> <p>Parameters:</p> Name Type Description Default <code>schedule_id</code> <code>str</code> <p>unique identifier for the schedule.</p> required <code>check_name</code> <code>str</code> <p>name of the check to get results for.</p> required <p>Returns:</p> Name Type Description <code>run</code> <code>list[dict]</code> <p>information about the schedule along with a unique identifier.</p> Source code in <code>uptrain/framework/remote.py</code> <pre><code>def get_schedule_results_all_dates(\n    self, schedule_id: str, check_name: str\n) -&gt; list[dict]:\n    \"\"\"Get the results of a schedule.\n\n    Args:\n        schedule_id: unique identifier for the schedule.\n        check_name: name of the check to get results for.\n\n    Returns:\n        run: information about the schedule along with a unique identifier.\n    \"\"\"\n    url = f\"{self.base_url}/schedule/{schedule_id}/results\"\n    params: dict = {\"check_name\": check_name}\n    response = self.client.get(url, params=params)\n    if not response.is_success:\n        logger.error(response.text)\n        response.raise_for_status()\n    else:\n        return pl.read_ndjson(response.content).to_dicts()\n</code></pre>"},{"location":"framework/Remote/#uptrain.framework.remote.APIClient.list_runs","title":"<code>list_runs(num=10, only_completed=False)</code>","text":"<p>List all the runs on the server.</p> <ul> <li>filter by scheduled/completed/in-process?</li> </ul> Source code in <code>uptrain/framework/remote.py</code> <pre><code>def list_runs(self, num: int = 10, only_completed: bool = False):\n    \"\"\"List all the runs on the server.\n\n    - filter by scheduled/completed/in-process?\n    \"\"\"\n    url = f\"{self.base_url}/runs\"\n    params: dict = {\"num\": num}\n    if only_completed:\n        params[\"status\"] = \"completed\"\n    response = self.client.get(url, params=params)\n    return raise_or_return(response)\n</code></pre>"},{"location":"framework/Remote/#uptrain.framework.remote.APIClient.list_schedules","title":"<code>list_schedules(num=10, active_only=True)</code>","text":"<p>List all the schedules on the server.</p> Source code in <code>uptrain/framework/remote.py</code> <pre><code>def list_schedules(self, num: int = 10, active_only: bool = True):\n    \"\"\"List all the schedules on the server.\"\"\"\n    url = f\"{self.base_url}/schedules\"\n    params: dict = {\"num\": num, \"active_only\": active_only}\n    response = self.client.get(url, params=params)\n    return raise_or_return(response)\n</code></pre>"},{"location":"framework/Remote/#uptrain.framework.remote.APIClient.log_and_evaluate","title":"<code>log_and_evaluate(project_name, data, checks, scenario_description=None, schema=None, metadata=None)</code>","text":"<p>Run an evaluation on the server and log the results. NOTE: This api is a bit different than the regular <code>evaluate</code> call.</p> <p>Parameters:</p> Name Type Description Default <code>project_name</code> <code>str</code> <p>Name of the project to evaluate on.</p> required <code>data</code> <code>Union[list[dict], DataFrame, DataFrame]</code> <p>Data to evaluate on. Either a Pandas DataFrame or a list of dicts.</p> required <code>checks</code> <code>list[Union[str, Evals, ParametricEval]]</code> <p>List of checks to evaluate on.</p> required <code>schema</code> <code>Union[DataSchema, dict[str, str], None]</code> <p>Schema of the data. Only required if the data attributes aren't typical (question, response, context).</p> <code>None</code> <code>metadata</code> <code>Optional[dict[str, Any]]</code> <p>Attributes to attach to this dataset. Useful for filtering and grouping in the UI.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>results</code> <p>List of dictionaries with each data point and corresponding evaluation results.</p> Source code in <code>uptrain/framework/remote.py</code> <pre><code>def log_and_evaluate(\n    self,\n    project_name: str,\n    data: t.Union[list[dict], pl.DataFrame, pd.DataFrame],\n    checks: list[t.Union[str, Evals, ParametricEval]],\n    scenario_description: t.Union[str, list[str], None] = None,\n    schema: t.Union[DataSchema, dict[str, str], None] = None,\n    metadata: t.Optional[dict[str, t.Any]] = None,\n):\n    \"\"\"Run an evaluation on the server and log the results.\n    NOTE: This api is a bit different than the regular `evaluate` call.\n\n    Args:\n        project_name: Name of the project to evaluate on.\n        data: Data to evaluate on. Either a Pandas DataFrame or a list of dicts.\n        checks: List of checks to evaluate on.\n        schema: Schema of the data. Only required if the data attributes aren't typical (question, response, context).\n        metadata: Attributes to attach to this dataset. Useful for filtering and grouping in the UI.\n\n    Returns:\n        results: List of dictionaries with each data point and corresponding evaluation results.\n    \"\"\"\n    url = f\"{self.base_url}/log_and_evaluate\"\n\n    if isinstance(data, pl.DataFrame):\n        data = data.to_dicts()\n    elif isinstance(data, pd.DataFrame):\n        data = data.to_dict(orient=\"records\")\n\n    if schema is None:\n        schema = DataSchema()\n    elif isinstance(schema, dict):\n        schema = DataSchema(**schema)\n\n    if metadata is None:\n        metadata = {}\n\n    checks = [Evals(m) if isinstance(m, str) else m for m in checks]\n    for m in checks:\n        assert isinstance(m, (Evals, ParametricEval))\n\n    req_attrs, ser_checks = set(), []\n    for m in checks:\n        if m in [Evals.SUB_QUERY_COMPLETENESS]:\n            req_attrs.update([schema.sub_questions, schema.question])\n        elif m in [Evals.CONTEXT_CONCISENESS]:\n            req_attrs.update(\n                [schema.question, schema.context, schema.concise_context]\n            )\n        elif m in [Evals.CONTEXT_RERANKING]:\n            req_attrs.update(\n                [schema.question, schema.context, schema.reranked_context]\n            )\n        elif m in [\n            Evals.FACTUAL_ACCURACY,\n            Evals.RESPONSE_COMPLETENESS_WRT_CONTEXT,\n            Evals.RESPONSE_CONSISTENCY,\n            Evals.CODE_HALLUCINATION,\n        ]:\n            req_attrs.update([schema.question, schema.context, schema.response])\n        elif m in [\n            Evals.VALID_RESPONSE,\n            Evals.RESPONSE_RELEVANCE,\n            Evals.RESPONSE_COMPLETENESS,\n            Evals.RESPONSE_CONCISENESS,\n            Evals.PROMPT_INJECTION,\n        ]:\n            req_attrs.update([schema.question, schema.response])\n        elif m in [Evals.CONTEXT_RELEVANCE]:\n            req_attrs.update([schema.question, schema.context])\n        elif (\n            m in [Evals.CRITIQUE_LANGUAGE]\n            or isinstance(m, CritiqueTone)\n            or isinstance(m, GuidelineAdherence)\n        ):\n            req_attrs.update([schema.response])\n        elif m in [\n            Evals.RESPONSE_ALIGNMENT_WITH_SCENARIO,\n            Evals.RESPONSE_SINCERITY_WITH_SCENARIO,\n        ]:\n            req_attrs.update(\n                [\n                    schema.question,\n                    schema.response,\n                    schema.scenario,\n                    schema.objective,\n                ]\n            )\n        elif isinstance(m, ResponseMatching):\n            req_attrs.update(\n                [schema.question, schema.response, schema.ground_truth]\n            )\n        elif isinstance(m, ConversationSatisfaction):\n            req_attrs.update([schema.conversation])\n        elif isinstance(m, JailbreakDetection):\n            req_attrs.update([schema.question])\n\n        if isinstance(m, ParametricEval):\n            dictm = m.dict()\n            dictm.update({\"scenario_description\": scenario_description})\n            ser_checks.append({\"check_name\": m.__class__.__name__, **dictm})\n        elif isinstance(m, Evals):\n            dictm = {\"scenario_description\": scenario_description}\n            ser_checks.append({\"check_name\": m.value, **dictm})\n        else:\n            raise ValueError(f\"Invalid metric: {m}\")\n    for idx, row in enumerate(data):\n        if not req_attrs.issubset(row.keys()):\n            raise ValueError(\n                f\"Row {idx} is missing required all required attributes for evaluation: {req_attrs}\"\n            )\n\n    # send in chunks of 50, so the connection doesn't time out waiting for the server\n    results = []\n    NUM_TRIES, BATCH_SIZE = 3, 50\n    for i in range(0, len(data), BATCH_SIZE):\n        response_json = None\n        for try_num in range(NUM_TRIES):\n            try:\n                logger.info(\n                    f\"Sending evaluation request for rows {i} to &lt;{i+BATCH_SIZE} to the Uptrain server\"\n                )\n                response = self.client.post(\n                    url,\n                    json={\n                        \"data\": data[i : i + BATCH_SIZE],\n                        \"checks\": ser_checks,\n                        \"metadata\": {\n                            \"project\": project_name,\n                            \"schema\": schema.dict(),\n                            **metadata,\n                            \"uptrain_settings\": self.settings.dict(),\n                        },\n                    },\n                )\n                response_json = raise_or_return(response)\n                break\n            except Exception as e:\n                logger.info(\"Retrying evaluation request\")\n                if try_num == NUM_TRIES - 1:\n                    logger.error(f\"Evaluation failed with error: {e}\")\n                    raise e\n\n        if response_json is not None:\n            results.extend(response_json)\n\n    return results\n</code></pre>"},{"location":"framework/Remote/#uptrain.framework.remote.APIClient.perform_root_cause_analysis","title":"<code>perform_root_cause_analysis(project_name, data, rca_template, scenario_description=None, schema=None, metadata=None)</code>","text":"<p>Perform root cause analysis on the server.</p> <p>Parameters:</p> Name Type Description Default <code>project_name</code> <code>str</code> <p>Name of the project to run root cause analysis on.</p> required <code>data</code> <code>Union[list[dict], DataFrame, DataFrame]</code> <p>Data to do rca on. Either a Pandas DataFrame or a list of dicts.</p> required <code>rca_template</code> <code>RcaTemplate</code> <p>rca template to run.</p> required <code>schema</code> <code>Union[DataSchema, dict[str, str], None]</code> <p>Schema of the data. Only required if the data attributes aren't typical (question, response, context).</p> <code>None</code> <code>metadata</code> <code>Optional[dict[str, Any]]</code> <p>Attributes to attach to this dataset. Useful for filtering and grouping in the UI.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>results</code> <p>List of dictionaries with each data point and corresponding error modes.</p> Source code in <code>uptrain/framework/remote.py</code> <pre><code>def perform_root_cause_analysis(\n    self,\n    project_name: str,\n    data: t.Union[list[dict], pl.DataFrame, pd.DataFrame],\n    rca_template: RcaTemplate,\n    scenario_description: t.Optional[str] = None,\n    schema: t.Union[DataSchema, dict[str, str], None] = None,\n    metadata: t.Optional[dict[str, t.Any]] = None,\n):\n    \"\"\"Perform root cause analysis on the server.\n\n    Args:\n        project_name: Name of the project to run root cause analysis on.\n        data: Data to do rca on. Either a Pandas DataFrame or a list of dicts.\n        rca_template: rca template to run.\n        schema: Schema of the data. Only required if the data attributes aren't typical (question, response, context).\n        metadata: Attributes to attach to this dataset. Useful for filtering and grouping in the UI.\n\n    Returns:\n        results: List of dictionaries with each data point and corresponding error modes.\n    \"\"\"\n\n    url = f\"{self.base_url}/perform_root_cause_analysis\"\n\n    if isinstance(data, pl.DataFrame):\n        data = data.to_dicts()\n    elif isinstance(data, pd.DataFrame):\n        data = data.to_dict(orient=\"records\")\n\n    if schema is None:\n        schema = DataSchema()\n    elif isinstance(schema, dict):\n        schema = DataSchema(**schema)\n\n    if metadata is None:\n        metadata = {}\n    metadata[\"internal_call\"] = metadata.get(\"internal_call\", False)\n\n    req_attrs, ser_templates = set(), []\n    if rca_template == RcaTemplate.RAG_WITH_CITATION:\n        req_attrs.update(\n            [schema.question, schema.response, schema.context, schema.cited_context]\n        )\n    else:\n        raise Exception(\"RCA Template not supported yet\")\n\n    dictn = {\"scenario_description\": scenario_description}\n    ser_templates.append({\"rca_template_name\": rca_template.value, **dictn})\n\n    for idx, row in enumerate(data):\n        if not req_attrs.issubset(row.keys()):\n            raise ValueError(\n                f\"Row {idx} is missing required all required attributes for evaluation: {req_attrs}\"\n            )\n\n    # send in chunks of 50, so the connection doesn't time out waiting for the server\n    results = []\n    NUM_TRIES, BATCH_SIZE = 3, 50\n    for i in range(0, len(data), BATCH_SIZE):\n        response_json = None\n        for try_num in range(NUM_TRIES):\n            try:\n                logger.info(\n                    f\"Sending root cause analysis request for rows {i} to &lt;{i+BATCH_SIZE} to the Uptrain server\"\n                )\n                response = self.client.post(\n                    url,\n                    json={\n                        \"data\": data[i : i + BATCH_SIZE],\n                        \"rca_templates\": ser_templates,\n                        \"metadata\": {\n                            \"project\": project_name,\n                            \"schema\": schema.dict(),\n                            **metadata,\n                            \"uptrain_settings\": self.settings.dict(),\n                        },\n                    },\n                )\n                response_json = raise_or_return(response)\n                break\n            except Exception as e:\n                logger.info(\"Retrying root cause analysis request\")\n                if try_num == NUM_TRIES - 1:\n                    logger.error(f\"Evaluation failed with error: {e}\")\n                    raise e\n\n        if response_json is not None:\n            results.extend(response_json)\n\n    return results\n</code></pre>"},{"location":"framework/Remote/#uptrain.framework.remote.APIClient.remove_schedule","title":"<code>remove_schedule(schedule_id)</code>","text":"<p>Remove a schedule.</p> <p>Parameters:</p> Name Type Description Default <code>schedule_id</code> <code>str</code> <p>unique identifier for the schedule.</p> required <p>Returns:</p> Name Type Description <code>run</code> <p>information about the schedule along with a unique identifier.</p> Source code in <code>uptrain/framework/remote.py</code> <pre><code>def remove_schedule(self, schedule_id: str):\n    \"\"\"Remove a schedule.\n\n    Args:\n        schedule_id: unique identifier for the schedule.\n\n    Returns:\n        run: information about the schedule along with a unique identifier.\n    \"\"\"\n    url = f\"{self.base_url}/schedule/{schedule_id}\"\n    response = self.client.delete(url)\n    return raise_or_return(response)\n</code></pre>"},{"location":"framework/Remote/#uptrain.framework.remote.APIClient.rerun_schedule","title":"<code>rerun_schedule(schedule_id, start_on, end_on=None)</code>","text":"<p>Rerun a schedule. - New checks added to the checkset are run for all dates - Existing checks are run only on the failed rows.</p> <p>Parameters:</p> Name Type Description Default <code>schedule_id</code> <code>str</code> <p>unique identifier for the run.</p> required <code>start_on</code> <code>str</code> <p>date to start the reruns on</p> required <code>end_on</code> <code>Optional[str]</code> <p>date to end the reruns on</p> <code>None</code> <p>Returns:</p> Name Type Description <code>run</code> <p>information about the schedule along with a unique identifier.</p> Source code in <code>uptrain/framework/remote.py</code> <pre><code>def rerun_schedule(\n    self, schedule_id: str, start_on: str, end_on: t.Optional[str] = None\n):\n    \"\"\"Rerun a schedule.\n    - New checks added to the checkset are run for all dates\n    - Existing checks are run only on the failed rows.\n\n    Args:\n        schedule_id: unique identifier for the run.\n        start_on: date to start the reruns on\n        end_on: date to end the reruns on\n\n    Returns:\n        run: information about the schedule along with a unique identifier.\n    \"\"\"\n    url = f\"{self.base_url}/schedule/{schedule_id}/rerun\"\n    params = {\"start_on\": start_on}\n    if end_on is not None:\n        params[\"end_on\"] = end_on\n    response = self.client.put(url, params=params)\n    return raise_or_return(response)\n</code></pre>"},{"location":"framework/Remote/#uptrain.framework.remote.APIClientWithoutAuth","title":"<code>APIClientWithoutAuth</code>","text":"Source code in <code>uptrain/framework/remote.py</code> <pre><code>class APIClientWithoutAuth:\n    base_url: str\n    client: httpx.Client\n\n    def __init__(self, settings: Settings = None) -&gt; None:\n        if settings is None:\n            settings = Settings()\n\n        server_url = settings.check_and_get(\"uptrain_server_url\")\n        self.base_url = server_url.rstrip(\"/\") + \"/api/open\"\n        self.client = httpx.Client(\n            timeout=httpx.Timeout(7200, connect=5),\n        )\n\n    def evaluate(\n        self,\n        data: list[dict],\n        checks: list[t.Union[Evals, ParametricEval, dict]],\n        metadata: dict,\n    ):\n        \"\"\"Run an evaluation on the UpTrain server (Doesn't require UpTrain API Key).\"\"\"\n\n        url = f\"{self.base_url}/evaluate_no_auth\"\n        response_json = []\n        try:\n            response = self.client.post(\n                url,\n                json={\"data\": data, \"checks\": checks, \"metadata\": metadata},\n            )\n            response_json = raise_or_return(response)\n        except Exception as e:\n            logger.error(f\"Evaluation failed with error: {e}\")\n            raise e\n\n        return response_json\n</code></pre>"},{"location":"framework/Remote/#uptrain.framework.remote.APIClientWithoutAuth.evaluate","title":"<code>evaluate(data, checks, metadata)</code>","text":"<p>Run an evaluation on the UpTrain server (Doesn't require UpTrain API Key).</p> Source code in <code>uptrain/framework/remote.py</code> <pre><code>def evaluate(\n    self,\n    data: list[dict],\n    checks: list[t.Union[Evals, ParametricEval, dict]],\n    metadata: dict,\n):\n    \"\"\"Run an evaluation on the UpTrain server (Doesn't require UpTrain API Key).\"\"\"\n\n    url = f\"{self.base_url}/evaluate_no_auth\"\n    response_json = []\n    try:\n        response = self.client.post(\n            url,\n            json={\"data\": data, \"checks\": checks, \"metadata\": metadata},\n        )\n        response_json = raise_or_return(response)\n    except Exception as e:\n        logger.error(f\"Evaluation failed with error: {e}\")\n        raise e\n\n    return response_json\n</code></pre>"},{"location":"framework/Signal/","title":"Signal","text":"<p>Signal Class wrapper for any dataframe column</p> <p>Attributes:</p> Name Type Description <code>col_name</code> <code>str</code> <p>Column Name</p> <code>identifier</code> <code>str</code> <p>Used as an identifier for printing</p> <p>Methods:</p> Name Description <code>run</code> <p>Evaluates on given data</p> Source code in <code>uptrain/framework/signal.py</code> <pre><code>class Signal:\n    \"\"\"\n    Signal Class wrapper for any dataframe column\n\n    Attributes:\n        col_name (str): Column Name\n        identifier (str): Used as an identifier for printing\n\n    Methods:\n        run(self, data): Evaluates on given data\n\n    \"\"\"\n\n    def __init__(self, col_name=\"\"):\n        self.col_name = col_name\n        self.identifier = self.col_name\n        self.children = []\n\n    def __json__(self):\n        return {\"col_name\": str(self)}\n\n    def run(self, data: pl.DataFrame) -&gt; pl.Series:\n        return data[self.col_name].to_numpy()\n\n    def __invert__(self):\n        return InvertSignal(self)\n\n    def __add__(self, other):\n        return AddSignal(self, other)\n\n    def __mul__(self, other):\n        return MulSignal(self, other)\n\n    def __and__(self, other):\n        return AndSignal(self, other)\n\n    def __or__(self, other):\n        return OrSignal(self, other)\n\n    def __xor__(self, other):\n        return XorSignal(self, other)\n\n    def __gt__(self, other):\n        return GreaterThanSignal(self, other)\n\n    def __lt__(self, other):\n        return LessThanSignal(self, other)\n\n    def __eq__(self, other):\n        return EqualToSignal(self, other)\n\n    def __ge__(self, other):\n        return GreaterEqualToSignal(self, other)\n\n    def __le__(self, other):\n        return LessEqualToSignal(self, other)\n\n    def __ne__(self, other):\n        return NotEqualToSignal(self, other)\n\n    def __str__(self):\n        txt = str(self.identifier)\n        if len(self.children):\n            txt += \"(\"\n            txt += \",\".join([str(x) for x in self.children])\n            txt += \")\"\n        return txt\n</code></pre>"},{"location":"integrations/EvalLlamaIndex/","title":"EvalLlamaIndex","text":"Source code in <code>uptrain/integrations/llama_index.py</code> <pre><code>class EvalLlamaIndex:\n    query_engine: BaseQueryEngine\n    client: t.Union[EvalLLM, APIClient]\n\n    def __init__(self, settings: Settings, query_engine: BaseQueryEngine) -&gt; None:\n        if settings is None:\n            raise Exception(\n                \"Please provide OpenAI API Key or Uptrain API Key in settings\"\n            )\n        if not isinstance(query_engine, BaseQueryEngine):\n            raise Exception(\"Please provide Query Engine for the evaluation\")\n        self.query_engine = query_engine\n\n        if settings.uptrain_access_token is not None:\n            self.client = APIClient(settings)\n        elif settings.check_and_get(\"openai_api_key\"):\n            self.client = EvalLLM(settings)\n\n    def evaluate(\n        self,\n        data: t.Union[list[dict], pl.DataFrame],\n        checks: list[t.Union[str, Evals, ParametricEval]],\n        project_name: str = None,\n        schema: t.Union[DataSchema, dict[str, str], None] = None,\n        metadata: t.Optional[dict[str, str]] = None,\n    ):\n        try:\n            from llama_index.async_utils import run_async_tasks\n        except ImportError:\n            raise ImportError(\n                \"llama_index must be installed to use this function. \"\n                \"Install it with `pip install llama_index`.\"\n            )\n        \"\"\"\n        Run evaluation of llama_index QueryEngine with different Evals\n\n        NOTE: This api doesn't log any data.\n        Args:\n            project_name: Name of the project to evaluate on.\n            client: EvalLLM or APIClient object used for the evaluation using user's openai keys or Uptrain API key.\n            data: Data to evaluate on. Either a Polars DataFrame or a list of dicts.\n            checks: List of checks to evaluate on.\n            schema: Schema of the data. Only required if the data attributes aren't typical (question, response, context).\n            metadata: Attributes to attach to this dataset. Useful for filtering and grouping in the UI.\n        Returns:\n            results: List of dictionaries with each data point and corresponding evaluation results.\n        \"\"\"\n\n        if isinstance(data, pl.DataFrame):\n            data = data.to_dicts()\n        import nest_asyncio\n\n        nest_asyncio.apply()\n\n        if schema is None:\n            schema = DataSchema()\n        elif isinstance(schema, dict):\n            schema = DataSchema(**schema)\n\n        responses = run_async_tasks(\n            [\n                self.query_engine.aquery(data[i][schema.question])\n                for i in range(len(data))\n            ]\n        )\n\n        for index, r in enumerate(responses):\n            data[index][schema.response] = r.response\n            data[index][schema.context] = \"\\n\".join(\n                [c.node.get_content() for c in r.source_nodes]\n            )\n\n        if isinstance(self.client, EvalLLM):\n            results = self.client.evaluate(\n                data=data, checks=checks, schema=schema, metadata=metadata\n            )\n        elif isinstance(self.client, APIClient):\n            results = self.client.log_and_evaluate(\n                project_name=project_name,\n                data=data,\n                checks=checks,\n                schema=schema,\n                metadata=metadata,\n            )\n        return results\n</code></pre>"},{"location":"operators/Accuracy/","title":"Accuracy","text":"<p>             Bases: <code>ColumnOp</code></p> <p>Operator for computing accuracy measures between predicted values and ground truth values.</p> <p>Attributes:</p> Name Type Description <code>kind</code> <code>Literal['NOT_EQUAL', 'ABS_ERROR']</code> <p>The type of accuracy measure.</p> <code>col_in_prediction</code> <code>str</code> <p>The name of the column containing the predicted values.</p> <code>col_in_ground_truth</code> <code>str</code> <p>The name of the column containing the ground truth values.</p> <code>col_out</code> <code>str</code> <p>The name of the output column containing the accuracy scores.</p> Example <pre><code>from uptrain.operators import Accuracy\nfrom uptrain.operators import CsvReader\n\n# Create an instance of the Accuracy operator\nop = Accuracy(\n        kind=\"NOT_EQUAL\",\n        col_in_prediction=\"prediction\",\n        col_in_ground_truth=\"ground_truth\"\n    )\n\n# Set up the operator\nop.setup()\n\n# Run the operator on the input data\ninput_data = pl.DataFrame(...)\naccuracy_scores = op.run(input_data)[\"output\"]\n\n# Print the accuracy scores\nprint(accuracy_scores)\n</code></pre> Output <pre><code>shape: (3,)\nSeries: '_col_0' [bool]\n[\n        true\n        false\n        true\n]\n</code></pre> Source code in <code>uptrain/operators/metrics.py</code> <pre><code>@register_op\nclass Accuracy(ColumnOp):\n    \"\"\"\n    Operator for computing accuracy measures between predicted values and ground truth values.\n\n    Attributes:\n        kind (Literal[\"NOT_EQUAL\", \"ABS_ERROR\"]): The type of accuracy measure.\n        col_in_prediction (str): The name of the column containing the predicted values.\n        col_in_ground_truth (str): The name of the column containing the ground truth values.\n        col_out (str): The name of the output column containing the accuracy scores.\n\n    Example:\n        ```\n        from uptrain.operators import Accuracy\n        from uptrain.operators import CsvReader\n\n        # Create an instance of the Accuracy operator\n        op = Accuracy(\n                kind=\"NOT_EQUAL\",\n                col_in_prediction=\"prediction\",\n                col_in_ground_truth=\"ground_truth\"\n            )\n\n        # Set up the operator\n        op.setup()\n\n        # Run the operator on the input data\n        input_data = pl.DataFrame(...)\n        accuracy_scores = op.run(input_data)[\"output\"]\n\n        # Print the accuracy scores\n        print(accuracy_scores)\n        ```\n\n    Output:\n        ```\n        shape: (3,)\n        Series: '_col_0' [bool]\n        [\n                true\n                false\n                true\n        ]\n        ```\n\n    \"\"\"\n\n    kind: t.Literal[\"NOT_EQUAL\", \"ABS_ERROR\"]\n    col_in_prediction: str = \"prediction\"\n    col_in_ground_truth: str = \"ground_truth\"\n    col_out: str = \"accuracy\"\n\n    def setup(self, settings: Settings):\n        return self\n\n    def run(self, data: pl.DataFrame) -&gt; TYPE_TABLE_OUTPUT:\n        preds = np.array(data.get_column(self.col_in_prediction))\n        gts = np.array(data.get_column(self.col_in_ground_truth))\n\n        if self.kind == \"NOT_EQUAL\":\n            acc = np.not_equal(preds, gts)\n        else:\n            acc = np.abs(preds - gts)\n        return {\"output\": data.with_columns([pl.Series(self.col_out, acc)])}\n</code></pre>"},{"location":"operators/ColumnComparison/","title":"ColumnComparison","text":"<p>             Bases: <code>ColumnOp</code></p> <p>Column operation to add a new boolean column which represents if the given columns are row-wise same or not</p> <p>Attributes:</p> Name Type Description <code>col_in_1</code> <code>str</code> <p>The name of the 1st input column to be compared row-wise.</p> <code>col_in_2</code> <code>str</code> <p>The name of the 2nd input column to be compared row-wise.</p> <code>col_out</code> <code>str</code> <p>The name of the output boolean column</p> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary containing the input DataFrame.</p> Example <pre><code>from uptrain.operators import ColumnComparison\ndf = pl.DataFrame({\n    \"column1\": [1, 2, 3],\n    \"column2\": [1, 2, 1],\n    \"column3\": [2, 2, 4]\n})\n\n# Create an instance of the ColumnComparison class\ncompare_op = ColumnComparison(\n                col_in_1=\"column1\",\n                col_in_2=\"column2\",\n                col_out=\"is_equal\"\n            )\n\n# Run the compare operation\noutput_df = compare_op.run(df)[\"output\"]\n\n# Print the output DataFrame\nprint(output_df)\n</code></pre> Output <pre><code>shape: (3, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 column1 \u2506 column2 \u2506 column3 \u2506is_equal \u2502\n\u2502 ---     \u2506 ---     \u2506 ---     \u2506 ---     \u2502\n\u2502 i64     \u2506 i64     \u2506 i64     \u2506 bool    \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1       \u2506 1       \u2506 2       \u2506 True    \u2502\n\u2502 2       \u2506 2       \u2506 2       \u2506 True    \u2502\n\u2502 3       \u2506 1       \u2506 4       \u2506 False   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> Source code in <code>uptrain/operators/table.py</code> <pre><code>@register_op\nclass ColumnComparison(ColumnOp):\n    \"\"\"\n    Column operation to add a new boolean column which represents if the given columns are row-wise same or not\n\n    Attributes:\n        col_in_1 (str): The name of the 1st input column to be compared row-wise.\n        col_in_2 (str): The name of the 2nd input column to be compared row-wise.\n        col_out (str): The name of the output boolean column\n\n    Returns:\n        dict: A dictionary containing the input DataFrame.\n\n    Example:\n        ```\n        from uptrain.operators import ColumnComparison\n        df = pl.DataFrame({\n            \"column1\": [1, 2, 3],\n            \"column2\": [1, 2, 1],\n            \"column3\": [2, 2, 4]\n        })\n\n        # Create an instance of the ColumnComparison class\n        compare_op = ColumnComparison(\n                        col_in_1=\"column1\",\n                        col_in_2=\"column2\",\n                        col_out=\"is_equal\"\n                    )\n\n        # Run the compare operation\n        output_df = compare_op.run(df)[\"output\"]\n\n        # Print the output DataFrame\n        print(output_df)\n        ```\n\n    Output:\n        ```\n        shape: (3, 2)\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502 column1 \u2506 column2 \u2506 column3 \u2506is_equal \u2502\n        \u2502 ---     \u2506 ---     \u2506 ---     \u2506 ---     \u2502\n        \u2502 i64     \u2506 i64     \u2506 i64     \u2506 bool    \u2502\n        \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n        \u2502 1       \u2506 1       \u2506 2       \u2506 True    \u2502\n        \u2502 2       \u2506 2       \u2506 2       \u2506 True    \u2502\n        \u2502 3       \u2506 1       \u2506 4       \u2506 False   \u2502\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n        ```\n\n    \"\"\"\n\n    col_in_1: str\n    col_in_2: str\n    col_out: str\n\n    def setup(self, settings: Settings):\n        return self\n\n    def run(self, data: pl.DataFrame) -&gt; TYPE_TABLE_OUTPUT:\n        return {\n            \"output\": data.with_columns(\n                data.select(pl.col(self.col_in_1) == pl.col(self.col_in_2))[\n                    \"Output\"\n                ].alias(self.col_out)\n            )\n        }\n</code></pre>"},{"location":"operators/ColumnExpand/","title":"ColumnExpand","text":"<p>             Bases: <code>ColumnOp</code></p> <p>Table operation to return the input DataFrame as it is without any modifications.</p> <p>Attributes:</p> Name Type Description <code>col_out_names</code> <code>list[str]</code> <p>The names of the output columns. Must be the same length as <code>col_vals</code>.</p> <code>col_vals</code> <code>list[Any]</code> <p>The values for the output columns. Must be the same length as <code>col_out_names</code>.</p> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary containing the input DataFrame.</p> Example <pre><code>from uptrain.operators import ColumnExpand\ndf = pl.DataFrame({\n    \"column1\": [1, 2, 3],\n    \"column2\": [\"A\", \"B\", \"C\"]\n})\n\n# Create an instance of the ColumnExpand class\nexpand_op = ColumnExpand(\n                col_out_names=[\"column1\", \"column2\"],\n                col_vals=[df[\"column1\"], df[\"column2\"]]\n            )\n\n# Run the expand operation\noutput_df = expand_op.run(df)[\"output\"]\n\n# Print the output DataFrame\nprint(output_df)\n</code></pre> Output <pre><code>shape: (3, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 column1 \u2506 column2 \u2502\n\u2502 ---     \u2506 ---     \u2502\n\u2502 i64     \u2506 str     \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1       \u2506 A       \u2502\n\u2502 2       \u2506 B       \u2502\n\u2502 3       \u2506 C       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> Source code in <code>uptrain/operators/table.py</code> <pre><code>@register_op\nclass ColumnExpand(ColumnOp):\n    \"\"\"\n    Table operation to return the input DataFrame as it is without any modifications.\n\n    Attributes:\n        col_out_names (list[str]): The names of the output columns. Must be the same length as `col_vals`.\n        col_vals (list[Any]): The values for the output columns. Must be the same length as `col_out_names`.\n\n    Returns:\n        dict: A dictionary containing the input DataFrame.\n\n    Example:\n        ```\n        from uptrain.operators import ColumnExpand\n        df = pl.DataFrame({\n            \"column1\": [1, 2, 3],\n            \"column2\": [\"A\", \"B\", \"C\"]\n        })\n\n        # Create an instance of the ColumnExpand class\n        expand_op = ColumnExpand(\n                        col_out_names=[\"column1\", \"column2\"],\n                        col_vals=[df[\"column1\"], df[\"column2\"]]\n                    )\n\n        # Run the expand operation\n        output_df = expand_op.run(df)[\"output\"]\n\n        # Print the output DataFrame\n        print(output_df)\n        ```\n\n    Output:\n        ```\n        shape: (3, 2)\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502 column1 \u2506 column2 \u2502\n        \u2502 ---     \u2506 ---     \u2502\n        \u2502 i64     \u2506 str     \u2502\n        \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n        \u2502 1       \u2506 A       \u2502\n        \u2502 2       \u2506 B       \u2502\n        \u2502 3       \u2506 C       \u2502\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n        ```\n\n    \"\"\"\n\n    col_out_names: list[str]\n    col_vals: list[t.Any]\n\n    def setup(self, settings: Settings):\n        assert len(self.col_out_names) == len(self.col_vals)\n        return self\n\n    def run(self, data: pl.DataFrame) -&gt; TYPE_TABLE_OUTPUT:\n        out = data.with_columns(\n            [\n                pl.lit(self.col_vals[idx]).alias(self.col_out_names[idx])\n                for idx in range(len(self.col_out_names))\n            ]\n        )\n        return {\"output\": out}\n</code></pre>"},{"location":"operators/ConceptDrift/","title":"ConceptDrift","text":"<p>             Bases: <code>ColumnOp</code></p> <p>Operator for detecting concept drift using the DDM (Drift Detection Method) or ADWIN (Adaptive Windowing) algorithm.</p> <p>Attributes:</p> Name Type Description <code>algorithm</code> <code>Literal['DDM', 'ADWIN']</code> <p>The algorithm to use for concept drift detection.</p> <code>params</code> <code>Union[ParamsDDM, ParamsADWIN]</code> <p>The parameters for the selected algorithm.</p> <code>col_in_measure</code> <code>str</code> <p>The name of the column in the input data representing the metric to measure concept drift.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the specified algorithm does not match the type of the parameters.</p> Example <pre><code>import polars as pl\nfrom uptrain.operators import ParamsDDM, ConceptDrift\n\n# Create an instance of the ParamsDDM class with the parameters\n\nparams_ddm = ParamsDDM(\n                warm_start=500,\n                warn_threshold=2.0,\n                alarm_threshold=3.0\n            )\n\n# Create an instance of the ConceptDrift operator\nop = ConceptDrift(\n        algorithm=\"DDM\",\n        params=params_ddm,\n        col_in_measure=\"metric\"\n    )\n\n# Set up the operator\nop.setup()\n\n# Run the operator on the input data\ninput_data = pl.DataFrame(...)\noutput = op.run(input_data)[\"extra\"]\n\n# Check the detected concept drift information\nif output[\"alert_info\"] is not None:\n    print(\"Counter:\", output[\"alert_info\"][\"counter\"])\n</code></pre> Output <pre><code>INFO     | uptrain.operators.drift:run:181 - Drift detected using DDM!\nCounter: 129466\n</code></pre> Source code in <code>uptrain/operators/drift.py</code> <pre><code>@register_op\nclass ConceptDrift(ColumnOp):\n    \"\"\"\n    Operator for detecting concept drift using the DDM (Drift Detection Method) or ADWIN (Adaptive Windowing) algorithm.\n\n    Attributes:\n        algorithm (Literal[\"DDM\", \"ADWIN\"]): The algorithm to use for concept drift detection.\n        params (Union[ParamsDDM, ParamsADWIN]): The parameters for the selected algorithm.\n        col_in_measure (str): The name of the column in the input data representing the metric to measure concept drift.\n\n    Raises:\n        ValueError: If the specified algorithm does not match the type of the parameters.\n\n    Example:\n        ```py\n        import polars as pl\n        from uptrain.operators import ParamsDDM, ConceptDrift\n\n        # Create an instance of the ParamsDDM class with the parameters\n\n        params_ddm = ParamsDDM(\n                        warm_start=500,\n                        warn_threshold=2.0,\n                        alarm_threshold=3.0\n                    )\n\n        # Create an instance of the ConceptDrift operator\n        op = ConceptDrift(\n                algorithm=\"DDM\",\n                params=params_ddm,\n                col_in_measure=\"metric\"\n            )\n\n        # Set up the operator\n        op.setup()\n\n        # Run the operator on the input data\n        input_data = pl.DataFrame(...)\n        output = op.run(input_data)[\"extra\"]\n\n        # Check the detected concept drift information\n        if output[\"alert_info\"] is not None:\n            print(\"Counter:\", output[\"alert_info\"][\"counter\"])\n        ```\n\n    Output:\n        ```\n        INFO     | uptrain.operators.drift:run:181 - Drift detected using DDM!\n        Counter: 129466\n        ```\n    \"\"\"\n\n    algorithm: t.Literal[\"DDM\", \"ADWIN\"]\n    params: t.Union[ParamsDDM, ParamsADWIN]\n    col_in_measure: str = \"metric\"\n\n    @root_validator\n    def _check_params(cls, values):\n        \"\"\"\n        Check if the parameters are valid for the specified algorithm.\n\n        \"\"\"\n        algo = values[\"algorithm\"]\n        params = values[\"params\"]\n        if algo == \"DDM\" and not isinstance(params, ParamsDDM):\n            raise ValueError(\n                f\"Expected params to be of type {ParamsDDM} for algorithm - DDM\"\n            )\n        elif algo == \"ADWIN\" and not isinstance(params, ParamsADWIN):\n            raise ValueError(\n                f\"Expected params to be of type {ParamsADWIN} for algorithm - ADWIN\"\n            )\n        return values\n\n    def setup(self):\n        if self.algorithm == \"DDM\":\n            self._algo_obj = drift.binary.DDM(**self.params.dict())  # type: ignore\n        elif self.algorithm == \"ADWIN\":\n            self._algo_obj = drift.ADWIN(**self.params.dict())  # type: ignore\n        self._counter = 0\n        self._avg_accuracy = 0.0\n        self._cuml_accuracy = 0.0\n        self._alert_info = None\n        return self\n\n    def run(self, data: pl.DataFrame) -&gt; TYPE_TABLE_OUTPUT:\n        ser = data.get_column(self.col_in_measure)\n\n        for val in ser:\n            self._algo_obj.update(val)\n            if self._algo_obj.drift_detected and self._alert_info is None:\n                msg = f\"Drift detected using {self.algorithm}!\"\n                self._alert_info = {\"counter\": self._counter, \"msg\": msg}\n                logger.info(msg)\n\n            self._counter += 1\n            self._cuml_accuracy += val\n\n        self._avg_accuracy = (\n            self._cuml_accuracy / self._counter if self._counter &gt; 0 else 0.0\n        )\n        return {\n            \"output\": None,\n            \"extra\": {\n                \"counter\": self._counter,\n                \"avg_accuracy\": self._avg_accuracy,\n                \"alert_info\": self._alert_info,\n            },\n        }\n</code></pre>"},{"location":"operators/CosineSimilarity/","title":"CosineSimilarity","text":"<p>             Bases: <code>ColumnOp</code></p> <p>Column operation to calculate the cosine similarity between two vectors representing text.</p> <p>Attributes:</p> Name Type Description <code>col_in_vector_1</code> <code>str</code> <p>The name of the column containing the first vector.</p> <code>col_in_vector_2</code> <code>str</code> <p>The name of the column containing the second vector.</p> <code>col_out</code> <code>str</code> <p>The name of the output column containing the cosine similarity scores.</p> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary containing the cosine similarity scores.</p> Example <pre><code>import polars as pl\nimport numpy as np\nfrom uptrain.operators import CosineSimilarity\n\n# Create a DataFrame\ndf = pl.DataFrame({\n    \"vector_1\": [np.array([0.1, 0.2, 0.3]), np.array([0.4, 0.5, 0.6])],\n    \"vector_2\": [np.array([0.7, 0.8, 0.9]), np.array([0.2, 0.3, 0.4])]\n})\n\n# Create an instance of the CosineSimilarity class\nsimilarity_op = CosineSimilarity(col_in_vector_1=\"vector_1\", col_in_vector_2=\"vector_2\")\n\n# Calculate the cosine similarity between the two vectors\nresult = similarity_op.run(df)\nsimilarity_scores = result[\"output\"]\n\n# Print the similarity scores\nprint(similarity_scores)\n</code></pre> Output <pre><code>shape: (2,)\nSeries: '_col_0' [f64]\n[\n        1.861259\n        0.288437\n]\n</code></pre> Source code in <code>uptrain/operators/similarity.py</code> <pre><code>@register_op\nclass CosineSimilarity(ColumnOp):\n    \"\"\"\n    Column operation to calculate the cosine similarity between two vectors representing text.\n\n    Attributes:\n        col_in_vector_1 (str): The name of the column containing the first vector.\n        col_in_vector_2 (str): The name of the column containing the second vector.\n        col_out (str): The name of the output column containing the cosine similarity scores.\n\n    Returns:\n        dict: A dictionary containing the cosine similarity scores.\n\n    Example:\n        ```\n        import polars as pl\n        import numpy as np\n        from uptrain.operators import CosineSimilarity\n\n        # Create a DataFrame\n        df = pl.DataFrame({\n            \"vector_1\": [np.array([0.1, 0.2, 0.3]), np.array([0.4, 0.5, 0.6])],\n            \"vector_2\": [np.array([0.7, 0.8, 0.9]), np.array([0.2, 0.3, 0.4])]\n        })\n\n        # Create an instance of the CosineSimilarity class\n        similarity_op = CosineSimilarity(col_in_vector_1=\"vector_1\", col_in_vector_2=\"vector_2\")\n\n        # Calculate the cosine similarity between the two vectors\n        result = similarity_op.run(df)\n        similarity_scores = result[\"output\"]\n\n        # Print the similarity scores\n        print(similarity_scores)\n        ```\n\n    Output:\n        ```\n        shape: (2,)\n        Series: '_col_0' [f64]\n        [\n                1.861259\n                0.288437\n        ]\n        ```\n\n    \"\"\"\n\n    col_in_vector_1: str\n    col_in_vector_2: str\n    col_out: str = \"cosine_similarity\"\n\n    def setup(self, settings: Settings):\n        return self\n\n    def run(self, data: pl.DataFrame) -&gt; TYPE_TABLE_OUTPUT:\n        vector_1 = data.get_column(self.col_in_vector_1)\n        vector_2 = data.get_column(self.col_in_vector_2)\n\n        results = []\n        for i in range(len(vector_1)):\n            v1 = np.array(vector_1[i])\n            v2 = np.array(vector_2[i])\n            similarity_score = np.dot(v1, v2) / np.linalg.norm(v1) * np.linalg.norm(v2)\n            results.append(similarity_score)\n\n        return {\"output\": data.with_columns(pl.Series(results).alias(self.col_out))}\n</code></pre>"},{"location":"operators/Distribution/","title":"Distribution","text":"<p>             Bases: <code>TransformOp</code></p> <p>Operator for computing distribution of similarity metrics.</p> <p>Attributes:</p> Name Type Description <code>kind</code> <code>Literal['cosine_similarity', 'rouge']</code> <p>The type of similarity metric.</p> <code>col_in_embs</code> <code>list[str]</code> <p>The input columns containing embeddings.</p> <code>col_in_groupby</code> <code>list[str]</code> <p>The columns to group by.</p> <code>col_out</code> <code>list[str] | None</code> <p>The output columns. If None, automatically generated column names will be used.</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If the number of output columns does not match the number of input embedding columns.</p> Example <pre><code>import polars as pl\nfrom uptrain.operators import Distribution\n\n# Create an instance of the Distribution operator\nop = Distribution(\n        kind=\"cosine_similarity\",\n        col_in_embs=[\"context_embeddings\", \"response_embeddings\"],\n        col_in_groupby=[\"question_idx\", \"experiment_id\"],\n        col_out=[\"similarity-context\", \"similarity-response\"],\n    )\n\n# Set up the operator\nop.setup()\n\n# Run the operator on the input data\ninput_data = pl.DataFrame(...)\noutput = op.run(input_data)[\"output\"]\n\n# Print the output\nprint(output)\n</code></pre> Output <pre><code>shape: (90, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 question_idx \u2506 experiment_id \u2506 similarity-context \u2506 similarity-response \u2502\n\u2502 ---          \u2506 ---           \u2506 ---                \u2506 ---                 \u2502\n\u2502 i64          \u2506 i64           \u2506 f64                \u2506 f64                 \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 2            \u2506 0             \u2506 0.314787           \u2506 1.0                 \u2502\n\u2502 2            \u2506 0             \u2506 0.387398           \u2506 0.204949            \u2502\n\u2502 2            \u2506 0             \u2506 0.344797           \u2506 0.23195             \u2502\n\u2502 2            \u2506 0             \u2506 0.306041           \u2506 1.0                 \u2502\n\u2502 \u2026            \u2506 \u2026             \u2506 \u2026                  \u2506 \u2026                   \u2502\n\u2502 0            \u2506 2             \u2506 0.997804           \u2506 0.996358            \u2502\n\u2502 0            \u2506 2             \u2506 0.66862            \u2506 0.300155            \u2502\n\u2502 0            \u2506 2             \u2506 0.224229           \u2506 0.637781            \u2502\n\u2502 0            \u2506 2             \u2506 0.379936           \u2506 0.260659            \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> Source code in <code>uptrain/operators/embs.py</code> <pre><code>@register_op\nclass Distribution(TransformOp):\n    \"\"\"\n    Operator for computing distribution of similarity metrics.\n\n    Attributes:\n        kind (Literal[\"cosine_similarity\", \"rouge\"]): The type of similarity metric.\n        col_in_embs (list[str]): The input columns containing embeddings.\n        col_in_groupby (list[str]): The columns to group by.\n        col_out (list[str] | None): The output columns. If None, automatically generated column names will be used.\n\n    Raises:\n        AssertionError: If the number of output columns does not match the number of input embedding columns.\n\n    Example:\n        ```\n        import polars as pl\n        from uptrain.operators import Distribution\n\n        # Create an instance of the Distribution operator\n        op = Distribution(\n                kind=\"cosine_similarity\",\n                col_in_embs=[\"context_embeddings\", \"response_embeddings\"],\n                col_in_groupby=[\"question_idx\", \"experiment_id\"],\n                col_out=[\"similarity-context\", \"similarity-response\"],\n            )\n\n        # Set up the operator\n        op.setup()\n\n        # Run the operator on the input data\n        input_data = pl.DataFrame(...)\n        output = op.run(input_data)[\"output\"]\n\n        # Print the output\n        print(output)\n        ```\n\n\n    Output:\n        ```\n        shape: (90, 4)\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502 question_idx \u2506 experiment_id \u2506 similarity-context \u2506 similarity-response \u2502\n        \u2502 ---          \u2506 ---           \u2506 ---                \u2506 ---                 \u2502\n        \u2502 i64          \u2506 i64           \u2506 f64                \u2506 f64                 \u2502\n        \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n        \u2502 2            \u2506 0             \u2506 0.314787           \u2506 1.0                 \u2502\n        \u2502 2            \u2506 0             \u2506 0.387398           \u2506 0.204949            \u2502\n        \u2502 2            \u2506 0             \u2506 0.344797           \u2506 0.23195             \u2502\n        \u2502 2            \u2506 0             \u2506 0.306041           \u2506 1.0                 \u2502\n        \u2502 \u2026            \u2506 \u2026             \u2506 \u2026                  \u2506 \u2026                   \u2502\n        \u2502 0            \u2506 2             \u2506 0.997804           \u2506 0.996358            \u2502\n        \u2502 0            \u2506 2             \u2506 0.66862            \u2506 0.300155            \u2502\n        \u2502 0            \u2506 2             \u2506 0.224229           \u2506 0.637781            \u2502\n        \u2502 0            \u2506 2             \u2506 0.379936           \u2506 0.260659            \u2502\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n        ```\n\n    \"\"\"\n\n    kind: t.Literal[\"cosine_similarity\", \"rouge\", \"norm_ratio\"]\n    col_in_embs: list[str]\n    col_in_groupby: list[str]\n    col_out: list[str] | None = None\n\n    @root_validator(pre=True)\n    def _check_cols(cls, values):\n        \"\"\"\n        Validator to check the validity of input and output column lists.\n\n        Args:\n            values (dict): The input attribute values.\n\n        Returns:\n            dict: The validated attribute values.\n\n        Raises:\n            AssertionError: If the number of output columns does not match the number of input embedding columns.\n\n        \"\"\"\n        if values[\"col_out\"] is not None:\n            assert len(values[\"col_out\"]) == len(\n                values[\"col_in_embs\"]\n            ), \"Distribution Op needs as many output columns as input embedding columns\"\n        return values\n\n    def setup(self, settings: Settings):\n        if self.kind == \"cosine_similarity\":\n            self._agg_func = get_cosine_sim_dist\n        elif self.kind == \"rouge\":\n            self._agg_func = get_rouge_score\n        elif self.kind == \"norm_ratio\":\n            self._agg_func = get_norm_ratio_dist\n        else:\n            raise NotImplementedError(\n                f\"Similarity metric: {self.kind} not supported for now.\"\n            )\n        return self\n\n    def run(self, data: pl.DataFrame) -&gt; TYPE_TABLE_OUTPUT:\n        if self.col_out is None:\n            agg_cols = [get_output_col_name_at(i) for i in range(len(self.col_in_embs))]\n        else:\n            agg_cols = self.col_out\n\n        dist_df = (\n            data.groupby(self.col_in_groupby, maintain_order=True)\n            .agg(\n                [\n                    pl.col(_col_in).apply(self._agg_func).alias(_col_out)\n                    for _col_in, _col_out in zip(self.col_in_embs, agg_cols)\n                ]\n            )\n            .explode(agg_cols)\n        )\n        return {\"output\": dist_df}\n</code></pre>"},{"location":"operators/Table/","title":"Table","text":"<p>             Bases: <code>OpBaseModel</code></p> <p>Operator to generate a table.</p> <p>Attributes:</p> Name Type Description <code>props</code> <code>dict</code> <p>Additional properties to pass to the Table constructor.</p> <code>title</code> <code>str</code> <p>The title of the chart.</p> Source code in <code>uptrain/operators/table.py</code> <pre><code>@register_op\nclass Table(OpBaseModel):\n    \"\"\"\n    Operator to generate a table.\n\n    Attributes:\n        props (dict): Additional properties to pass to the Table constructor.\n        title (str): The title of the chart.\n\n    \"\"\"\n\n    props: dict = Field(default_factory=dict)\n    title: str = \"\"\n\n    kind = \"table\"\n\n    def setup(self, settings: Settings):\n        return self\n\n    def run(self, data: pl.DataFrame) -&gt; TYPE_TABLE_OUTPUT:\n        return {\"output\": None}\n</code></pre>"},{"location":"operators/UMAP/","title":"UMAP","text":"<p>             Bases: <code>ColumnOp</code></p> <p>Operator for performing UMAP dimensionality reduction.</p> <p>Attributes:</p> Name Type Description <code>col_in_embs</code> <code>str</code> <p>The input column containing embeddings.</p> <code>n_components</code> <code>int</code> <p>Number of components to keep.</p> <code>col_out</code> <code>str</code> <p>The umap column containing embeddings.</p> Example <pre><code>import polars as pl\nfrom uptrain.operators import UMAP\n\n# Create an instance of the UMAP operator\nop = UMAP(\n        col_in_embs=\"embedding\",\n        n_components=6,\n        col_out= \"umap_embedding\"\n\n    )\n\n# Set up the operator\nop.setup()\n\n# Run the operator on the input data\ninput_data = pl.DataFrame(...)\noutput = op.run(input_data)\n\n# Get the output DataFrame\numap_df = output[\"output\"]\n</code></pre> Output <pre><code>shape: (2,)\nSeries: '_col_0' [list[f32]]\n[\n        [0.098575, 0.056978, \u2026 -0.071038]\n        [0.072772, 0.073564, \u2026 -0.043947]\n]\n</code></pre> Source code in <code>uptrain/operators/embs.py</code> <pre><code>@register_op\nclass UMAP(ColumnOp):\n    \"\"\"\n    Operator for performing UMAP dimensionality reduction.\n\n    Attributes:\n        col_in_embs (str): The input column containing embeddings.\n        n_components (int): Number of components to keep.\n        col_out (str): The umap column containing embeddings.\n\n    Example:\n        ```\n        import polars as pl\n        from uptrain.operators import UMAP\n\n        # Create an instance of the UMAP operator\n        op = UMAP(\n                col_in_embs=\"embedding\",\n                n_components=6,\n                col_out= \"umap_embedding\"\n\n            )\n\n        # Set up the operator\n        op.setup()\n\n        # Run the operator on the input data\n        input_data = pl.DataFrame(...)\n        output = op.run(input_data)\n\n        # Get the output DataFrame\n        umap_df = output[\"output\"]\n        ```\n\n    Output:\n        ```\n        shape: (2,)\n        Series: '_col_0' [list[f32]]\n        [\n                [0.098575, 0.056978, \u2026 -0.071038]\n                [0.072772, 0.073564, \u2026 -0.043947]\n        ]\n        ```\n\n    \"\"\"\n\n    col_in_embs: str = \"embedding\"\n    n_components: int = 6\n    col_out: str = \"umap_embedding\"\n\n    def setup(self, settings: Settings):\n        return self\n\n    def run(self, data: pl.DataFrame) -&gt; TYPE_TABLE_OUTPUT:\n        combined_embs = np.asarray(list(data[self.col_in_embs]))\n        umap_output = umap.UMAP(n_components=self.n_components, metric=\"cosine\", random_state=42).fit_transform(combined_embs)  # type: ignore\n        umap_output = [[round(y, 8) for y in list(x)] for x in list(umap_output)]\n        output_cols = [pl.Series(umap_output).alias(self.col_out)]\n        output_cols.extend(\n            [\n                pl.Series(np.array(umap_output)[:, idx]).alias(\n                    self.col_out + \"_\" + str(idx)\n                )\n                for idx in range(self.n_components)\n            ]\n        )\n\n        return {\"output\": data.with_columns(output_cols)}\n</code></pre>"},{"location":"operators/IO/BigQueryReader/","title":"BigQueryReader","text":"<p>             Bases: <code>TransformOp</code></p> <p>Read data from a bigquery table.</p> <p>NOTE: To use this operator, you must include the GCP service account credentials in the settings, using the key <code>gcp_service_account_credentials</code>.</p> <p>Attributes:</p> Name Type Description <code>query</code> <code>str</code> <p>Query to run against the BigQuery table.</p> <code>col_timestamp</code> <code>str</code> <p>Column name to use as the timestamp column. Only used in the context of monitoring.</p> Example <pre><code>from uptrain.operators.io import BigQueryReader\n\nquery = \"SELECT * FROM `bigquery-public-data.samples.shakespeare` LIMIT 10\"\nreader = BigQueryReader(\n    query=query,\n    col_timestamp=\"timestamp\"\n)\noutput = reader.setup().run()[\"output\"]\n</code></pre> Source code in <code>uptrain/operators/io/bq.py</code> <pre><code>@register_op\nclass BigQueryReader(TransformOp):\n    \"\"\"Read data from a bigquery table.\n\n    NOTE: To use this operator, you must include the GCP service account credentials in\n    the settings, using the key `gcp_service_account_credentials`.\n\n    Attributes:\n        query (str): Query to run against the BigQuery table.\n        col_timestamp (str): Column name to use as the timestamp column. Only used in the context of monitoring.\n\n    Example:\n        ```python\n        from uptrain.operators.io import BigQueryReader\n\n        query = \"SELECT * FROM `bigquery-public-data.samples.shakespeare` LIMIT 10\"\n        reader = BigQueryReader(\n            query=query,\n            col_timestamp=\"timestamp\"\n        )\n        output = reader.setup().run()[\"output\"]\n        ```\n    \"\"\"\n\n    query: str\n    col_timestamp: str = \"timestamp\"\n    limit_rows: int | None = None\n\n    def setup(self, settings: Settings):\n        from google.oauth2 import service_account\n\n        gcp_sa_creds = settings.check_and_get(\"gcp_service_account_credentials\")\n        credentials = service_account.Credentials.from_service_account_info(\n            gcp_sa_creds\n        )\n        self._client = bigquery.Client(credentials=credentials)\n        return self\n\n    def run(self) -&gt; TYPE_TABLE_OUTPUT:\n        if self.limit_rows is None:\n            query_job = self._client.query(self.query)\n        else:\n            query_job = self._client.query(\n                self.query + \" LIMIT \" + str(self.limit_rows)\n            )\n        rows = query_job.result()\n        return {\"output\": pl.from_arrow(rows.to_arrow())}\n</code></pre>"},{"location":"operators/IO/CsvReader/","title":"CsvReader","text":"<p>             Bases: <code>TransformOp</code></p> <p>Reads data from a csv file.</p> <p>Attributes:</p> Name Type Description <code>fpath</code> <code>str</code> <p>Path to the csv file.</p> <code>batch_size</code> <code>Optional[int]</code> <p>Number of rows to read at a time. Defaults to None, which reads the entire file.</p> Source code in <code>uptrain/operators/io/base.py</code> <pre><code>@register_op\nclass CsvReader(TransformOp):\n    \"\"\"Reads data from a csv file.\n\n    Attributes:\n        fpath (str): Path to the csv file.\n        batch_size (Optional[int]): Number of rows to read at a time. Defaults to None, which reads the entire file.\n\n    \"\"\"\n\n    fpath: str\n    batch_size: t.Optional[int] = None\n\n    def setup(self, settings: Settings):\n        self._executor = TextReaderExecutor(self)\n        return self\n\n    def run(self) -&gt; TYPE_TABLE_OUTPUT:\n        return {\"output\": self._executor.run()}\n</code></pre>"},{"location":"operators/IO/DeltaReader/","title":"DeltaReader","text":"<p>             Bases: <code>TransformOp</code></p> <p>Reads data from a Delta Lake table.</p> <p>Attributes:</p> Name Type Description <code>fpath</code> <code>str</code> <p>File path to the Delta Lake table.</p> <code>batch_split</code> <code>bool</code> <p>Whether to read the table in batches. Defaults to False.</p> Source code in <code>uptrain/operators/io/base.py</code> <pre><code>@register_op\nclass DeltaReader(TransformOp):\n    \"\"\"Reads data from a Delta Lake table.\n\n    Attributes:\n        fpath (str): File path to the Delta Lake table.\n        batch_split (bool): Whether to read the table in batches. Defaults to False.\n\n    \"\"\"\n\n    fpath: str\n    batch_split: bool = False\n    _dataset: t.Any  # pyarrow dataset\n    _batch_generator: t.Optional[t.Iterator[t.Any]]  # record batch generator\n\n    def setup(self, settings: Settings):\n        lazy_load_dep(\"pyarrow\", \"pyarrow&gt;=10.0.0\")\n        dl = lazy_load_dep(\"deltatable\", \"deltalake&gt;=0.9\")\n\n        self._dataset = dl.DeltaTable(self.fpath).to_pyarrow_dataset()\n        if self.is_incremental:\n            self._batch_generator = iter(self._dataset.to_batches())\n        return self\n\n    @property\n    def is_incremental(self) -&gt; bool:\n        return self.batch_split is True\n\n    def run(self) -&gt; TYPE_TABLE_OUTPUT:\n        if not self.is_incremental:\n            data = pl.from_arrow(self._dataset.to_table())\n        else:\n            try:\n                data = pl.from_arrow(next(self._batch_generator))  # type: ignore\n            except StopIteration:\n                data = None\n\n        if data is not None:\n            assert isinstance(data, pl.DataFrame)\n        return {\"output\": data}\n</code></pre>"},{"location":"operators/IO/DeltaWriter/","title":"DeltaWriter","text":"<p>             Bases: <code>OpBaseModel</code></p> Source code in <code>uptrain/operators/io/base.py</code> <pre><code>@register_op\nclass DeltaWriter(OpBaseModel):\n    fpath: str\n    columns: t.Optional[list[str]] = None\n\n    def setup(self, settings: Settings):\n        dl = lazy_load_dep(\"deltatable\", \"deltalake&gt;=0.9\")\n\n        return self\n\n    def run(self, data: pl.DataFrame) -&gt; TYPE_TABLE_OUTPUT:\n        if self.columns is None:\n            self.columns = list(data.columns)\n        assert set(self.columns) == set(data.columns)\n        data.write_delta(self.fpath, mode=\"append\")\n        return {\"output\": data}\n\n    def to_reader(self):\n        return DeltaReader(fpath=self.fpath)  # type: ignore\n</code></pre>"},{"location":"operators/IO/DuckDBReader/","title":"DuckDBReader","text":"<p>             Bases: <code>TransformOp</code></p> <p>Read data from a Duckdb table.</p> <p>Attributes:</p> Name Type Description <code>fpath</code> <code>str</code> <p>Path to the Duckdb file.</p> <code>query</code> <code>str</code> <p>Query to run against the duckdb database.</p> <code>col_timestamp</code> <code>str</code> <p>Column name to use as the timestamp column. Only used in the context of monitoring.</p> Example <pre><code>from uptrain.operators.io import DuckDBReader\n\nreader = DuckDBReader(\n    fpath=\"data/duckdb.db\",\n    query=\"SELECT * FROM my_table\",\n    col_timestamp=\"timestamp\",\n)\noutput = reader.setup().run()[\"output\"]\n</code></pre> Source code in <code>uptrain/operators/io/duck.py</code> <pre><code>@register_op\nclass DuckDBReader(TransformOp):\n    \"\"\"Read data from a Duckdb table.\n\n    Attributes:\n        fpath (str): Path to the Duckdb file.\n        query (str): Query to run against the duckdb database.\n        col_timestamp (str): Column name to use as the timestamp column. Only used in the context of monitoring.\n\n    Example:\n        ```python\n        from uptrain.operators.io import DuckDBReader\n\n        reader = DuckDBReader(\n            fpath=\"data/duckdb.db\",\n            query=\"SELECT * FROM my_table\",\n            col_timestamp=\"timestamp\",\n        )\n        output = reader.setup().run()[\"output\"]\n        ```\n    \"\"\"\n\n    fpath: str\n    query: str\n    col_timestamp: str = \"timestamp\"\n\n    def setup(self, settings: Settings):\n        self._conn = duckdb.connect(self.fpath)\n        return self\n\n    def run(self) -&gt; TYPE_TABLE_OUTPUT:\n        res = self._conn.query(self.query).to_arrow_table()\n        return {\"output\": pl.from_arrow(res)}\n</code></pre>"},{"location":"operators/IO/ExcelReader/","title":"ExcelReader","text":"<p>             Bases: <code>TransformOp</code></p> <p>Reads an excel file.</p> <p>Attributes:</p> Name Type Description <code>fpath</code> <code>str</code> <p>Path to the xlsx file.</p> <code>batch_size</code> <code>Optional[int]</code> <p>Number of rows to read at a time. Defaults to None, which reads the entire file.</p> Source code in <code>uptrain/operators/io/excel.py</code> <pre><code>@register_op\nclass ExcelReader(TransformOp):\n    \"\"\"Reads an excel file.\n\n    Attributes:\n        fpath (str): Path to the xlsx file.\n        batch_size (Optional[int]): Number of rows to read at a time. Defaults to None, which reads the entire file.\n\n    \"\"\"\n\n    fpath: str\n    batch_size: t.Optional[int] = None\n\n    def setup(self, settings: Settings):\n        return self\n\n    def run(self) -&gt; TYPE_TABLE_OUTPUT:\n        return {\"output\": pl.read_excel(self.fpath)}\n</code></pre>"},{"location":"operators/IO/JsonReader/","title":"JsonReader","text":"<p>             Bases: <code>TransformOp</code></p> <p>Reads data from a json file.</p> <p>Attributes:</p> Name Type Description <code>fpath</code> <code>str</code> <p>Path to the json file.</p> <code>batch_size</code> <code>Optional[int]</code> <p>Number of rows to read at a time. Defaults to None, which reads the entire file.</p> Source code in <code>uptrain/operators/io/base.py</code> <pre><code>@register_op\nclass JsonReader(TransformOp):\n    \"\"\"Reads data from a json file.\n\n    Attributes:\n        fpath (str): Path to the json file.\n        batch_size (Optional[int]): Number of rows to read at a time. Defaults to None, which reads the entire file.\n\n    \"\"\"\n\n    fpath: str\n    batch_size: t.Optional[int] = None\n\n    def setup(self, settings: Settings):\n        self._executor = TextReaderExecutor(self)\n        return self\n\n    def run(self) -&gt; TYPE_TABLE_OUTPUT:\n        return {\"output\": self._executor.run()}\n</code></pre>"},{"location":"operators/IO/JsonWriter/","title":"JsonWriter","text":"<p>             Bases: <code>OpBaseModel</code></p> Source code in <code>uptrain/operators/io/base.py</code> <pre><code>@register_op\nclass JsonWriter(OpBaseModel):\n    fpath: str\n    columns: t.Optional[list[str]] = None\n\n    def setup(self, settings: Settings):\n        return self\n\n    def to_reader(self):\n        return JsonReader(fpath=self.fpath)  # type: ignore\n\n    def run(self, data: pl.DataFrame) -&gt; TYPE_TABLE_OUTPUT:\n        if self.columns is None:\n            self.columns = list(data.columns)\n        assert set(self.columns) == set(data.columns)\n        with open(self.fpath, \"a\") as f:\n            f.write(data.write_ndjson())\n        return {\"output\": data}\n</code></pre>"},{"location":"operators/charts/BarChart/","title":"BarChart","text":"<p>             Bases: <code>Chart</code></p> <p>Operator to generate a bar chart.</p> <p>Attributes:</p> Name Type Description <code>props</code> <code>dict</code> <p>Additional properties to pass to the BarChart constructor.</p> <code>x</code> <code>str</code> <p>The name of the column to use for the x-axis.</p> <code>y</code> <code>str</code> <p>The name of the column to use for the y-axis.</p> <code>color</code> <code>str</code> <p>The name of the column to use for the color.</p> <code>barmode</code> <code>str</code> <p>The type of bar chart to generate.</p> <code>title</code> <code>str</code> <p>The title of the chart.</p> <code>description</code> <code>str</code> <p>Add a description of the chart being created.</p> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary containing the chart object.</p> Example <pre><code>import polars as pl\nfrom uptrain.operators import BarChart\n\n# Create a DataFrame\ndf = pl.DataFrame({\n    \"x\": [1, 2, 3, 4, 5],\n    \"y\": [10, 20, 15, 25, 30]\n})\n\n# Create a bar chart using the BarChart class\nbar_chart = BarChart(x=\"x\", y=\"y\", title=\"Bar Chart\")\n\n# Generate the bar chart\nchart = bar_chart.run(df)[\"extra\"][\"chart\"]\n\n# Show the chart\nchart.show()\n</code></pre> Source code in <code>uptrain/operators/chart.py</code> <pre><code>@register_op\nclass BarChart(Chart):\n    \"\"\"\n    Operator to generate a bar chart.\n\n    Attributes:\n        props (dict): Additional properties to pass to the BarChart constructor.\n        x (str): The name of the column to use for the x-axis.\n        y (str): The name of the column to use for the y-axis.\n        color (str): The name of the column to use for the color.\n        barmode (str): The type of bar chart to generate.\n        title (str): The title of the chart.\n        description (str): Add a description of the chart being created.\n\n    Returns:\n        dict: A dictionary containing the chart object.\n\n    Example:\n        ```\n        import polars as pl\n        from uptrain.operators import BarChart\n\n        # Create a DataFrame\n        df = pl.DataFrame({\n            \"x\": [1, 2, 3, 4, 5],\n            \"y\": [10, 20, 15, 25, 30]\n        })\n\n        # Create a bar chart using the BarChart class\n        bar_chart = BarChart(x=\"x\", y=\"y\", title=\"Bar Chart\")\n\n        # Generate the bar chart\n        chart = bar_chart.run(df)[\"extra\"][\"chart\"]\n\n        # Show the chart\n        chart.show()\n        ```\n\n    \"\"\"\n\n    props: dict = Field(default_factory=dict)\n    title: str = \"\"\n    x: str = \"\"\n    y: str = \"\"\n    description: str = \"\"\n    color: str = \"\"\n\n    barmode: str = \"group\"\n\n    kind = \"bar\"\n</code></pre>"},{"location":"operators/charts/CustomPlotlyChart/","title":"CustomPlotlyChart","text":"<p>             Bases: <code>Chart</code></p> <p>Operator to generate a custom plotly chart, other than the ones provided by uptrain.</p> <p>Attributes:</p> Name Type Description <code>props</code> <code>dict</code> <p>Additional properties to pass to the PlotlyChart constructor.</p> <code>title</code> <code>str</code> <p>The title of the chart.</p> <code>x</code> <code>str</code> <p>The name of the column to use for the x-axis.</p> <code>y</code> <code>str</code> <p>The name of the column to use for the y-axis.</p> <code>color</code> <code>str</code> <p>The name of the column to use for the color.</p> <code>kind</code> <code>str</code> <p>The type of chart to generate.</p> <code>description</code> <code>str</code> <p>Add a description of the chart being created.</p> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary containing the chart object.</p> Example <pre><code>import polars as pl\nfrom uptrain.operators import CustomPlotlyChart\n\n# Create a DataFrame\ndf = pl.DataFrame({\n    \"x\": [1, 2, 3, 4, 5],\n    \"y\": [10, 20, 15, 25, 30]\n})\n\n# Create a funnel chart using the CustomPlotlyChart class\nfunnel_chart = CustomPlotlyChart(x=\"x\", y=\"y\", title=\"Funnel Chart\")\n\n# Generate the funnel chart\nchart = funnel_chart.run(df)[\"extra\"][\"chart\"]\n\n# Show the chart\nchart.show()\n</code></pre> Source code in <code>uptrain/operators/chart.py</code> <pre><code>@register_op\nclass CustomPlotlyChart(Chart):\n    \"\"\"\n    Operator to generate a custom plotly chart, other than the ones provided by uptrain.\n\n    Attributes:\n        props (dict): Additional properties to pass to the PlotlyChart constructor.\n        title (str): The title of the chart.\n        x (str): The name of the column to use for the x-axis.\n        y (str): The name of the column to use for the y-axis.\n        color (str): The name of the column to use for the color.\n        kind (str): The type of chart to generate.\n        description (str): Add a description of the chart being created.\n\n    Returns:\n        dict: A dictionary containing the chart object.\n\n    Example:\n        ```\n        import polars as pl\n        from uptrain.operators import CustomPlotlyChart\n\n        # Create a DataFrame\n        df = pl.DataFrame({\n            \"x\": [1, 2, 3, 4, 5],\n            \"y\": [10, 20, 15, 25, 30]\n        })\n\n        # Create a funnel chart using the CustomPlotlyChart class\n        funnel_chart = CustomPlotlyChart(x=\"x\", y=\"y\", title=\"Funnel Chart\")\n\n        # Generate the funnel chart\n        chart = funnel_chart.run(df)[\"extra\"][\"chart\"]\n\n        # Show the chart\n        chart.show()\n        ```\n\n    \"\"\"\n\n    props: dict = Field(default_factory=dict)\n    title: str = \"\"\n    x: str = \"\"\n    y: str = \"\"\n    description: str = \"\"\n    color: str = \"\"\n    kind: str = Field(default_factory=str)\n</code></pre>"},{"location":"operators/charts/Histogram/","title":"Histogram","text":"<p>             Bases: <code>Chart</code></p> <p>Operator to generate a histogram.</p> <p>Attributes:</p> Name Type Description <code>props</code> <code>dict</code> <p>Additional properties to pass to the Histogram chart constructor.</p> <code>title</code> <code>str</code> <p>The title of the chart.</p> <code>x</code> <code>str</code> <p>The name of the column to use for the x-axis.</p> <code>y</code> <code>str</code> <p>The name of the column to use for the y-axis.</p> <code>color</code> <code>str</code> <p>The name of the column to use for the color.</p> <code>nbins</code> <code>int</code> <p>The maximum number of bins to use for the histogram.</p> <code>description</code> <code>str</code> <p>Add a description of the chart being created.</p> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary containing the chart object.</p> Example <pre><code>import polars as pl\nfrom uptrain.operators import Histogram\n\n# Create a DataFrame\ndf = pl.DataFrame({\n    \"x\": [1, 2, 3, 4, 5],\n    \"y\": [10, 20, 15, 25, 30]\n})\n\n# Create a histogram chart using the Histogram class\nhistogram_chart = Histogram(x=\"x\", y=\"y\", title=\"Histogram Chart\")\n\n# Generate the histogram chart\nchart = histogram_chart.run(df)[\"extra\"][\"chart\"]\n\n# Show the chart\nchart.show()\n</code></pre> Source code in <code>uptrain/operators/chart.py</code> <pre><code>@register_op\nclass Histogram(Chart):\n    \"\"\"\n    Operator to generate a histogram.\n\n    Attributes:\n        props (dict): Additional properties to pass to the Histogram chart constructor.\n        title (str): The title of the chart.\n        x (str): The name of the column to use for the x-axis.\n        y (str): The name of the column to use for the y-axis.\n        color (str): The name of the column to use for the color.\n        nbins (int): The maximum number of bins to use for the histogram.\n        description (str): Add a description of the chart being created.\n\n    Returns:\n        dict: A dictionary containing the chart object.\n\n    Example:\n        ```\n        import polars as pl\n        from uptrain.operators import Histogram\n\n        # Create a DataFrame\n        df = pl.DataFrame({\n            \"x\": [1, 2, 3, 4, 5],\n            \"y\": [10, 20, 15, 25, 30]\n        })\n\n        # Create a histogram chart using the Histogram class\n        histogram_chart = Histogram(x=\"x\", y=\"y\", title=\"Histogram Chart\")\n\n        # Generate the histogram chart\n        chart = histogram_chart.run(df)[\"extra\"][\"chart\"]\n\n        # Show the chart\n        chart.show()\n        ```\n\n    \"\"\"\n\n    props: dict = Field(default_factory=dict)\n    title: str = \"\"\n    x: str = \"\"\n    y: str = \"\"\n    description: str = \"\"\n    color: str = \"\"\n    nbins: int = 20\n\n    kind = \"histogram\"\n</code></pre>"},{"location":"operators/charts/LineChart/","title":"LineChart","text":"<p>             Bases: <code>Chart</code></p> <p>Operator to generate a line chart.</p> <p>Attributes:</p> Name Type Description <code>props</code> <code>dict</code> <p>Additional properties to pass to the LineChart constructor.</p> <code>x</code> <code>str</code> <p>The name of the column to use for the x-axis.</p> <code>y</code> <code>str</code> <p>The name of the column to use for the y-axis.</p> <code>color</code> <code>str</code> <p>The name of the column to use for the color.</p> <code>title</code> <code>str</code> <p>The title of the chart.</p> <code>description</code> <code>str</code> <p>Add a description of the chart being created.</p> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary containing the chart object.</p> Example <pre><code>import polars as pl\nfrom uptrain.operators import LineChart\n\n# Create a DataFrame\ndf = pl.DataFrame({\n    \"x\": [1, 2, 3, 4, 5],\n    \"y\": [10, 20, 15, 25, 30]\n})\n\n# Create a line chart using the LineChart class\nline_chart = LineChart(x=\"x\", y=\"y\", title=\"Line Chart\")\n\n# Generate the line chart\nchart = line_chart.run(df)[\"extra\"][\"chart\"]\n\n# Show the chart\nchart.show()\n</code></pre> Source code in <code>uptrain/operators/chart.py</code> <pre><code>@register_op\nclass LineChart(Chart):\n    \"\"\"\n    Operator to generate a line chart.\n\n    Attributes:\n        props (dict): Additional properties to pass to the LineChart constructor.\n        x (str): The name of the column to use for the x-axis.\n        y (str): The name of the column to use for the y-axis.\n        color (str): The name of the column to use for the color.\n        title (str): The title of the chart.\n        description (str): Add a description of the chart being created.\n\n    Returns:\n        dict: A dictionary containing the chart object.\n\n    Example:\n        ```\n        import polars as pl\n        from uptrain.operators import LineChart\n\n        # Create a DataFrame\n        df = pl.DataFrame({\n            \"x\": [1, 2, 3, 4, 5],\n            \"y\": [10, 20, 15, 25, 30]\n        })\n\n        # Create a line chart using the LineChart class\n        line_chart = LineChart(x=\"x\", y=\"y\", title=\"Line Chart\")\n\n        # Generate the line chart\n        chart = line_chart.run(df)[\"extra\"][\"chart\"]\n\n        # Show the chart\n        chart.show()\n        ```\n\n    \"\"\"\n\n    props: dict = Field(default_factory=dict)\n    title: str = \"\"\n    x: str = \"\"\n    y: str = \"\"\n    description: str = \"\"\n    color: str = \"\"\n\n    kind = \"line\"\n</code></pre>"},{"location":"operators/charts/MultiPlot/","title":"MultiPlot","text":"<p>             Bases: <code>Chart</code></p> <p>Operator to generate a subplot that can display multiple charts side-by-side.</p> <p>Attributes:</p> Name Type Description <code>props</code> <code>dict</code> <p>Additional properties to pass to the MultiPlot constructor.</p> <code>title</code> <code>str</code> <p>The title of the chart.</p> <code>charts</code> <code>list</code> <p>A list of charts to display in the subplot.</p> <code>description</code> <code>str</code> <p>Add a description of the chart being created. (supports upto 70 characters without overlapping on graph)</p> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary containing the chart object.</p> Example <pre><code>import polars as pl\nfrom uptrain.operators import MultiPlot, LineChart, ScatterPlot, BarChart, Histogram\n\n# Create a DataFrame\ndf = pl.DataFrame({\n    \"x\": [1, 2, 3, 4, 5],\n    \"y\": [10, 20, 15, 25, 30]\n})\n\n# Create a multiplot using the MultiPlot class\nmultiplot = MultiPlot(\n    props={},\n    charts=[\n        LineChart(\n            x=\"x\",\n            y=\"y\",\n            title=\"Line Chart\"\n        ),\n        ScatterPlot(\n            x=\"x\",\n            y=\"y\",\n            title=\"Scatter Plot\"\n        ),\n        BarChart(\n            x=\"x\",\n            y=\"y\",\n            title=\"Bar Chart\"\n        ),\n        Histogram(\n            x=\"x\",\n            y=\"y\",\n            title=\"Histogram\"\n        )\n    ],\n    title=\"MultiPlot\",\n)\n\n# Generate the multiplot\nchart = multiplot.run(df)[\"extra\"][\"chart\"]\n\n# Show the chart\nchart.show()\n</code></pre> Source code in <code>uptrain/operators/chart.py</code> <pre><code>@register_op\nclass MultiPlot(Chart):\n    \"\"\"\n    Operator to generate a subplot that can display multiple charts side-by-side.\n\n    Attributes:\n        props (dict): Additional properties to pass to the MultiPlot constructor.\n        title (str): The title of the chart.\n        charts (list): A list of charts to display in the subplot.\n        description (str): Add a description of the chart being created. (supports upto 70 characters without overlapping on graph)\n\n    Returns:\n        dict: A dictionary containing the chart object.\n\n    Example:\n        ```\n        import polars as pl\n        from uptrain.operators import MultiPlot, LineChart, ScatterPlot, BarChart, Histogram\n\n        # Create a DataFrame\n        df = pl.DataFrame({\n            \"x\": [1, 2, 3, 4, 5],\n            \"y\": [10, 20, 15, 25, 30]\n        })\n\n        # Create a multiplot using the MultiPlot class\n        multiplot = MultiPlot(\n            props={},\n            charts=[\n                LineChart(\n                    x=\"x\",\n                    y=\"y\",\n                    title=\"Line Chart\"\n                ),\n                ScatterPlot(\n                    x=\"x\",\n                    y=\"y\",\n                    title=\"Scatter Plot\"\n                ),\n                BarChart(\n                    x=\"x\",\n                    y=\"y\",\n                    title=\"Bar Chart\"\n                ),\n                Histogram(\n                    x=\"x\",\n                    y=\"y\",\n                    title=\"Histogram\"\n                )\n            ],\n            title=\"MultiPlot\",\n        )\n\n        # Generate the multiplot\n        chart = multiplot.run(df)[\"extra\"][\"chart\"]\n\n        # Show the chart\n        chart.show()\n        ```\n\n    \"\"\"\n\n    props: dict = Field(default_factory=dict)\n    title: str = \"\"\n    description: str = \"\"\n    charts: list\n\n    kind = \"multiplot\"\n\n    def run(self, data: pl.DataFrame) -&gt; TYPE_TABLE_OUTPUT:\n        if type(self.charts[0]) == dict:\n            self.charts = [Chart(**chart).setup() for chart in self.charts]\n\n        fig = ps.make_subplots(\n            rows=1,\n            cols=len(self.charts),\n            shared_xaxes=True,\n            shared_yaxes=True,\n            horizontal_spacing=0.05,\n            vertical_spacing=0.2,  # Adjust this value for spacing between graph and annotation\n            subplot_titles=[chart.title for chart in self.charts],\n        )\n\n        annotation_single_height = (\n            -0.1\n        )  # Adjust this value for single-line annotation position\n        annotation_multi_height = (\n            -0.3\n        )  # Adjust this value for multiline annotation position\n        annotation_line_height = (\n            -0.05\n        )  # Adjust this value for multiline annotation spacing\n\n        for idx, chart in enumerate(self.charts):\n            plot = getattr(px, chart.kind)(polars_to_pandas(data), **chart.props)\n\n            # Use only the first trace from the plot\n            trace = plot.data[0]\n            fig.add_trace(trace, row=1, col=idx + 1)\n\n            # Break down description into segments of four words each for long descriptions\n            words = chart.description.split()\n            if len(words) &gt; 12:\n                description_lines = [words[i : i + 4] for i in range(0, len(words), 4)]\n                multiline_description = \"&lt;br&gt;\".join(\n                    \" \".join(line) for line in description_lines\n                )\n                annotation_text = multiline_description\n                annotation_height = annotation_multi_height\n            else:\n                annotation_text = chart.description\n                annotation_height = annotation_single_height\n\n            # Add annotation for the description\n            annotation = dict(\n                text=annotation_text,\n                align=\"center\",\n                showarrow=False,\n                xref=f\"x{idx + 1}\",\n                yref=\"paper\",\n                x=0.5,\n                y=annotation_height,\n                font=dict(size=10),\n            )\n            fig.add_annotation(annotation)\n\n        fig.update_layout(\n            title_text=self.title,\n            showlegend=False,\n            width=800,\n            height=400,\n            margin=dict(t=100, b=50),  # Adjust the bottom margin to center the title\n        )\n\n        return {\"output\": None, \"extra\": {\"chart\": fig}}\n</code></pre>"},{"location":"operators/charts/ScatterPlot/","title":"ScatterPlot","text":"<p>             Bases: <code>Chart</code></p> <p>Operator to generate a scatter chart.</p> <p>Attributes:</p> Name Type Description <code>props</code> <code>dict</code> <p>Additional properties to pass to the ScatterPlot constructor.</p> <code>title</code> <code>str</code> <p>The title of the chart.</p> <code>x</code> <code>str</code> <p>The name of the column to use for the x-axis.</p> <code>y</code> <code>str</code> <p>The name of the column to use for the y-axis.</p> <code>color</code> <code>str</code> <p>The name of the column to use for the color.</p> <code>description</code> <code>str</code> <p>Add a description of the chart being created.</p> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary containing the chart object.</p> Example <pre><code>import polars as pl\nfrom uptrain.operators import ScatterPlot\n\n# Create a DataFrame\ndf = pl.DataFrame({\n    \"x\": [1, 2, 3, 4, 5],\n    \"y\": [10, 20, 15, 25, 30]\n})\n\n# Create a scatter chart using the ScatterPlot class\nscatter_plot = ScatterPlot(x=\"x\", y=\"y\", title=\"Scatter Plot\")\n\n# Generate the scatter plot\nchart = scatter_plot.run(df)[\"extra\"][\"chart\"]\n\n# Show the chart\nchart.show()\n</code></pre> Source code in <code>uptrain/operators/chart.py</code> <pre><code>@register_op\nclass ScatterPlot(Chart):\n    \"\"\"\n    Operator to generate a scatter chart.\n\n    Attributes:\n        props (dict): Additional properties to pass to the ScatterPlot constructor.\n        title (str): The title of the chart.\n        x (str): The name of the column to use for the x-axis.\n        y (str): The name of the column to use for the y-axis.\n        color (str): The name of the column to use for the color.\n        description (str): Add a description of the chart being created.\n\n    Returns:\n        dict: A dictionary containing the chart object.\n\n    Example:\n        ```\n        import polars as pl\n        from uptrain.operators import ScatterPlot\n\n        # Create a DataFrame\n        df = pl.DataFrame({\n            \"x\": [1, 2, 3, 4, 5],\n            \"y\": [10, 20, 15, 25, 30]\n        })\n\n        # Create a scatter chart using the ScatterPlot class\n        scatter_plot = ScatterPlot(x=\"x\", y=\"y\", title=\"Scatter Plot\")\n\n        # Generate the scatter plot\n        chart = scatter_plot.run(df)[\"extra\"][\"chart\"]\n\n        # Show the chart\n        chart.show()\n        ```\n\n    \"\"\"\n\n    props: dict = Field(default_factory=dict)\n    title: str = \"\"\n    x: str = \"\"\n    y: str = \"\"\n    description: str = \"\"\n    color: str = \"\"\n    symbol: str = \"circle\"\n\n    kind = \"scatter\"\n</code></pre>"},{"location":"operators/code/SQL/ExecuteAndCompareSQL/","title":"ExecuteAndCompareSQL","text":"<p>             Bases: <code>TransformOp</code></p> <p>Execute predicted SQL, ground truth SQL and compute execution accuracy of the predicted sql.</p> <p>For now, we expect the output to exactly match along with the column names.</p> <p>ignore_column_order, ignore_row_order params attempt to do a semantic match by ignoring the order. This allows us to correctly compare <code>SELECT a,b</code> and <code>SELECT b,a</code> if the column order is not important. However, the intent in the text query also needs to be taken in consideration for correct sematic match.</p> <p>Attributes:</p> Name Type Description <code>col_in_response_sql</code> <code>str</code> <p>Column containing response SQL.</p> <code>col_in_gt_sql</code> <code>str</code> <p>Column containing schema definition tables and columns as a json dict Table -&gt; [columns].</p> <code>col_in_db_path</code> <code>str</code> <p>Column to store if tables are valid.</p> <code>col_out_execution_accuracy</code> <code>str</code> <p>Column to store if columns are valid.</p> <code>ignore_column_order</code> <code>bool</code> <p>Boolean param to ignore column order when comparing SQL output. True by default.</p> <code>ignore_row_order</code> <code>bool</code> <p>Boolean param to ignore row order when comparing SQL output. True by default.</p> Source code in <code>uptrain/operators/code/sql.py</code> <pre><code>@register_op\nclass ExecuteAndCompareSQL(TransformOp):\n    \"\"\"\n    Execute predicted SQL, ground truth SQL and compute execution accuracy of the predicted sql.\n\n    For now, we expect the output to exactly match along with the column names.\n\n    ignore_column_order, ignore_row_order params attempt to do a semantic match by ignoring the order. This allows us to\n    correctly compare `SELECT a,b` and `SELECT b,a` if the column order is not important. However, the intent in the\n    text query also needs to be taken in consideration for correct sematic match.\n\n    Attributes:\n        col_in_response_sql (str): Column containing response SQL.\n        col_in_gt_sql (str): Column containing schema definition tables and columns as a json dict Table -&gt; [columns].\n        col_in_db_path (str): Column to store if tables are valid.\n        col_out_execution_accuracy (str): Column to store if columns are valid.\n        ignore_column_order (bool): Boolean param to ignore column order when comparing SQL output. True by default.\n        ignore_row_order (bool): Boolean param to ignore row order when comparing SQL output. True by default.\n\n    \"\"\"\n\n    col_in_response_sql: str\n    col_in_gt_sql: str\n    col_in_db_path: str\n    col_out_execution_accuracy: str\n    ignore_column_order: bool = True\n    ignore_row_order: bool = True\n\n    def setup(self, settings: Settings):\n        return self\n\n    def run(self, data: pl.DataFrame) -&gt; TYPE_TABLE_OUTPUT:\n        response_sqls = data.get_column(self.col_in_response_sql)\n        gt_sqls = data.get_column(self.col_in_gt_sql)\n        db_paths = data.get_column(self.col_in_db_path)\n        results = []\n        for response_sql, gt_sql, db_path in zip(response_sqls, gt_sqls, db_paths):\n            results.append(\n                execute_and_compare_sql(\n                    response_sql,\n                    gt_sql,\n                    db_path,\n                    ignore_column_order=self.ignore_column_order,\n                    ignore_row_order=self.ignore_row_order,\n                )\n            )\n        return {\n            \"output\": data.with_columns(\n                [pl.Series(self.col_out_execution_accuracy, results)]\n            )\n        }\n</code></pre>"},{"location":"operators/code/SQL/ParseCreateStatements/","title":"ParseCreateStatements","text":"<p>             Bases: <code>TransformOp</code></p> <p>Read tables and columns from \";\" separated CREATE TABLE statements and writes a json dictionary Table -&gt; [columns].</p> <p>Attributes:</p> Name Type Description <code>col_in_schema_def</code> <code>str</code> <p>Column name of schema def containing CREATE TABLE statements.</p> <code>col_out_tables</code> <code>str</code> <p>Column to write parsed tables and columns.</p> Source code in <code>uptrain/operators/code/sql.py</code> <pre><code>@register_op\nclass ParseCreateStatements(TransformOp):\n    \"\"\"\n    Read tables and columns from \";\" separated CREATE TABLE statements and writes a json dictionary Table -&gt; [columns].\n\n    Attributes:\n        col_in_schema_def (str): Column name of schema def containing CREATE TABLE statements.\n        col_out_tables (str): Column to write parsed tables and columns.\n\n    \"\"\"\n\n    col_in_schema_def: str\n    col_out_tables: str\n\n    def setup(self, settings: Settings):\n        return self\n\n    def __fetch_tables_columns(self, create_statements) -&gt; t.Tuple(str, str, str):\n        tables_and_columns = {}\n        # SQL statements are separated by ';'\n        statements = create_statements.split(\";\")\n\n        # Create table statements\n        for statement in statements:\n            if statement.upper().strip().startswith(\"CREATE TABLE\"):\n                # Add the statement to the list\n                statement = statement.strip()\n                # TODO: handle input database dialect instead of assuming it.\n                parsed = sqlglot.parse(statement, read=sqlglot.Dialects.SQLITE)\n                table, columns = extract_tables_and_columns_from_create(parsed[0])\n                tables_and_columns[table] = list(columns)\n\n        return json.dumps(tables_and_columns)\n\n    def run(self, data: pl.DataFrame) -&gt; TYPE_TABLE_OUTPUT:\n        schemas = data.get_column(self.col_in_schema_def)\n        tables = [self.__fetch_tables_columns(schema) for schema in schemas]\n        return {\"output\": data.with_columns([pl.Series(self.col_out_tables, tables)])}\n</code></pre>"},{"location":"operators/code/SQL/ParseSQL/","title":"ParseSQL","text":"<p>             Bases: <code>TransformOp</code></p> <p>Read tables and columns from a generic SQL SELECT statement and writes a json dictionary Table -&gt; [columns]. Note that we don't use table schema definition to do this but instead simply parse the SQL. Output might have a placeholder table to include columns that are accessed without a table descriptor.</p> <p>This is typically used along with ValidateTables to validate tables and columns in the predicted SQL.</p> <p>Attributes:</p> Name Type Description <code>col_in_sql</code> <code>str</code> <p>Column of input SQL containing SQL SELECT statement.</p> <code>col_out_tables</code> <code>str</code> <p>Column to write parsed tables and columns.</p> <code>col_out_is_valid_sql</code> <code>str</code> <p>Column to store if sql is valid as per sql parser.</p> Source code in <code>uptrain/operators/code/sql.py</code> <pre><code>@register_op\nclass ParseSQL(TransformOp):\n    \"\"\"\n    Read tables and columns from a generic SQL SELECT statement and writes a json dictionary Table -&gt; [columns].\n    Note that we don't use table schema definition to do this but instead simply parse the SQL. Output might have a\n    placeholder table to include columns that are accessed without a table descriptor.\n\n    This is typically used along with ValidateTables to validate tables and columns in the predicted SQL.\n\n    Attributes:\n        col_in_sql (str): Column of input SQL containing SQL SELECT statement.\n        col_out_tables (str): Column to write parsed tables and columns.\n        col_out_is_valid_sql (str): Column to store if sql is valid as per sql parser.\n\n    \"\"\"\n\n    col_in_sql: str\n    col_out_tables: str\n    col_out_is_valid_sql: str\n\n    def setup(self, settings: Settings):\n        return self\n\n    def run(self, data: pl.DataFrame) -&gt; TYPE_TABLE_OUTPUT:\n        sqls = data.get_column(self.col_in_sql)\n        tables = []\n        is_valid = []\n        for sql in sqls:\n            try:\n                # TODO: parse using expected dialect\n                parsed = sqlglot.parse(sql)\n                tables_and_columns = extract_tables_and_columns(parsed[0])\n                # Since sets are not serializable, convert to list\n                for table, columns in tables_and_columns.items():\n                    tables_and_columns[table] = list(columns)\n                tables.append(json.dumps(tables_and_columns))\n                is_valid.append(True)\n            except sqlglot.errors.ParseError:\n                tables.append(json.dumps({}))\n                is_valid.append(False)\n\n        return {\n            \"output\": data.with_columns(\n                [\n                    pl.Series(self.col_out_tables, tables),\n                    pl.Series(self.col_out_is_valid_sql, is_valid),\n                ]\n            )\n        }\n</code></pre>"},{"location":"operators/code/SQL/ValidateTables/","title":"ValidateTables","text":"<p>             Bases: <code>TransformOp</code></p> <p>Ensures that table and column names from response/predicted SQL are valid tables columns as per the schema definition.</p> <p>This is typically used with ParseSQL and ParseCreateStatements.</p> <p>Attributes:</p> Name Type Description <code>col_in_response_tables</code> <code>str</code> <p>Column containing response SQL tables and columns as a json dict Table -&gt; [columns].</p> <code>col_in_schema_tables</code> <code>str</code> <p>Column containing schema definition tables and columns as a json dict Table -&gt; [columns].</p> <code>col_out_is_tables_valid</code> <code>str</code> <p>Column to store if tables are valid.</p> <code>col_out_is_cols_valid</code> <code>str</code> <p>Column to store if columns are valid.</p> Source code in <code>uptrain/operators/code/sql.py</code> <pre><code>@register_op\nclass ValidateTables(TransformOp):\n    \"\"\"\n    Ensures that table and column names from response/predicted SQL are valid tables columns as per the schema\n    definition.\n\n    This is typically used with ParseSQL and ParseCreateStatements.\n\n    Attributes:\n        col_in_response_tables (str): Column containing response SQL tables and columns as a json dict Table -&gt; [columns].\n        col_in_schema_tables (str): Column containing schema definition tables and columns as a json dict Table -&gt; [columns].\n        col_out_is_tables_valid (str): Column to store if tables are valid.\n        col_out_is_cols_valid (str): Column to store if columns are valid.\n\n    \"\"\"\n\n    col_in_response_tables: str\n    col_in_schema_tables: str\n    col_out_is_tables_valid: str\n    col_out_is_cols_valid: str\n\n    def setup(self, settings: Settings):\n        return self\n\n    def run(self, data: pl.DataFrame) -&gt; TYPE_TABLE_OUTPUT:\n        response_tables = data.get_column(self.col_in_response_tables)\n        schema_tables = data.get_column(self.col_in_schema_tables)\n        results = []\n        results_column = []\n        for response_table, schema_table in zip(response_tables, schema_tables):\n            is_valid_table = True\n            is_valid_column = True\n            # deserialize json\n            s = json.loads(schema_table)\n            r = json.loads(response_table)\n            columns_list = [columns for table, columns in s.items()]\n            all_columns = set(itertools.chain(*columns_list))\n            for table, columns in r.items():\n                is_valid_table &amp;= table == PLACEHOLDER_TABLE or table in s\n                # TODO: handle aliases and do lineage check\n                is_valid_column = (\n                    is_valid_column\n                    and is_valid_table\n                    and (\n                        (\n                            table != PLACEHOLDER_TABLE\n                            and set(columns).issubset(set(s[table]))\n                        )\n                        or (\n                            table == PLACEHOLDER_TABLE\n                            and set(columns).issubset(all_columns)\n                        )\n                    )\n                )\n\n            results.append(is_valid_table)\n            results_column.append(is_valid_column)\n        return {\n            \"output\": data.with_columns(\n                [\n                    pl.Series(self.col_out_is_tables_valid, results),\n                    pl.Series(self.col_out_is_cols_valid, results_column),\n                ]\n            )\n        }\n</code></pre>"},{"location":"operators/language/ContextRelevance/","title":"ContextRelevance","text":"<p>             Bases: <code>ColumnOp</code></p> <p>Grade how relevant the context was to the question asked.</p> <p>Attributes:</p> Name Type Description <code>col_question</code> <code>str</code> <p>(str) Column Name for the stored questions</p> <code>col_context</code> <code>str</code> <p>(str) Coloumn name for stored context</p> <code>col_out</code> <code>str</code> <p>Column name to output scores</p> <code>scenario_description</code> <code>str</code> <p>Optional scenario description to incorporate in the evaluation prompt</p> <code>score_mapping</code> <code>dict</code> <p>Mapping of different grades to float scores</p> <p>Raises:</p> Type Description <code>Exception</code> <p>Raises exception for any failed evaluation attempts</p> Source code in <code>uptrain/operators/language/context_quality.py</code> <pre><code>@register_op\nclass ContextRelevance(ColumnOp):\n    \"\"\"\n    Grade how relevant the context was to the question asked.\n\n    Attributes:\n        col_question: (str) Column Name for the stored questions\n        col_context: (str) Coloumn name for stored context\n        col_out (str): Column name to output scores\n        scenario_description (str): Optional scenario description to incorporate in the evaluation prompt\n        score_mapping (dict): Mapping of different grades to float scores\n\n    Raises:\n        Exception: Raises exception for any failed evaluation attempts\n\n    \"\"\"\n\n    col_question: str = \"question\"\n    col_context: str = \"context\"\n    col_out: str = \"score_context_relevance\"\n    scenario_description: t.Optional[str] = None\n    score_mapping: dict = {\"A\": 1.0, \"B\": 0.5, \"C\": 0.0}\n\n    def setup(self, settings: t.Optional[Settings] = None):\n        from uptrain.framework.remote import APIClient\n\n        assert settings is not None\n        self.settings = settings\n        if self.settings.evaluate_locally:\n            self._api_client = LLMMulticlient(settings)\n        else:\n            self._api_client = APIClient(settings)\n        return self\n\n    def run(self, data: pl.DataFrame) -&gt; TYPE_TABLE_OUTPUT:\n        data_send = polars_to_json_serializable_dict(data)\n        for row in data_send:\n            row[\"question\"] = row.pop(self.col_question)\n            row[\"context\"] = row.pop(self.col_context)\n\n        try:\n            if self.settings.evaluate_locally:\n                results = self.evaluate_local(data_send)\n            else:\n                results = self._api_client.evaluate(\n                    \"context_relevance\",\n                    data_send,\n                    {\"scenario_description\": self.scenario_description},\n                )\n        except Exception as e:\n            logger.error(f\"Failed to run evaluation for `ContextRelevance`: {e}\")\n            raise e\n\n        assert results is not None\n        return {\n            \"output\": data.with_columns(\n                pl.from_dicts(results).rename({\"score_context_relevance\": self.col_out})\n            )\n        }\n\n    def context_relevance_classify_validate_func(self, llm_output):\n        is_correct = True\n        is_correct = is_correct and (\"Choice\" in json.loads(llm_output))\n        is_correct = is_correct and json.loads(llm_output)[\"Choice\"] in [\"A\", \"B\", \"C\"]\n        return is_correct\n\n    def context_relevance_cot_validate_func(self, llm_output):\n        is_correct = self.context_relevance_classify_validate_func(llm_output)\n        is_correct = is_correct and (\"Reasoning\" in json.loads(llm_output))\n        return is_correct\n\n    def evaluate_local(self, data):\n        \"\"\"\n        Our methodology is based on the model grade evaluation introduced by openai evals.\n        \"\"\"\n\n        self.scenario_description, scenario_vars = parse_scenario_description(\n            self.scenario_description\n        )\n        input_payloads = []\n        if self.settings.eval_type == \"basic\":\n            few_shot_examples = CONTEXT_RELEVANCE_FEW_SHOT__CLASSIFY\n            output_format = CONTEXT_RELEVANCE_OUTPUT_FORMAT__CLASSIFY\n            validation_func = self.context_relevance_classify_validate_func\n            prompting_instructions = CLASSIFY\n        elif self.settings.eval_type == \"cot\":\n            few_shot_examples = CONTEXT_RELEVANCE_FEW_SHOT__COT\n            output_format = CONTEXT_RELEVANCE_OUTPUT_FORMAT__COT\n            validation_func = self.context_relevance_cot_validate_func\n            prompting_instructions = CHAIN_OF_THOUGHT\n        else:\n            raise ValueError(\n                f\"Invalid eval_type: {self.settings.eval_type}. Must be either 'basic' or 'cot'\"\n            )\n\n        for idx, row in enumerate(data):\n            kwargs = row\n            kwargs.update(\n                {\n                    \"output_format\": output_format,\n                    \"prompting_instructions\": prompting_instructions,\n                    \"few_shot_examples\": few_shot_examples,\n                }\n            )\n            try:\n                grading_prompt_template = CONTEXT_RELEVANCE_PROMPT_TEMPLATE.replace(\n                    \"{scenario_description}\", self.scenario_description\n                ).format(**kwargs)\n            except KeyError as e:\n                raise KeyError(\n                    f\"Missing required attribute(s) for scenario description: {e}\"\n                )\n            input_payloads.append(\n                self._api_client.make_payload(idx, grading_prompt_template)\n            )\n        output_payloads = self._api_client.fetch_responses(\n            input_payloads, validation_func\n        )\n\n        results = []\n        for res in output_payloads:\n            idx = res.metadata[\"index\"]\n            output = {\n                \"score_context_relevance\": None,\n                \"explanation_context_relevance\": None,\n            }\n            try:\n                score = self.score_mapping[\n                    json.loads(res.response.choices[0].message.content)[\"Choice\"]\n                ]\n                output[\"score_context_relevance\"] = float(score)\n                output[\"explanation_context_relevance\"] = res.response.choices[\n                    0\n                ].message.content\n            except Exception:\n                logger.error(\n                    f\"Error when processing payload at index {idx}: {res.error}\"\n                )\n            results.append((idx, output))\n        results = [val for _, val in sorted(results, key=lambda x: x[0])]\n\n        return results\n</code></pre>"},{"location":"operators/language/ContextRelevance/#uptrain.operators.ContextRelevance.evaluate_local","title":"<code>evaluate_local(data)</code>","text":"<p>Our methodology is based on the model grade evaluation introduced by openai evals.</p> Source code in <code>uptrain/operators/language/context_quality.py</code> <pre><code>def evaluate_local(self, data):\n    \"\"\"\n    Our methodology is based on the model grade evaluation introduced by openai evals.\n    \"\"\"\n\n    self.scenario_description, scenario_vars = parse_scenario_description(\n        self.scenario_description\n    )\n    input_payloads = []\n    if self.settings.eval_type == \"basic\":\n        few_shot_examples = CONTEXT_RELEVANCE_FEW_SHOT__CLASSIFY\n        output_format = CONTEXT_RELEVANCE_OUTPUT_FORMAT__CLASSIFY\n        validation_func = self.context_relevance_classify_validate_func\n        prompting_instructions = CLASSIFY\n    elif self.settings.eval_type == \"cot\":\n        few_shot_examples = CONTEXT_RELEVANCE_FEW_SHOT__COT\n        output_format = CONTEXT_RELEVANCE_OUTPUT_FORMAT__COT\n        validation_func = self.context_relevance_cot_validate_func\n        prompting_instructions = CHAIN_OF_THOUGHT\n    else:\n        raise ValueError(\n            f\"Invalid eval_type: {self.settings.eval_type}. Must be either 'basic' or 'cot'\"\n        )\n\n    for idx, row in enumerate(data):\n        kwargs = row\n        kwargs.update(\n            {\n                \"output_format\": output_format,\n                \"prompting_instructions\": prompting_instructions,\n                \"few_shot_examples\": few_shot_examples,\n            }\n        )\n        try:\n            grading_prompt_template = CONTEXT_RELEVANCE_PROMPT_TEMPLATE.replace(\n                \"{scenario_description}\", self.scenario_description\n            ).format(**kwargs)\n        except KeyError as e:\n            raise KeyError(\n                f\"Missing required attribute(s) for scenario description: {e}\"\n            )\n        input_payloads.append(\n            self._api_client.make_payload(idx, grading_prompt_template)\n        )\n    output_payloads = self._api_client.fetch_responses(\n        input_payloads, validation_func\n    )\n\n    results = []\n    for res in output_payloads:\n        idx = res.metadata[\"index\"]\n        output = {\n            \"score_context_relevance\": None,\n            \"explanation_context_relevance\": None,\n        }\n        try:\n            score = self.score_mapping[\n                json.loads(res.response.choices[0].message.content)[\"Choice\"]\n            ]\n            output[\"score_context_relevance\"] = float(score)\n            output[\"explanation_context_relevance\"] = res.response.choices[\n                0\n            ].message.content\n        except Exception:\n            logger.error(\n                f\"Error when processing payload at index {idx}: {res.error}\"\n            )\n        results.append((idx, output))\n    results = [val for _, val in sorted(results, key=lambda x: x[0])]\n\n    return results\n</code></pre>"},{"location":"operators/language/DocsLinkVersion/","title":"DocsLinkVersion","text":"<p>             Bases: <code>ColumnOp</code></p> <p>Operator to extract version numbers from URLs in text data.</p> <p>Attributes:</p> Name Type Description <code>domain_name</code> <code>str</code> <p>Filter down to links from this domain. Defaults to None.</p> <code>col_in_text</code> <code>str</code> <p>The name of the input column containing the text data.</p> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary containing the extracted version numbers.</p> Example <pre><code>import polars as pl\nfrom uptrain.operators import DocsLinkVersion\n\n# Create a DataFrame\ndf = pl.DataFrame({\n    \"text\": [\"https://docs.streamlit.io/1.9.0/library/api-reference/charts/st.plotly_chart#stplotly_chart\", \"No version here\"]\n})\n\n# Create an instance of the DocsLinkVersion class\nlink_op = DocsLinkVersion(col_in_text=\"text\")\n\n# Extract the version numbers\nversions = link_op.run(df)[\"output\"]\n\n# Print the extracted version numbers\nprint(versions)\n</code></pre> Output <pre><code>shape: (2,)\nSeries: '_col_0' [str]\n[\n        \"1.9.0\"\n        null\n]\n</code></pre> Source code in <code>uptrain/operators/language/text.py</code> <pre><code>@register_op\nclass DocsLinkVersion(ColumnOp):\n    \"\"\"\n    Operator to extract version numbers from URLs in text data.\n\n    Attributes:\n        domain_name (str, optional): Filter down to links from this domain. Defaults to None.\n        col_in_text (str): The name of the input column containing the text data.\n\n    Returns:\n        dict: A dictionary containing the extracted version numbers.\n\n    Example:\n        ```\n        import polars as pl\n        from uptrain.operators import DocsLinkVersion\n\n        # Create a DataFrame\n        df = pl.DataFrame({\n            \"text\": [\"https://docs.streamlit.io/1.9.0/library/api-reference/charts/st.plotly_chart#stplotly_chart\", \"No version here\"]\n        })\n\n        # Create an instance of the DocsLinkVersion class\n        link_op = DocsLinkVersion(col_in_text=\"text\")\n\n        # Extract the version numbers\n        versions = link_op.run(df)[\"output\"]\n\n        # Print the extracted version numbers\n        print(versions)\n        ```\n\n    Output:\n        ```\n        shape: (2,)\n        Series: '_col_0' [str]\n        [\n                \"1.9.0\"\n                null\n        ]\n        ```\n\n    \"\"\"\n\n    domain_name: t.Optional[str] = None  # filter down to links from this domain\n    col_in_text: str\n    col_out: str = \"docs_link_version\"\n\n    def setup(self, settings: Settings):\n        return self\n\n    def run(self, data: pl.DataFrame) -&gt; TYPE_TABLE_OUTPUT:\n        def fetch_version(text):\n            for link in extract_links(text, self.domain_name):\n                v = extract_version(link)\n                if v is not None:\n                    return v\n            return None\n\n        results = data.get_column(self.col_in_text).apply(fetch_version)\n        return {\"output\": data.with_columns([results.alias(self.col_out)])}\n</code></pre>"},{"location":"operators/language/Embedding/","title":"Embedding","text":"<p>             Bases: <code>ColumnOp</code></p> <p>Column operation that generates embeddings for text using pre-trained models.</p> <p>Attributes:</p> Name Type Description <code>model</code> <code>Literal['MiniLM-L6-v2', 'instructor-xl', 'mpnet-base-v2', 'bge-large-zh-v1.5']</code> <p>The name of the pre-trained model to use.</p> <code>col_in_text</code> <code>str</code> <p>The name of the text column in the DataFrame.</p> <code>col_out</code> <code>str</code> <p>The name of the output column in the DataFrame.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If the specified model is not supported.</p> <p>Returns:</p> Name Type Description <code>TYPE_TABLE_OUTPUT</code> <p>A dictionary containing the generated embeddings.</p> Example <pre><code>import polars as pl\nfrom uptrain.operators import Embedding\n\n# Create a DataFrame\ndf = pl.DataFrame({\n    \"text\": [\"This is the first sentence.\", \"Here is another sentence.\"]\n})\n\n# Create an instance of the Embedding class\nembedding_op = Embedding(model=\"MiniLM-L6-v2\", col_in_text=\"text\", col_out=\"embedding\")\n\n# Set up the Embedding operator\nembedding_op.setup()\n\n# Generate embeddings for the text column\nembeddings = embedding_op.run(df)[\"output\"]\n\n# Print the embeddings\nprint(embeddings)\n</code></pre> Output <pre><code>shape: (2,)\nSeries: '_col_0' [list[f32]]\n[\n        [0.098575, 0.056978, \u2026 -0.071038]\n        [0.072772, 0.073564, \u2026 -0.043947]\n]\n</code></pre> Source code in <code>uptrain/operators/embedding/embedding.py</code> <pre><code>@register_op\nclass Embedding(ColumnOp):\n    \"\"\"\n    Column operation that generates embeddings for text using pre-trained models.\n\n    Attributes:\n        model (Literal[\"MiniLM-L6-v2\", \"instructor-xl\", \"mpnet-base-v2\", \"bge-large-zh-v1.5\"]): The name of the pre-trained model to use.\n        col_in_text (str): The name of the text column in the DataFrame.\n        col_out (str): The name of the output column in the DataFrame.\n\n    Raises:\n        Exception: If the specified model is not supported.\n\n    Returns:\n        TYPE_TABLE_OUTPUT: A dictionary containing the generated embeddings.\n\n    Example:\n        ```\n        import polars as pl\n        from uptrain.operators import Embedding\n\n        # Create a DataFrame\n        df = pl.DataFrame({\n            \"text\": [\"This is the first sentence.\", \"Here is another sentence.\"]\n        })\n\n        # Create an instance of the Embedding class\n        embedding_op = Embedding(model=\"MiniLM-L6-v2\", col_in_text=\"text\", col_out=\"embedding\")\n\n        # Set up the Embedding operator\n        embedding_op.setup()\n\n        # Generate embeddings for the text column\n        embeddings = embedding_op.run(df)[\"output\"]\n\n        # Print the embeddings\n        print(embeddings)\n        ```\n\n    Output:\n        ```\n        shape: (2,)\n        Series: '_col_0' [list[f32]]\n        [\n                [0.098575, 0.056978, \u2026 -0.071038]\n                [0.072772, 0.073564, \u2026 -0.043947]\n        ]\n        ```\n\n    \"\"\"\n\n    model: str = \"\"  # t.Literal[\"MiniLM-L6-v2\", \"instructor-xl\", \"mpnet-base-v2\", \"bge-large-zh-v1.5\", \"instructor-large\"]\n    col_in_text: str = \"text\"\n    col_out: str = \"embedding\"\n    batch_size: int = 128\n\n    def setup(self, settings: Settings):\n        self._compute_method = settings.embedding_compute_method\n        if settings.embedding_compute_method == \"local\":\n            if self.model == \"instructor-xl\":\n                InstructorEmbedding = lazy_load_dep(\n                    \"InstructorEmbedding\", \"InstructorEmbedding\"\n                )\n                self._model_obj = InstructorEmbedding.INSTRUCTOR(self.model)  # type: ignore\n            elif self.model == \"MiniLM-L6-v2\":\n                sentence_transformers = lazy_load_dep(\n                    \"sentence_transformers\", \"sentence-transformers\"\n                )\n                self._model_obj = sentence_transformers.SentenceTransformer(\n                    \"sentence-transformers/all-MiniLM-L6-v2\"\n                )  # type: ignore\n            elif self.model == \"mpnet-base-v2\":\n                sentence_transformers = lazy_load_dep(\n                    \"sentence_transformers\", \"sentence-transformers\"\n                )\n                self._model_obj = sentence_transformers.SentenceTransformer(\n                    \"sentence-transformers/all-mpnet-base-v2\"\n                )\n            elif self.model == \"bge-large-zh-v1.5\":\n                sentence_transformers = lazy_load_dep(\n                    \"sentence_transformers\", \"sentence-transformers\"\n                )\n                self._model_obj = sentence_transformers.SentenceTransformer(\n                    \"BAAI/bge-large-zh-v1.5\"\n                )\n            else:\n                sentence_transformers = lazy_load_dep(\n                    \"sentence_transformers\", \"sentence-transformers\"\n                )\n                self._model_obj = sentence_transformers.SentenceTransformer(\n                    \"sentence-transformers/\" + self.model\n                )\n                # raise Exception(f\"Embeddings model: {self.model} is not supported yet.\")\n        elif settings.embedding_compute_method == \"replicate\":\n            replicate = lazy_load_dep(\"replicate\", \"replicate\")\n            self._model_obj = replicate.Client(api_token=settings.replicate_api_token)\n            if self.model == \"mpnet-base-v2\":\n                self._model_url = \"replicate/all-mpnet-base-v2:b6b7585c9640cd7a9572c6e129c9549d79c9c31f0d3fdce7baac7c67ca38f305\"\n            elif self.model == \"bge-large-zh-v1.5\":\n                self._model_url = \"nateraw/bge-large-en-v1.5:9cf9f015a9cb9c61d1a2610659cdac4a4ca222f2d3707a68517b18c198a9add1\"\n            else:\n                raise Exception(f\"Embeddings model: {self.model} is not supported yet.\")\n        elif settings.embedding_compute_method == \"api\":\n            self._model_obj = {\n                \"embedding_model_url\": settings.embedding_model_url,\n                \"model\": self.model,\n                \"authorization_key\": settings.embedding_model_api_token,\n            }\n        return self\n\n    def run(self, data: pl.DataFrame) -&gt; TYPE_TABLE_OUTPUT:\n        text = data.get_column(self.col_in_text)\n        if self.model in [\"instructor-xl\", \"instructor-large\", \"bge-large-zh-v1.5\"]:\n            inputs = [[\"Represent the sentence: \", x] for x in text]\n        elif self.model == \"MiniLM-L6-v2\" or self.model == \"mpnet-base-v2\":\n            inputs = list(text)\n        else:\n            # raise Exception(\"Embeddings model not supported\")\n            inputs = list(text)\n\n        results = []\n        BATCH_SIZE = self.batch_size\n        if self.model == \"bge-large-zh-v1.5\":\n            emb_length = 1024\n        elif self.model in [\"instructor-xl\", \"instructor-large\"]:\n            emb_length = 768\n        for idx in range(int(np.ceil(len(inputs) / BATCH_SIZE))):\n            if self._compute_method == \"local\":\n                run_res = self._model_obj.encode(\n                    inputs[idx * BATCH_SIZE : (idx + 1) * BATCH_SIZE]\n                )\n            elif self._compute_method == \"replicate\":\n                run_res = [\n                    x[\"embedding\"]\n                    for x in self._model_obj.run(\n                        self._model_url,\n                        input={\n                            \"text_batch\": json.dumps(\n                                inputs[idx * BATCH_SIZE : (idx + 1) * BATCH_SIZE]\n                            )\n                        },\n                    )\n                ]\n            elif self._compute_method == \"api\":\n                try:\n                    run_res = [\n                        x[\"embedding\"]\n                        for x in requests.post(\n                            self._model_obj[\"embedding_model_url\"],\n                            json={\n                                \"model\": self.model,\n                                \"input\": inputs[\n                                    idx * BATCH_SIZE : (idx + 1) * BATCH_SIZE\n                                ],\n                            },\n                            headers={\n                                \"Authorization\": f\"Bearer {self._model_obj['authorization_key']}\"\n                            },\n                        ).json()[\"data\"]\n                    ]\n                    emb_length = len(run_res[0])\n                except:\n                    run_res = []\n                    for elem_idx in range(idx * BATCH_SIZE, (idx + 1) * BATCH_SIZE):\n                        if elem_idx &lt; len(inputs):\n                            try:\n                                run_res.extend(\n                                    [\n                                        x[\"embedding\"]\n                                        for x in requests.post(\n                                            self._model_obj[\"embedding_model_url\"],\n                                            json={\n                                                \"model\": self.model,\n                                                \"input\": [inputs[elem_idx]],\n                                            },\n                                            headers={\n                                                \"Authorization\": f\"Bearer {self._model_obj['authorization_key']}\"\n                                            },\n                                        ).json()[\"data\"]\n                                    ]\n                                )\n                            except Exception as e:\n                                logger.error(f\"Error while computing embeddings: {e}\")\n                                if len(run_res) != 0:\n                                    run_res.append(run_res[-1])\n                                else:\n                                    run_res.append([0] * emb_length)\n            results.extend(run_res)\n            logger.info(\n                f\"Running batch: {idx} out of {int(np.ceil(len(inputs)/BATCH_SIZE))} for operator Embedding\"\n            )\n        return {\"output\": data.with_columns([pl.Series(results).alias(self.col_out)])}\n</code></pre>"},{"location":"operators/language/GrammarScore/","title":"GrammarScore","text":"<p>             Bases: <code>ColumnOp</code></p> <p>Operator to test the grammatical correctness of sentences using the OpenAI GPT-3.5-turbo language model.</p> <p>Attributes:</p> Name Type Description <code>col_in_text</code> <code>str</code> <p>The name of the input column containing the sentences to evaluate.</p> <code>col_out</code> <code>str</code> <p>The name of the output column containing the grammar scores.</p> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary containing the grammar scores for each input sentence.</p> Source code in <code>uptrain/operators/language/grammar.py</code> <pre><code>@register_op\nclass GrammarScore(ColumnOp):\n    \"\"\"\n    Operator to test the grammatical correctness of sentences using the OpenAI GPT-3.5-turbo language model.\n\n    Attributes:\n        col_in_text (str): The name of the input column containing the sentences to evaluate.\n        col_out (str): The name of the output column containing the grammar scores.\n\n    Returns:\n        dict: A dictionary containing the grammar scores for each input sentence.\n\n    \"\"\"\n\n    col_in_text: str = \"text\"\n    col_out: str = \"grammar_score\"\n\n    def setup(self, settings: t.Optional[Settings] = None):\n        self._api_client = LLMMulticlient(settings=settings)\n        self._settings = settings\n        return self\n\n    def _make_payload(self, id: t.Any, text: str) -&gt; Payload:\n        data = {\n            \"model\": \"gpt-3.5-turbo\",\n            \"messages\": [\n                {\n                    \"role\": \"system\",\n                    \"content\": \"You are a grammatical correctness evaluator who produces only a number and no explanation.\",\n                },\n                {\n                    \"role\": \"user\",\n                    \"content\": \"Score following sentence on grammatical correctness on a scale of 0 to 100: \\n\\n {statement}\".format(\n                        statement=text\n                    ),\n                },\n            ],\n        }\n        if self._settings.seed is not None:\n            data[\"seed\"] = self._settings.seed\n\n        return Payload(\n            data=data,\n            metadata={\"index\": id},\n        )\n\n    def run(self, data: pl.DataFrame) -&gt; TYPE_TABLE_OUTPUT:\n        text_ser = data.get_column(self.col_in_text)\n        input_payloads = [\n            self._make_payload(idx, text) for idx, text in enumerate(text_ser)\n        ]\n        output_payloads = self._api_client.fetch_responses(input_payloads)\n\n        results = []\n        for res in output_payloads:\n            assert (\n                res is not None\n            ), \"Response should not be None, we should've handled exceptions beforehand.\"\n            idx = res.metadata[\"index\"]\n            if res.error is not None:\n                logger.error(\n                    f\"Error when processing payload at index {idx}: {res.error}\"\n                )\n                results.append((idx, None))\n            else:\n                resp_text = res.response.choices[0].message.content\n                number = int(re.findall(r\"\\d+\", resp_text)[0])\n                results.append((idx, number))\n\n        result_scores = pl.Series(\n            [val for _, val in sorted(results, key=lambda x: x[0])]\n        )\n        return {\"output\": data.with_columns([result_scores.alias(self.col_out)])}\n</code></pre>"},{"location":"operators/language/GuidelineAdherenceScore/","title":"GuidelineAdherenceScore","text":"<p>             Bases: <code>ColumnOp</code></p> <p>Grade if the LLM agent follows the given guideline or not.</p> <p>Attributes:     col_question (str): Column name for the stored questions     col_response (str): Column name for the stored responses     guideline (str): Guideline to be followed     guideline_name (str): Name the given guideline to be used in the score column name     response_schema (str | None): Schema of the response if it is in form of a json string     col_out (str): Column name to output scores     score_mapping (dict): Mapping of different grades to float scores</p> <p>Raises:</p> Type Description <code>Exception</code> <p>Raises exception for any failed evaluation attempts</p> Source code in <code>uptrain/operators/language/guideline.py</code> <pre><code>@register_op\nclass GuidelineAdherenceScore(ColumnOp):\n    \"\"\"\n    Grade if the LLM agent follows the given guideline or not.\n\n     Attributes:\n        col_question (str): Column name for the stored questions\n        col_response (str): Column name for the stored responses\n        guideline (str): Guideline to be followed\n        guideline_name (str): Name the given guideline to be used in the score column name\n        response_schema (str | None): Schema of the response if it is in form of a json string\n        col_out (str): Column name to output scores\n        score_mapping (dict): Mapping of different grades to float scores\n\n    Raises:\n        Exception: Raises exception for any failed evaluation attempts\n\n    \"\"\"\n\n    col_question: str = \"question\"\n    col_response: str = \"response\"\n    guideline: str\n    guideline_name: str = \"guideline\"\n    response_schema: t.Union[str, None] = None\n    col_out: str = \"score_guideline_adherence\"\n    scenario_description: t.Union[str, list[str], None] = None\n    score_mapping: dict = {\"A\": 1.0, \"B\": 0.0}\n\n    def setup(self, settings: t.Optional[Settings] = None):\n        from uptrain.framework.remote import APIClient\n\n        assert settings is not None\n        self.settings = settings\n        if self.settings.evaluate_locally:\n            self._api_client = LLMMulticlient(settings)\n        else:\n            self._api_client = APIClient(settings)\n        return self\n\n    def run(self, data: pl.DataFrame) -&gt; TYPE_TABLE_OUTPUT:\n        data_send = polars_to_json_serializable_dict(data)\n        for row in data_send:\n            row[\"question\"] = row.pop(self.col_question)\n            row[\"response\"] = row.pop(self.col_response)\n\n        try:\n            if self.settings.evaluate_locally:\n                results = self.evaluate_local(data_send)\n            else:\n                results = self._api_client.evaluate(\n                    \"GuidelineAdherence\",\n                    data_send,\n                    {\n                        \"guideline\": self.guideline,\n                        \"guideline_name\": self.guideline_name,\n                        \"response_schema\": self.response_schema,\n                        \"scenario_description\": self.scenario_description,\n                    },\n                )\n        except Exception as e:\n            logger.error(f\"Failed to run evaluation for `GuidelineAdherenceScore`: {e}\")\n            raise e\n\n        assert results is not None\n        return {\n            \"output\": data.with_columns(\n                pl.from_dicts(results).rename(\n                    {f\"score_{self.guideline_name}_adherence\": self.col_out}\n                )\n            )\n        }\n\n    def guideline_adherence_classify_validate_func(self, llm_output):\n        is_correct = True\n        is_correct = is_correct and (\"Choice\" in json.loads(llm_output))\n        is_correct = is_correct and json.loads(llm_output)[\"Choice\"] in [\"A\", \"B\"]\n        return is_correct\n\n    def guideline_adherence_cot_validate_func(self, llm_output):\n        is_correct = self.guideline_adherence_classify_validate_func(llm_output)\n        is_correct = is_correct and (\"Reasoning\" in json.loads(llm_output))\n        return is_correct\n\n    def evaluate_local(self, data):\n        \"\"\"\n        Our methodology is based on the model grade evaluation introduced by openai evals.\n        \"\"\"\n\n        self.scenario_description, scenario_vars = parse_scenario_description(\n            self.scenario_description\n        )\n        input_payloads = []\n        if self.settings.eval_type == \"basic\":\n            few_shot_examples = GUIDELINE_ADHERENCE_FEW_SHOT__CLASSIFY\n            output_format = GUIDELINE_ADHERENCE_OUTPUT_FORMAT__CLASSIFY\n            validation_func = self.guideline_adherence_classify_validate_func\n            prompting_instructions = CLASSIFY\n        elif self.settings.eval_type == \"cot\":\n            few_shot_examples = GUIDELINE_ADHERENCE_FEW_SHOT__COT\n            output_format = GUIDELINE_ADHERENCE_OUTPUT_FORMAT__COT\n            validation_func = self.guideline_adherence_cot_validate_func\n            prompting_instructions = CHAIN_OF_THOUGHT\n        else:\n            raise ValueError(\n                f\"Invalid eval_type: {self.settings.eval_type}. Must be either 'basic' or 'cot'\"\n            )\n\n        for idx, row in enumerate(data):\n            kwargs = row\n            kwargs.update(\n                {\n                    \"output_format\": output_format,\n                    \"prompting_instructions\": prompting_instructions,\n                    \"few_shot_examples\": few_shot_examples,\n                    \"guideline\": self.guideline,\n                }\n            )\n            try:\n                grading_prompt_template = GUIDELINE_ADHERENCE_PROMPT_TEMPLATE.replace(\n                    \"{scenario_description}\", self.scenario_description\n                ).format(**kwargs)\n            except KeyError as e:\n                raise KeyError(\n                    f\"Missing required attribute(s) for scenario description: {e}\"\n                )\n            input_payloads.append(\n                self._api_client.make_payload(idx, grading_prompt_template)\n            )\n        output_payloads = self._api_client.fetch_responses(\n            input_payloads, validation_func\n        )\n\n        results = []\n\n        for res in output_payloads:\n            idx = res.metadata[\"index\"]\n            output = {\n                \"score_guideline_adherence\": None,\n                \"explanation_guideline_adherence\": None,\n            }\n            try:\n                score = self.score_mapping[\n                    json.loads(res.response.choices[0].message.content)[\"Choice\"]\n                ]\n                output[\"score_guideline_adherence\"] = float(score)\n                output[\"explanation_guideline_adherence\"] = res.response.choices[\n                    0\n                ].message.content\n            except Exception:\n                logger.error(\n                    f\"Error when processing payload at index {idx}: {res.error}\"\n                )\n            results.append((idx, output))\n\n        results = [val for _, val in sorted(results, key=lambda x: x[0])]\n        return results\n</code></pre>"},{"location":"operators/language/GuidelineAdherenceScore/#uptrain.operators.GuidelineAdherenceScore.evaluate_local","title":"<code>evaluate_local(data)</code>","text":"<p>Our methodology is based on the model grade evaluation introduced by openai evals.</p> Source code in <code>uptrain/operators/language/guideline.py</code> <pre><code>def evaluate_local(self, data):\n    \"\"\"\n    Our methodology is based on the model grade evaluation introduced by openai evals.\n    \"\"\"\n\n    self.scenario_description, scenario_vars = parse_scenario_description(\n        self.scenario_description\n    )\n    input_payloads = []\n    if self.settings.eval_type == \"basic\":\n        few_shot_examples = GUIDELINE_ADHERENCE_FEW_SHOT__CLASSIFY\n        output_format = GUIDELINE_ADHERENCE_OUTPUT_FORMAT__CLASSIFY\n        validation_func = self.guideline_adherence_classify_validate_func\n        prompting_instructions = CLASSIFY\n    elif self.settings.eval_type == \"cot\":\n        few_shot_examples = GUIDELINE_ADHERENCE_FEW_SHOT__COT\n        output_format = GUIDELINE_ADHERENCE_OUTPUT_FORMAT__COT\n        validation_func = self.guideline_adherence_cot_validate_func\n        prompting_instructions = CHAIN_OF_THOUGHT\n    else:\n        raise ValueError(\n            f\"Invalid eval_type: {self.settings.eval_type}. Must be either 'basic' or 'cot'\"\n        )\n\n    for idx, row in enumerate(data):\n        kwargs = row\n        kwargs.update(\n            {\n                \"output_format\": output_format,\n                \"prompting_instructions\": prompting_instructions,\n                \"few_shot_examples\": few_shot_examples,\n                \"guideline\": self.guideline,\n            }\n        )\n        try:\n            grading_prompt_template = GUIDELINE_ADHERENCE_PROMPT_TEMPLATE.replace(\n                \"{scenario_description}\", self.scenario_description\n            ).format(**kwargs)\n        except KeyError as e:\n            raise KeyError(\n                f\"Missing required attribute(s) for scenario description: {e}\"\n            )\n        input_payloads.append(\n            self._api_client.make_payload(idx, grading_prompt_template)\n        )\n    output_payloads = self._api_client.fetch_responses(\n        input_payloads, validation_func\n    )\n\n    results = []\n\n    for res in output_payloads:\n        idx = res.metadata[\"index\"]\n        output = {\n            \"score_guideline_adherence\": None,\n            \"explanation_guideline_adherence\": None,\n        }\n        try:\n            score = self.score_mapping[\n                json.loads(res.response.choices[0].message.content)[\"Choice\"]\n            ]\n            output[\"score_guideline_adherence\"] = float(score)\n            output[\"explanation_guideline_adherence\"] = res.response.choices[\n                0\n            ].message.content\n        except Exception:\n            logger.error(\n                f\"Error when processing payload at index {idx}: {res.error}\"\n            )\n        results.append((idx, output))\n\n    results = [val for _, val in sorted(results, key=lambda x: x[0])]\n    return results\n</code></pre>"},{"location":"operators/language/KeywordDetector/","title":"KeywordDetector","text":"<p>             Bases: <code>ColumnOp</code></p> <p>Detect keywords in given text.</p> <p>Attributes:</p> Name Type Description <code>col_in_text</code> <code>str</code> <p>(str) The input text.</p> <code>keyword</code> <code>str</code> <p>(str) The keywords to be checked.</p> <code>col_out</code> <code>str</code> <p>str = \"keyword_detector\"</p> Source code in <code>uptrain/operators/language/text.py</code> <pre><code>@register_op\nclass KeywordDetector(ColumnOp):\n    \"\"\"\n    Detect keywords in given text.\n\n    Attributes:\n        col_in_text: (str) The input text.\n        keyword: (str) The keywords to be checked.\n        col_out: str = \"keyword_detector\"\n\n\n\n    \"\"\"\n\n    col_in_text: str\n    keyword: str\n    col_out: str = \"keyword_detector\"\n\n    def setup(self, settings: Settings):\n        return self\n\n    def run(self, data: pl.DataFrame) -&gt; TYPE_TABLE_OUTPUT:\n        results = data.get_column(self.col_in_text).apply(\n            lambda sql: self.keyword in sql\n        )\n        return {\"output\": data.with_columns([results.alias(self.col_out)])}\n</code></pre>"},{"location":"operators/language/LanguageCritique/","title":"LanguageCritique","text":"<p>             Bases: <code>ColumnOp</code></p> <p>Operator to score the fluency of machine generated responses. It provides a score for each of the aspects on a scale of 0 to 1, along with an explanation for the score.</p> <p>Attributes:</p> Name Type Description <code>col_response</code> <code>str</code> <p>The name of the input column containing response text</p> <code>col_out</code> <code>str</code> <p>The name of the output column containing the scores</p> <code>scenario_description</code> <code>str | None</code> <p>Optional scenario description to incorporate in the evaluation prompt</p> <code>score_mapping</code> <code>dict</code> <p>Mapping of different grades to float scores</p> <p>Raises:</p> Type Description <code>Exception</code> <p>Raises exception for any failed evaluation attempts</p> Source code in <code>uptrain/operators/language/language_quality.py</code> <pre><code>@register_op\nclass LanguageCritique(ColumnOp):\n    \"\"\"\n    Operator to score the fluency of machine generated responses.\n    It provides a score for each of the aspects on a scale of 0 to 1, along with an\n    explanation for the score.\n\n    Attributes:\n        col_response (str): The name of the input column containing response text\n        col_out (str): The name of the output column containing the scores\n        scenario_description (str | None): Optional scenario description to incorporate in the evaluation prompt\n        score_mapping (dict): Mapping of different grades to float scores\n\n    Raises:\n        Exception: Raises exception for any failed evaluation attempts\n\n    \"\"\"\n\n    col_response: str = \"response\"\n    col_out: str = \"score_language_critique\"\n    scenario_description: t.Optional[str] = None\n    score_mapping: dict = {\"A\": 1.0, \"B\": 0.5, \"C\": 0.0}\n\n    def setup(self, settings: t.Optional[Settings] = None):\n        from uptrain.framework.remote import APIClient\n\n        assert settings is not None\n        self.settings = settings\n        if self.settings.evaluate_locally:\n            self._api_client = LLMMulticlient(settings)\n        else:\n            self._api_client = APIClient(settings)\n        return self\n\n    def run(self, data: pl.DataFrame) -&gt; TYPE_TABLE_OUTPUT:\n        data_send = polars_to_json_serializable_dict(data)\n        for row in data_send:\n            row[\"response\"] = row.pop(self.col_response)\n\n        try:\n            if self.settings.evaluate_locally:\n                results = self.evaluate_local(data_send)\n            else:\n                results = self._api_client.evaluate(\"critique_language\", data_send)\n        except Exception as e:\n            logger.error(f\"Failed to run evaluation for `LanguageCritique`: {e}\")\n            raise e\n\n        assert results is not None\n        return {\n            \"output\": data.with_columns(\n                pl.from_dicts(results).rename({\"score_language_critique\": self.col_out})\n            )\n        }\n\n    def critique_language_classify_validate_func(self, llm_output):\n        is_correct = True\n        is_correct = is_correct and (\"Choice\" in json.loads(llm_output))\n        is_correct = is_correct and json.loads(llm_output)[\"Choice\"] in [\"A\", \"B\", \"C\"]\n        return is_correct\n\n    def critique_language_cot_validate_func(self, llm_output):\n        is_correct = self.critique_language_classify_validate_func(llm_output)\n        is_correct = is_correct and (\"Reasoning\" in json.loads(llm_output))\n        return is_correct\n\n    def evaluate_local(self, data):\n        \"\"\"\n        Our methodology is based on the model grade evaluation introduced by openai evals.\n        \"\"\"\n\n        self.scenario_description, scenario_vars = parse_scenario_description(\n            self.scenario_description\n        )\n        input_payloads = []\n        if self.settings.eval_type == \"basic\":\n            few_shot_examples = LANGUAGE_FLUENCY_FEW_SHOT__CLASSIFY\n            output_format = LANGUAGE_FLUENCY_OUTPUT_FORMAT__CLASSIFY\n            validation_func = self.critique_language_classify_validate_func\n            prompting_instructions = CLASSIFY\n        elif self.settings.eval_type == \"cot\":\n            few_shot_examples = LANGUAGE_FLUENCY_FEW_SHOT__COT\n            output_format = LANGUAGE_FLUENCY_OUTPUT_FORMAT__COT\n            validation_func = self.critique_language_cot_validate_func\n            prompting_instructions = CHAIN_OF_THOUGHT\n        else:\n            raise ValueError(\n                f\"Invalid eval_type: {self.settings.eval_type}. Must be either 'basic' or 'cot'\"\n            )\n\n        for idx, row in enumerate(data):\n            kwargs = row\n            kwargs.update(\n                {\n                    \"output_format\": output_format,\n                    \"prompting_instructions\": prompting_instructions,\n                    \"few_shot_examples\": few_shot_examples,\n                }\n            )\n            try:\n                grading_prompt_template = LANGUAGE_FLUENCY_PROMPT_TEMPLATE.replace(\n                    \"{scenario_description}\", self.scenario_description\n                ).format(**kwargs)\n            except KeyError as e:\n                raise KeyError(\n                    f\"Missing required attribute(s) for scenario description: {e}\"\n                )\n            input_payloads.append(\n                self._api_client.make_payload(idx, grading_prompt_template)\n            )\n        output_payloads = self._api_client.fetch_responses(\n            input_payloads, validation_func\n        )\n\n        results = []\n        for res in output_payloads:\n            idx = res.metadata[\"index\"]\n            output = {\n                \"score_critique_language\": None,\n                \"explanation_critique_language\": None,\n            }\n            try:\n                score = self.score_mapping[\n                    json.loads(res.response.choices[0].message.content)[\"Choice\"]\n                ]\n                output[\"score_critique_language\"] = float(score)\n                output[\"explanation_critique_language\"] = res.response.choices[\n                    0\n                ].message.content\n            except Exception:\n                logger.error(\n                    f\"Error when processing payload at index {idx}: {res.error}\"\n                )\n            results.append((idx, output))\n\n        results = [val for _, val in sorted(results, key=lambda x: x[0])]\n\n        return results\n</code></pre>"},{"location":"operators/language/LanguageCritique/#uptrain.operators.LanguageCritique.evaluate_local","title":"<code>evaluate_local(data)</code>","text":"<p>Our methodology is based on the model grade evaluation introduced by openai evals.</p> Source code in <code>uptrain/operators/language/language_quality.py</code> <pre><code>def evaluate_local(self, data):\n    \"\"\"\n    Our methodology is based on the model grade evaluation introduced by openai evals.\n    \"\"\"\n\n    self.scenario_description, scenario_vars = parse_scenario_description(\n        self.scenario_description\n    )\n    input_payloads = []\n    if self.settings.eval_type == \"basic\":\n        few_shot_examples = LANGUAGE_FLUENCY_FEW_SHOT__CLASSIFY\n        output_format = LANGUAGE_FLUENCY_OUTPUT_FORMAT__CLASSIFY\n        validation_func = self.critique_language_classify_validate_func\n        prompting_instructions = CLASSIFY\n    elif self.settings.eval_type == \"cot\":\n        few_shot_examples = LANGUAGE_FLUENCY_FEW_SHOT__COT\n        output_format = LANGUAGE_FLUENCY_OUTPUT_FORMAT__COT\n        validation_func = self.critique_language_cot_validate_func\n        prompting_instructions = CHAIN_OF_THOUGHT\n    else:\n        raise ValueError(\n            f\"Invalid eval_type: {self.settings.eval_type}. Must be either 'basic' or 'cot'\"\n        )\n\n    for idx, row in enumerate(data):\n        kwargs = row\n        kwargs.update(\n            {\n                \"output_format\": output_format,\n                \"prompting_instructions\": prompting_instructions,\n                \"few_shot_examples\": few_shot_examples,\n            }\n        )\n        try:\n            grading_prompt_template = LANGUAGE_FLUENCY_PROMPT_TEMPLATE.replace(\n                \"{scenario_description}\", self.scenario_description\n            ).format(**kwargs)\n        except KeyError as e:\n            raise KeyError(\n                f\"Missing required attribute(s) for scenario description: {e}\"\n            )\n        input_payloads.append(\n            self._api_client.make_payload(idx, grading_prompt_template)\n        )\n    output_payloads = self._api_client.fetch_responses(\n        input_payloads, validation_func\n    )\n\n    results = []\n    for res in output_payloads:\n        idx = res.metadata[\"index\"]\n        output = {\n            \"score_critique_language\": None,\n            \"explanation_critique_language\": None,\n        }\n        try:\n            score = self.score_mapping[\n                json.loads(res.response.choices[0].message.content)[\"Choice\"]\n            ]\n            output[\"score_critique_language\"] = float(score)\n            output[\"explanation_critique_language\"] = res.response.choices[\n                0\n            ].message.content\n        except Exception:\n            logger.error(\n                f\"Error when processing payload at index {idx}: {res.error}\"\n            )\n        results.append((idx, output))\n\n    results = [val for _, val in sorted(results, key=lambda x: x[0])]\n\n    return results\n</code></pre>"},{"location":"operators/language/ModelGradeScore/","title":"ModelGradeScore","text":"<p>             Bases: <code>ColumnOp</code></p> <p>Operator to calculate the grade score of text completions using a custom prompt for grading. It is a wrapper using the same utilities from the OpenAI evals library, replacing just the completion call.</p> <p>Attributes:</p> Name Type Description <code>grading_prompt_template</code> <code>str</code> <p>Template for the grading prompt.</p> <code>eval_type</code> <code>Literal['cot_classify', 'classify', 'classify_cot']</code> <p>The type of evaluation for grading (\"cot_classify\" by default).</p> <code>choice_strings</code> <code>list[str]</code> <p>The list of choice strings for grading.</p> <code>choice_scores</code> <code>dict[str, float]</code> <p>The dictionary mapping choice strings to scores.</p> <code>context_vars</code> <code>dict[str, str]</code> <p>A dictionary mapping context variable names to corresponding columns in the input dataset.</p> Source code in <code>uptrain/operators/language/model_grade.py</code> <pre><code>@register_op\nclass ModelGradeScore(ColumnOp):\n    \"\"\"\n    Operator to calculate the grade score of text completions using a custom prompt\n    for grading. It is a wrapper using the same utilities from the OpenAI evals library,\n    replacing just the completion call.\n\n    Attributes:\n        grading_prompt_template (str): Template for the grading prompt.\n        eval_type (Literal[\"cot_classify\", \"classify\", \"classify_cot\"]): The type of evaluation for grading (\"cot_classify\" by default).\n        choice_strings (list[str]): The list of choice strings for grading.\n        choice_scores (dict[str, float]): The dictionary mapping choice strings to scores.\n        context_vars (dict[str, str]): A dictionary mapping context variable names to corresponding\n            columns in the input dataset.\n    \"\"\"\n\n    grading_prompt_template: str\n    eval_type: t.Literal[\n        \"cot_classify\", \"classify\", \"classify_cot\", \"tot_classify\", \"tot_score\"\n    ] = \"cot_classify\"\n    choice_strings: list[str]\n    choice_scores: dict[str, float]  # t.Union[dict[str, float], dict[str, list[float]]]\n    context_vars: dict[str, str]\n    col_out: t.Union[str, list[str]] = \"model_grade_score\"\n\n    def setup(self, settings: Settings):\n        self._api_client = LLMMulticlient(settings=settings)\n        self._settings = settings\n        self.model = settings.model.replace(\"azure/\", \"\")\n        if not (self.eval_type in [\"cot_classify\", \"tot_classify\", \"tot_score\"]):\n            raise Exception(\n                \"Only eval_type: cot_classify and tot_classify is supported for model grading check\"\n            )\n        for choice, score in self.choice_scores.items():\n            score = format(score, \".3f\")\n            if score[-1] == \"0\":\n                score = score[0:-1]\n                if score[-1] == \"0\":\n                    score = score[0:-1]\n            self.choice_scores[choice] = score\n        choice_scores = \"(\" + str(self.choice_scores)[1:-1] + \")\"\n        choice_scores_text = \"\"\n        for choice, score in self.choice_scores.items():\n            choice_scores_text += (\n                f\"If selected choice is {choice}, score should be {score}. \"\n            )\n        scores_text = \"(\" + \", \".join(list(self.choice_scores.values())) + \")\"\n        answer_prompt = ANSWER_PROMPTS[self.eval_type].format(\n            choice_scores=choice_scores,\n            choice_scores_text=choice_scores_text,\n            scores_text=scores_text,\n        )\n        self.grading_prompt_template += answer_prompt\n        return self\n\n    def _make_payload(self, id: t.Any, messages: list[dict]) -&gt; Payload:\n        if self._settings.seed is None:\n            return Payload(\n                data={\"model\": self.model, \"messages\": messages, \"temperature\": 0.2},\n                metadata={\"index\": id},\n            )\n        else:\n            return Payload(\n                data={\n                    \"model\": self.model,\n                    \"messages\": messages,\n                    \"temperature\": 0.2,\n                    \"seed\": self._settings.seed,\n                },\n                metadata={\"index\": id},\n            )\n\n    def get_choice_via_llm(self, text: str, grading_prompt_template: str) -&gt; str:\n        \"\"\"Queries LLM to get score from the text\"\"\"\n\n        prompt = f\"\"\"\n        Extract the score from the given text. The available choices and associated scores is present in the context.\n\n        Context: {grading_prompt_template}\n        Text: {text}\n\n        Score:\n        \"\"\"\n\n        payload = self._make_payload(0, [{\"role\": \"user\", \"content\": prompt}])\n        output_payload = self._api_client.fetch_responses([payload])[0]\n\n        try:\n            score = output_payload.response.choices[0].message.content\n            float(score)\n            return score\n        except:\n            return str(0.0)\n\n    def get_choice(\n        self,\n        text: str,\n        eval_type: str,\n        match_fn: Union[str, Callable],\n        choice_strings: Iterable[str],\n        choice_scores: dict = {},\n    ) -&gt; str:\n        \"\"\"Clean the answer string to a choice string to one of choice_strings. Return '__invalid__.' if no match.\"\"\"\n        if eval_type == \"tot_score\":\n            score = \"\"\n            if len(score) == 0:\n                scores_matches = re.findall(\n                    r\"(\\[Score\\]\\: [1-5]|Score: [1-5]|Score is [1-5])\", text\n                )\n                if len(scores_matches) != 0:\n                    score = str(scores_matches[0].split(\" \")[-1])\n\n            if len(score) == 0:\n                scores_matches = re.findall(\n                    r\"(\\[score\\]\\: [1-5]|score: [1-5]|score is [1-5])\", text\n                )\n                if len(scores_matches) != 0:\n                    score = str(scores_matches[0].split(\" (\")[-1])\n\n            if len(score) == 1:\n                return float(score)\n\n            logging.warn(\n                f\"Choices {choice_strings} not parsable for {eval_type}: {text}\"\n            )\n            return -5\n        elif eval_type == \"tot_classify\":\n            score = \"\"\n            if len(score) == 0:\n                scores_matches = re.findall(\n                    r\"(\\[Choice\\]\\: [a-zA-Z]|Choice: [a-zA-Z]|choice is [a-zA-Z])\", text\n                )\n                if len(scores_matches) != 0:\n                    score = str(scores_matches[0].split(\" \")[-1])\n\n            if len(score) == 0:\n                scores_matches = re.findall(\n                    r\"(\\[Choice\\]\\: \\([a-zA-Z]|Choice: \\([a-zA-Z]|choice is \\([a-zA-Z])\",\n                    text,\n                )\n                if len(scores_matches) != 0:\n                    score = str(scores_matches[0].split(\" (\")[-1])\n\n            if len(score) == 0:\n                scores_matches = re.findall(\n                    r\"(\\[Argument\\]\\: \\([a-zA-Z]|Argument: \\([a-zA-Z]|argument is \\([a-zA-Z])\",\n                    text,\n                )\n                if len(scores_matches) != 0:\n                    score = str(scores_matches[0].split(\" (\")[-1])\n\n            if len(score) == 1:\n                if score.upper() in choice_scores:\n                    return choice_scores[score.upper()]\n                elif score.lower() in choice_scores:\n                    return choice_scores[score.lower()]\n\n            logging.warn(\n                f\"Choices {choice_strings} not parsable for {eval_type}: {text}\"\n            )\n            return -1\n        else:\n            is_fn_extract_score = False\n            if match_fn == \"extract_score\":\n                is_fn_extract_score = True\n            else:\n                if isinstance(match_fn, str):\n                    match_fn = MATCH_FNS[match_fn]\n            lines = text.strip().split(\"\\n\")\n            if eval_type.startswith(\"cot_classify\"):\n                lines = lines[::-1]  # reverse lines\n            for line in lines:\n                line = line.strip()\n                if not line:\n                    continue\n                if is_fn_extract_score:\n                    if \".\" in line:\n                        part_before_decimal = line.split(\".\")[0][::-1]\n                        prev_char = \"\"\n                        for char in part_before_decimal:\n                            new_char = char + prev_char\n                            try:\n                                float(new_char)\n                            except:\n                                break\n                            prev_char = new_char\n                        part_before_decimal = prev_char\n\n                        part_after_decimal = line.split(\".\")[1]\n                        prev_char = \"\"\n                        for char in part_after_decimal:\n                            new_char = prev_char + char\n                            try:\n                                float(new_char)\n                            except:\n                                break\n                            prev_char = new_char\n                        part_after_decimal = prev_char\n                        choice = part_before_decimal + \".\" + part_after_decimal\n                        try:\n                            float(choice)\n                            if float(choice) &gt; 1.0 or float(choice) &lt; 0.0:\n                                return self.get_choice_via_llm(\n                                    text, self.grading_prompt_template\n                                )\n                            return str(choice)\n                        except:\n                            return self.get_choice_via_llm(\n                                text, self.grading_prompt_template\n                            )\n                    else:\n                        return self.get_choice_via_llm(\n                            text, self.grading_prompt_template\n                        )\n                else:\n                    line = \"\".join(c for c in line if c not in string.punctuation)\n                    if not line:\n                        continue\n                    for choice in choice_strings:\n                        if match_fn(line, choice):\n                            return choice\n            logging.warn(\n                f\"Choices {choice_strings} not parsable for {eval_type}: {text}\"\n            )\n            return INVALID_STR\n\n    def run(self, data: pl.DataFrame) -&gt; TYPE_TABLE_OUTPUT:\n        prompts = []\n        for row in data.rows(named=True):\n            subs = {k: row[v] for k, v in self.context_vars.items()}\n            # fill in context variables in the prompt template\n            _prompt = self.grading_prompt_template.format(**subs)\n            # following the `evals` code to create the grading instruction\n            #  https://github.com/openai/evals/blob/main/evals/elsuite/modelgraded/classify_utils.py\n            _prompt_chat = [{\"role\": \"user\", \"content\": _prompt}]\n            # _prompt_chat = append_answer_prompt(\n            #     prompt=[{\"role\": \"user\", \"content\": _prompt}],\n            #     eval_type=self.eval_type,\n            #     choice_strings=self.choice_strings,\n            # )\n            prompts.append(_prompt_chat)\n\n        input_payloads = [\n            self._make_payload(idx, prompt_msgs)\n            for idx, prompt_msgs in enumerate(prompts)\n        ]\n        output_payloads = self._api_client.fetch_responses(input_payloads)\n\n        results = []\n        for res in output_payloads:\n            idx = res.metadata[\"index\"]\n            if res.error is not None:\n                logger.error(\n                    f\"Error when processing payload at index {idx}: {res.error}\"\n                )\n                results.append((idx, None, None))\n            else:\n                try:\n                    resp_text = res.response.choices[0].message.content\n                    choice = self.get_choice(\n                        text=resp_text,\n                        eval_type=self.eval_type,\n                        match_fn=\"extract_score\",\n                        choice_strings=self.choice_strings,\n                        choice_scores=self.choice_scores,\n                    )\n                    score = float(choice)\n                    results.append((idx, score, resp_text))\n                except Exception as e:\n                    logger.error(\n                        f\"Error when processing payload at index {idx}, not API error: {e}\"\n                    )\n                    results.append((idx, None, None))\n\n        results = sorted(results, key=lambda x: x[0])\n        if isinstance(self.col_out, list):\n            result_scores = [\n                pl.Series(\n                    [val[idx] if val is not None else None for _, val, _ in results]\n                ).alias(self.col_out[idx])\n                for idx in range(len(self.col_out))\n            ]\n            result_scores.extend(\n                [\n                    pl.Series([explanation for _, _, explanation in results]).alias(\n                        self.col_out[idx] + \"_explanation\"\n                    )\n                    for idx in range(len(self.col_out))\n                ]\n            )\n        else:\n            result_scores = [\n                pl.Series([val for _, val, _ in results]).alias(self.col_out),\n                pl.Series([explanation for _, _, explanation in results]).alias(\n                    self.col_out + \"_explanation\"\n                ),\n            ]\n        return {\"output\": data.with_columns(result_scores)}\n</code></pre>"},{"location":"operators/language/ModelGradeScore/#uptrain.operators.ModelGradeScore.get_choice","title":"<code>get_choice(text, eval_type, match_fn, choice_strings, choice_scores={})</code>","text":"<p>Clean the answer string to a choice string to one of choice_strings. Return 'invalid.' if no match.</p> Source code in <code>uptrain/operators/language/model_grade.py</code> <pre><code>def get_choice(\n    self,\n    text: str,\n    eval_type: str,\n    match_fn: Union[str, Callable],\n    choice_strings: Iterable[str],\n    choice_scores: dict = {},\n) -&gt; str:\n    \"\"\"Clean the answer string to a choice string to one of choice_strings. Return '__invalid__.' if no match.\"\"\"\n    if eval_type == \"tot_score\":\n        score = \"\"\n        if len(score) == 0:\n            scores_matches = re.findall(\n                r\"(\\[Score\\]\\: [1-5]|Score: [1-5]|Score is [1-5])\", text\n            )\n            if len(scores_matches) != 0:\n                score = str(scores_matches[0].split(\" \")[-1])\n\n        if len(score) == 0:\n            scores_matches = re.findall(\n                r\"(\\[score\\]\\: [1-5]|score: [1-5]|score is [1-5])\", text\n            )\n            if len(scores_matches) != 0:\n                score = str(scores_matches[0].split(\" (\")[-1])\n\n        if len(score) == 1:\n            return float(score)\n\n        logging.warn(\n            f\"Choices {choice_strings} not parsable for {eval_type}: {text}\"\n        )\n        return -5\n    elif eval_type == \"tot_classify\":\n        score = \"\"\n        if len(score) == 0:\n            scores_matches = re.findall(\n                r\"(\\[Choice\\]\\: [a-zA-Z]|Choice: [a-zA-Z]|choice is [a-zA-Z])\", text\n            )\n            if len(scores_matches) != 0:\n                score = str(scores_matches[0].split(\" \")[-1])\n\n        if len(score) == 0:\n            scores_matches = re.findall(\n                r\"(\\[Choice\\]\\: \\([a-zA-Z]|Choice: \\([a-zA-Z]|choice is \\([a-zA-Z])\",\n                text,\n            )\n            if len(scores_matches) != 0:\n                score = str(scores_matches[0].split(\" (\")[-1])\n\n        if len(score) == 0:\n            scores_matches = re.findall(\n                r\"(\\[Argument\\]\\: \\([a-zA-Z]|Argument: \\([a-zA-Z]|argument is \\([a-zA-Z])\",\n                text,\n            )\n            if len(scores_matches) != 0:\n                score = str(scores_matches[0].split(\" (\")[-1])\n\n        if len(score) == 1:\n            if score.upper() in choice_scores:\n                return choice_scores[score.upper()]\n            elif score.lower() in choice_scores:\n                return choice_scores[score.lower()]\n\n        logging.warn(\n            f\"Choices {choice_strings} not parsable for {eval_type}: {text}\"\n        )\n        return -1\n    else:\n        is_fn_extract_score = False\n        if match_fn == \"extract_score\":\n            is_fn_extract_score = True\n        else:\n            if isinstance(match_fn, str):\n                match_fn = MATCH_FNS[match_fn]\n        lines = text.strip().split(\"\\n\")\n        if eval_type.startswith(\"cot_classify\"):\n            lines = lines[::-1]  # reverse lines\n        for line in lines:\n            line = line.strip()\n            if not line:\n                continue\n            if is_fn_extract_score:\n                if \".\" in line:\n                    part_before_decimal = line.split(\".\")[0][::-1]\n                    prev_char = \"\"\n                    for char in part_before_decimal:\n                        new_char = char + prev_char\n                        try:\n                            float(new_char)\n                        except:\n                            break\n                        prev_char = new_char\n                    part_before_decimal = prev_char\n\n                    part_after_decimal = line.split(\".\")[1]\n                    prev_char = \"\"\n                    for char in part_after_decimal:\n                        new_char = prev_char + char\n                        try:\n                            float(new_char)\n                        except:\n                            break\n                        prev_char = new_char\n                    part_after_decimal = prev_char\n                    choice = part_before_decimal + \".\" + part_after_decimal\n                    try:\n                        float(choice)\n                        if float(choice) &gt; 1.0 or float(choice) &lt; 0.0:\n                            return self.get_choice_via_llm(\n                                text, self.grading_prompt_template\n                            )\n                        return str(choice)\n                    except:\n                        return self.get_choice_via_llm(\n                            text, self.grading_prompt_template\n                        )\n                else:\n                    return self.get_choice_via_llm(\n                        text, self.grading_prompt_template\n                    )\n            else:\n                line = \"\".join(c for c in line if c not in string.punctuation)\n                if not line:\n                    continue\n                for choice in choice_strings:\n                    if match_fn(line, choice):\n                        return choice\n        logging.warn(\n            f\"Choices {choice_strings} not parsable for {eval_type}: {text}\"\n        )\n        return INVALID_STR\n</code></pre>"},{"location":"operators/language/ModelGradeScore/#uptrain.operators.ModelGradeScore.get_choice_via_llm","title":"<code>get_choice_via_llm(text, grading_prompt_template)</code>","text":"<p>Queries LLM to get score from the text</p> Source code in <code>uptrain/operators/language/model_grade.py</code> <pre><code>def get_choice_via_llm(self, text: str, grading_prompt_template: str) -&gt; str:\n    \"\"\"Queries LLM to get score from the text\"\"\"\n\n    prompt = f\"\"\"\n    Extract the score from the given text. The available choices and associated scores is present in the context.\n\n    Context: {grading_prompt_template}\n    Text: {text}\n\n    Score:\n    \"\"\"\n\n    payload = self._make_payload(0, [{\"role\": \"user\", \"content\": prompt}])\n    output_payload = self._api_client.fetch_responses([payload])[0]\n\n    try:\n        score = output_payload.response.choices[0].message.content\n        float(score)\n        return score\n    except:\n        return str(0.0)\n</code></pre>"},{"location":"operators/language/OpenAIGradeScore/","title":"OpenAIGradeScore","text":"<p>             Bases: <code>ColumnOp</code></p> <p>Operator to calculate the grade score of text completions using OpenAI models.</p> <p>Attributes:</p> Name Type Description <code>col_in_input</code> <code>str</code> <p>The name of the input column containing the prompts.</p> <code>col_in_completion</code> <code>str</code> <p>The name of the input column containing the completions.</p> <code>eval_name</code> <code>str</code> <p>The name of the OpenAI evaluation to use.</p> <code>col_out</code> <code>str</code> <p>The name of the output column containing the grade scores.</p> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary containing the calculated grade scores.</p> Source code in <code>uptrain/operators/language/model_grade.py</code> <pre><code>@register_op\nclass OpenAIGradeScore(ColumnOp):\n    \"\"\"\n    Operator to calculate the grade score of text completions using OpenAI models.\n\n    Attributes:\n        col_in_input (str): The name of the input column containing the prompts.\n        col_in_completion (str): The name of the input column containing the completions.\n        eval_name (str): The name of the OpenAI evaluation to use.\n        col_out (str): The name of the output column containing the grade scores.\n\n    Returns:\n        dict: A dictionary containing the calculated grade scores.\n\n    \"\"\"\n\n    col_in_input: str = \"prompt\"\n    col_in_completion: str = \"response\"\n    col_out: str = \"openai_grade_score\"\n    eval_name: str\n\n    def setup(self, settings: Settings):\n        self._settings = settings\n        return self\n\n    def run(self, data: pl.DataFrame) -&gt; TYPE_TABLE_OUTPUT:\n        samples = data.select(\n            [\n                pl.col(self.col_in_input).alias(\"input\"),\n                pl.col(self.col_in_completion).alias(\"completion\"),\n            ]\n        )\n        from uptrain.operators.language.openai_evals import OpenaiEval\n\n        grading_op = OpenaiEval(\n            bundle_path=\"\",\n            completion_name=\"gpt-3.5-turbo\",\n            eval_name=self.eval_name,\n        )\n\n        grading_op.setup(settings=self._settings)\n        oaieval_res = grading_op.run(samples)\n        assert (\n            \"extra\" in oaieval_res\n            and \"metrics\" in oaieval_res[\"extra\"]\n            and \"score\" in oaieval_res[\"extra\"][\"metrics\"]\n        )\n\n        results = pl.Series(oaieval_res[\"extra\"][\"metrics\"][\"score\"])\n        return {\"output\": data.with_columns([results.alias(self.col_out)])}\n</code></pre>"},{"location":"operators/language/OpenaiEval/","title":"OpenaiEval","text":"<p>             Bases: <code>ColumnOp</code></p> <p>Operator for running OpenAI evals.</p> <p>Attributes:</p> Name Type Description <code>bundle_path</code> <code>str</code> <p>Path to the bundle containing eval resources.</p> <code>completion_name</code> <code>str</code> <p>Name of the completion function to use.</p> <code>eval_name</code> <code>str</code> <p>Name of the eval to run.</p> Source code in <code>uptrain/operators/language/openai_evals.py</code> <pre><code>@register_op\nclass OpenaiEval(ColumnOp):\n    \"\"\"\n    Operator for running OpenAI evals.\n\n    Attributes:\n        bundle_path (str): Path to the bundle containing eval resources.\n        completion_name (str): Name of the completion function to use.\n        eval_name (str): Name of the eval to run.\n\n    \"\"\"\n\n    bundle_path: str\n    completion_name: str\n    eval_name: str\n\n    def setup(self, settings: Settings):\n        import openai\n\n        openai.api_key = settings.check_and_get(\"openai_api_key\")\n        return self\n\n    def run(self, data: pl.DataFrame) -&gt; TYPE_TABLE_OUTPUT:\n        registry = evals.registry.Registry()\n        registry_path = os.path.join(self.bundle_path, \"custom_registry\")\n        registry.add_registry_paths([registry_path])\n\n        eval_name = self.eval_name\n        eval_spec = registry.get_eval(eval_name)\n        assert (\n            eval_spec is not None\n        ), f\"Eval {eval_name} not found. Available: {list(sorted(registry._evals.keys()))}\"\n\n        eval_name = eval_spec.key\n        assert eval_name is not None\n\n        # NOTE: create a temporary file with the samples if we are overriding the dataset\n        path_samples_file = f\"/tmp/{uuid.uuid4()}.jsonl\"\n        data.write_ndjson(path_samples_file)\n        eval_spec.args[\"samples_jsonl\"] = path_samples_file  # type: ignore\n\n        # NOTE: add `custom_fns` to the python path\"\n        if self.bundle_path not in sys.path:\n            sys.path.append(self.bundle_path)\n        completion_fns = [self.completion_name]\n        completion_fn_instances = [\n            registry.make_completion_fn(url) for url in completion_fns\n        ]\n\n        run_config = {\n            \"completion_fns\": completion_fns,\n            \"eval_spec\": eval_spec,\n            \"seed\": 42,\n            \"max_samples\": None,\n            \"command\": \"\",\n            \"initial_settings\": {\"visible\": True},\n        }\n        run_spec = evals.base.RunSpec(\n            completion_fns=completion_fns,\n            eval_name=eval_name,\n            base_eval=eval_name.split(\".\")[0],\n            split=eval_name.split(\".\")[1],\n            run_config=run_config,\n            created_by=\"uptrain\",\n        )\n        recorder = UptrainEvalRecorder(run_spec=run_spec)\n\n        eval_class = registry.get_class(eval_spec)\n        extra_eval_params = {}\n        # eval_class for classify is ModelBasedClassify\n        eval = eval_class(\n            completion_fns=completion_fn_instances,\n            eval_registry_path=registry_path,\n            seed=42,\n            name=eval_name,\n            registry=registry,\n            **extra_eval_params,\n        )\n        final_report = eval.run(recorder)\n\n        if path_samples_file is not None:\n            os.remove(path_samples_file)\n\n        extra = {\n            \"all_events\": recorder.get_list_events(),\n            \"run_data\": recorder.get_run_data(),\n            \"final_report\": to_py_types(final_report),\n        }\n        unique_types = set(x[\"type\"] for x in recorder.get_list_events())\n        for typ in unique_types:\n            extra[typ] = pl.from_dicts(\n                [\n                    x[\"data\"]\n                    for x in sorted(\n                        recorder.get_list_events(typ),\n                        key=lambda x: int(x[\"sample_id\"].split(\".\")[-1]),\n                    )\n                ]\n            )\n\n        return {\"output\": None, \"extra\": extra}\n</code></pre>"},{"location":"operators/language/OutputParser/","title":"OutputParser","text":"<p>             Bases: <code>ColumnOp</code></p> <p>Takes a table of LLM respones and parses them into individual columns</p> <p>Attributes:</p> Name Type Description <code>col_in_response</code> <code>str</code> <p>The name of the column containing the raw model response.</p> <code>col_out_mapping</code> <code>str</code> <p>A dictionary containing the mapping of keys in response to their output column names.</p> <p>Returns:</p> Name Type Description <code>TYPE_TABLE_OUTPUT</code> <p>A dictionary containing the dataset with the output text.</p> Source code in <code>uptrain/operators/language/generation.py</code> <pre><code>@register_op\nclass OutputParser(ColumnOp):\n    \"\"\"\n    Takes a table of LLM respones and parses them into individual columns\n\n    Attributes:\n        col_in_response (str): The name of the column containing the raw model response.\n        col_out_mapping (str): A dictionary containing the mapping of keys in response to their output column names.\n\n    Returns:\n        TYPE_TABLE_OUTPUT: A dictionary containing the dataset with the output text.\n    \"\"\"\n\n    col_in_response: str\n    col_out_mapping: dict\n\n    def setup(self, settings: \"Settings\"):\n        return self\n\n    \"\"\"Parse the LLM responses\"\"\"\n\n    def run(self, data: pl.DataFrame) -&gt; TYPE_TABLE_OUTPUT:\n        responses = data[self.col_in_response]\n        parsed_responses = pl.DataFrame([json.loads(x) for x in responses])\n        return {\n            \"output\": data.with_columns(\n                [parsed_responses[k].alias(v) for k, v in self.col_out_mapping.items()]\n            )\n        }\n</code></pre>"},{"location":"operators/language/PromptEval/","title":"PromptEval","text":"<p>             Bases: <code>TransformOp</code></p> <p>Operator for running prompt-based evaluations.</p> <p>Attributes:</p> Name Type Description <code>prompt_template</code> <code>str</code> <p>Template for the prompt string.</p> <code>prompt_variables</code> <code>list[str]</code> <p>List of variables to substitute in the prompt template.</p> <code>gt_variables</code> <code>list[str]</code> <p>List of ground truth variables.</p> <code>model_name</code> <code>str</code> <p>Name of the model to use for evaluation.</p> <code>col_out_prompt</code> <code>str</code> <p>Output column name for prompts. Defaults to \"prompt\".</p> <code>col_out_response</code> <code>str</code> <p>Output column name for responses. Defaults to \"response\".</p> <code>_settings</code> <code>Settings</code> <p>The framework settings.</p> Source code in <code>uptrain/operators/language/openai_evals.py</code> <pre><code>class PromptEval(TransformOp):\n    \"\"\"\n    Operator for running prompt-based evaluations.\n\n    Attributes:\n        prompt_template (str): Template for the prompt string.\n        prompt_variables (list[str]): List of variables to substitute in the prompt template.\n        gt_variables (list[str]): List of ground truth variables.\n        model_name (str): Name of the model to use for evaluation.\n        col_out_prompt (str, optional): Output column name for prompts. Defaults to \"prompt\".\n        col_out_response (str, optional): Output column name for responses. Defaults to \"response\".\n        _settings (Settings): The framework settings.\n\n    \"\"\"\n\n    prompt_template: str\n    prompt_variables: list[str]\n    gt_variables: list[str]\n    model_name: str\n    col_out_prompt: str = \"prompt\"\n    col_out_response: str = \"response\"\n    _settings: Settings\n\n    def setup(self, settings: Settings):\n        self._settings = settings\n        return self\n\n    def _validate_data(self, data: pl.DataFrame) -&gt; None:\n        for col in self.prompt_variables:\n            assert (\n                col in data.columns\n            ), f\"Column for the prompt variable: {col} not found in input data.\"\n        for col in self.gt_variables:\n            assert (\n                col in data.columns\n            ), f\"Column for the ground truth variable: {col} not found in input data.\"\n\n    def _construct_prompts(self, data: pl.DataFrame) -&gt; pl.DataFrame:\n        prompts = [\n            self.prompt_template.format(**{k: row[k] for k in self.prompt_variables})\n            for row in data.rows(named=True)\n        ]\n        return pl.from_dict({\"input\": prompts})\n\n    def run(self, data: pl.DataFrame) -&gt; TYPE_TABLE_OUTPUT:\n        self._validate_data(data)\n        prompts = self._construct_prompts(data)\n\n        eval_op = OpenaiEval(\n            bundle_path=os.path.join(UPTRAIN_BASE_DIR, \"openai_eval_custom\"),\n            completion_name=self.model_name,\n            eval_name=\"model_run_all\",\n        )\n\n        eval_op.setup(self._settings)\n        oaieval_res = eval_op.run(prompts)\n        assert \"extra\" in oaieval_res and \"sampling\" in oaieval_res[\"extra\"]\n        results = oaieval_res[\"extra\"][\"sampling\"]\n\n        return {\n            \"output\": data.with_columns(\n                [\n                    pl.Series(results[\"prompt\"]).alias(self.col_out_prompt),\n                    pl.Series(\n                        list(itertools.chain(*results[\"sampled\"])),\n                    ).alias(self.col_out_response),\n                ]\n            )\n        }\n</code></pre>"},{"location":"operators/language/PromptGenerator/","title":"PromptGenerator","text":"<p>             Bases: <code>TransformOp</code></p> <p>Operator to generate text given different prompts/LLM-input-parameters.</p> <p>Attributes:</p> Name Type Description <code>prompt_template</code> <code>str) </code> <p>A string template for the prompt.</p> <code>prompt_params</code> <code>dict[str, list[str]]</code> <p>A dictionary mapping parameter names to lists of values. The cartesian product of all the parameter values will be used to construct the prompts.</p> <code>models</code> <code>list[str]</code> <p>A list of models to run the experiment on.</p> <code>context_vars</code> <code>Union[list[str], dict[str, str]]</code> <p>A dictionary mapping context variable names to corresponding columns in the input dataset.</p> <code>col_out_prefix</code> <code>str</code> <p>Prefix for the output columns.</p> Source code in <code>uptrain/operators/language/generation.py</code> <pre><code>@register_op\nclass PromptGenerator(TransformOp):\n    \"\"\"Operator to generate text given different prompts/LLM-input-parameters.\n\n    Attributes:\n        prompt_template (str) : A string template for the prompt.\n        prompt_params (dict[str, list[str]]): A dictionary mapping parameter names to lists of values.\n            The cartesian product of all the parameter values will be used to\n            construct the prompts.\n        models (list[str]): A list of models to run the experiment on.\n        context_vars (Union[list[str], dict[str, str]]): A dictionary mapping context variable names to corresponding\n            columns in the input dataset.\n        col_out_prefix (str): Prefix for the output columns.\n\n    \"\"\"\n\n    prompt_template: str\n    prompt_params: dict[str, list[str]]\n    models: list[str]\n    context_vars: t.Union[list[str], dict[str, str]]\n    col_out_prefix: str = \"exp_\"\n\n    def setup(self, settings: \"Settings\"):\n        self._settings = settings\n        if isinstance(self.context_vars, list):\n            self.context_vars = dict(zip(self.context_vars, self.context_vars))\n        return self\n\n    \"\"\"Construct all the prompt variations and generate completions for each.\"\"\"\n\n    def run(self, data: pl.DataFrame) -&gt; TYPE_TABLE_OUTPUT:\n        list_params = []\n        for experiment_id, combo in enumerate(\n            itertools.product(*self.prompt_params.values(), self.models)\n        ):\n            prompt_params, model = combo[:-1], combo[-1]\n            variables = dict(zip(self.prompt_params.keys(), prompt_params))\n            list_params.append(\n                {\n                    \"template\": self.prompt_template,\n                    self.col_out_prefix + \"model\": model,\n                    **{self.col_out_prefix + k: v for k, v in variables.items()},\n                    self.col_out_prefix + \"experiment_id\": experiment_id,\n                }\n            )\n        params_dataset = pl.DataFrame(list_params)\n\n        # Do a cross join of the input with the params dataset to get all the\n        # inputs for the completion step\n        input_dataset = data.join(params_dataset, on=None, how=\"cross\")\n\n        # construct the prompts by iterating over the rows of the input dataset and\n        # formatting the prompt template with the row values\n        prompts = []\n        for row in input_dataset.iter_rows(named=True):\n            fill = {k: row[v] for k, v in self.context_vars.items()}\n            fill.update(\n                {k: row[self.col_out_prefix + k] for k in self.prompt_params.keys()}\n            )\n\n            # TODO: Temp Fix for handling json in prompts. Permanent fix is to integrate langchain?\n            try:\n                prompt = row[\"template\"].format(**fill)\n            except:\n                prompt = row[\"template\"]\n                for k, v in fill.items():\n                    prompt = prompt.replace(\"{{\" + k + \"}}\", v)\n            prompts.append(prompt)\n\n        input_w_prompts = input_dataset.with_columns(\n            [pl.Series(self.col_out_prefix + \"prompt\", prompts)]\n        )\n        return {\"output\": input_w_prompts}\n</code></pre>"},{"location":"operators/language/ResponseCompleteness/","title":"ResponseCompleteness","text":"<p>             Bases: <code>ColumnOp</code></p> <p>Grades if the response is able to answer the question asked completely or not.</p> <p>Attributes:</p> Name Type Description <code>col_question</code> <code>str</code> <p>(str) Column Name for the stored questions</p> <code>col_response</code> <code>str</code> <p>(str) Coloumn name for stored response</p> <code>col_out</code> <code>str</code> <p>(str) Column name for the output score</p> <code>scenario_description</code> <code>str</code> <p>Optional scenario description to incorporate in the evaluation prompt</p> <code>score_mapping</code> <code>dict</code> <p>Mapping of different grades to float scores</p> <p>Raises:</p> Type Description <code>Exception</code> <p>Raises exception for any failed evaluation attempts</p> Source code in <code>uptrain/operators/language/response_quality.py</code> <pre><code>@register_op\nclass ResponseCompleteness(ColumnOp):\n    \"\"\"\n    Grades if the response is able to answer the question asked completely or not.\n\n    Attributes:\n        col_question: (str) Column Name for the stored questions\n        col_response: (str) Coloumn name for stored response\n        col_out: (str) Column name for the output score\n        scenario_description (str): Optional scenario description to incorporate in the evaluation prompt\n        score_mapping (dict): Mapping of different grades to float scores\n\n    Raises:\n        Exception: Raises exception for any failed evaluation attempts\n\n    \"\"\"\n\n    col_question: str = \"question\"\n    col_response: str = \"response\"\n    col_out: str = \"score_response_completeness\"\n    scenario_description: t.Optional[str] = None\n    score_mapping: dict = {\"A\": 1.0, \"B\": 0.5, \"C\": 0.0}\n\n    def setup(self, settings: t.Optional[Settings] = None):\n        from uptrain.framework.remote import APIClient\n\n        assert settings is not None\n        self.settings = settings\n        if self.settings.evaluate_locally:\n            self._api_client = LLMMulticlient(settings)\n        else:\n            self._api_client = APIClient(settings)\n        return self\n\n    def run(self, data: pl.DataFrame) -&gt; TYPE_TABLE_OUTPUT:\n        data_send = polars_to_json_serializable_dict(data)\n        for row in data_send:\n            row[\"question\"] = row.pop(self.col_question)\n            row[\"response\"] = row.pop(self.col_response)\n\n        try:\n            if self.settings.evaluate_locally:\n                results = self.evaluate_local(data_send)\n            else:\n                results = self._api_client.evaluate(\n                    \"response_completeness\",\n                    data_send,\n                    {\"scenario_description\": self.scenario_description},\n                )\n        except Exception as e:\n            logger.error(f\"Failed to run evaluation for `ResponseCompleteness`: {e}\")\n            raise e\n\n        assert results is not None\n        return {\n            \"output\": data.with_columns(\n                pl.from_dicts(results).rename(\n                    {\"score_response_completeness\": self.col_out}\n                )\n            )\n        }\n\n    def response_completeness_classify_validate_func(self, llm_output):\n        is_correct = True\n        is_correct = is_correct and (\"Choice\" in json.loads(llm_output))\n        is_correct = is_correct and json.loads(llm_output)[\"Choice\"] in [\"A\", \"B\", \"C\"]\n        return is_correct\n\n    def response_completeness_cot_validate_func(self, llm_output):\n        is_correct = self.response_completeness_classify_validate_func(llm_output)\n        is_correct = is_correct and (\"Reasoning\" in json.loads(llm_output))\n        return is_correct\n\n    def evaluate_local(self, data):\n        \"\"\"\n        Our methodology is based on the model grade evaluation introduced by openai evals.\n        \"\"\"\n\n        self.scenario_description, scenario_vars = parse_scenario_description(\n            self.scenario_description\n        )\n        input_payloads = []\n        if self.settings.eval_type == \"basic\":\n            few_shot_examples = RESPONSE_COMPLETENESS_FEW_SHOT__CLASSIFY\n            output_format = RESPONSE_COMPLETENESS_OUTPUT_FORMAT__CLASSIFY\n            validation_func = self.response_completeness_classify_validate_func\n            prompting_instructions = CLASSIFY\n        elif self.settings.eval_type == \"cot\":\n            few_shot_examples = RESPONSE_COMPLETENESS_FEW_SHOT__COT\n            output_format = RESPONSE_COMPLETENESS_OUTPUT_FORMAT__COT\n            validation_func = self.response_completeness_cot_validate_func\n            prompting_instructions = CHAIN_OF_THOUGHT\n        else:\n            raise ValueError(\n                f\"Invalid eval_type: {self.settings.eval_type}. Must be either 'basic' or 'cot'\"\n            )\n\n        for idx, row in enumerate(data):\n            kwargs = row\n            kwargs.update(\n                {\n                    \"output_format\": output_format,\n                    \"prompting_instructions\": prompting_instructions,\n                    \"few_shot_examples\": few_shot_examples,\n                }\n            )\n            try:\n                grading_prompt_template = RESPONSE_COMPLETENESS_PROMPT_TEMPLATE.replace(\n                    \"{scenario_description}\", self.scenario_description\n                ).format(**kwargs)\n            except KeyError as e:\n                raise KeyError(\n                    f\"Missing required attribute(s) for scenario description: {e}\"\n                )\n            input_payloads.append(\n                self._api_client.make_payload(idx, grading_prompt_template)\n            )\n        output_payloads = self._api_client.fetch_responses(\n            input_payloads, validation_func\n        )\n\n        results = []\n        for res in output_payloads:\n            idx = res.metadata[\"index\"]\n            output = {\n                \"score_response_completeness\": None,\n                \"explanation_response_completeness\": None,\n            }\n            try:\n                score = self.score_mapping[\n                    json.loads(res.response.choices[0].message.content)[\"Choice\"]\n                ]\n                output[\"score_response_completeness\"] = float(score)\n                output[\"explanation_response_completeness\"] = res.response.choices[\n                    0\n                ].message.content\n            except Exception:\n                logger.error(\n                    f\"Error when processing payload at index {idx}: {res.error}\"\n                )\n            results.append((idx, output))\n        results = [val for _, val in sorted(results, key=lambda x: x[0])]\n\n        return results\n</code></pre>"},{"location":"operators/language/ResponseCompleteness/#uptrain.operators.ResponseCompleteness.evaluate_local","title":"<code>evaluate_local(data)</code>","text":"<p>Our methodology is based on the model grade evaluation introduced by openai evals.</p> Source code in <code>uptrain/operators/language/response_quality.py</code> <pre><code>def evaluate_local(self, data):\n    \"\"\"\n    Our methodology is based on the model grade evaluation introduced by openai evals.\n    \"\"\"\n\n    self.scenario_description, scenario_vars = parse_scenario_description(\n        self.scenario_description\n    )\n    input_payloads = []\n    if self.settings.eval_type == \"basic\":\n        few_shot_examples = RESPONSE_COMPLETENESS_FEW_SHOT__CLASSIFY\n        output_format = RESPONSE_COMPLETENESS_OUTPUT_FORMAT__CLASSIFY\n        validation_func = self.response_completeness_classify_validate_func\n        prompting_instructions = CLASSIFY\n    elif self.settings.eval_type == \"cot\":\n        few_shot_examples = RESPONSE_COMPLETENESS_FEW_SHOT__COT\n        output_format = RESPONSE_COMPLETENESS_OUTPUT_FORMAT__COT\n        validation_func = self.response_completeness_cot_validate_func\n        prompting_instructions = CHAIN_OF_THOUGHT\n    else:\n        raise ValueError(\n            f\"Invalid eval_type: {self.settings.eval_type}. Must be either 'basic' or 'cot'\"\n        )\n\n    for idx, row in enumerate(data):\n        kwargs = row\n        kwargs.update(\n            {\n                \"output_format\": output_format,\n                \"prompting_instructions\": prompting_instructions,\n                \"few_shot_examples\": few_shot_examples,\n            }\n        )\n        try:\n            grading_prompt_template = RESPONSE_COMPLETENESS_PROMPT_TEMPLATE.replace(\n                \"{scenario_description}\", self.scenario_description\n            ).format(**kwargs)\n        except KeyError as e:\n            raise KeyError(\n                f\"Missing required attribute(s) for scenario description: {e}\"\n            )\n        input_payloads.append(\n            self._api_client.make_payload(idx, grading_prompt_template)\n        )\n    output_payloads = self._api_client.fetch_responses(\n        input_payloads, validation_func\n    )\n\n    results = []\n    for res in output_payloads:\n        idx = res.metadata[\"index\"]\n        output = {\n            \"score_response_completeness\": None,\n            \"explanation_response_completeness\": None,\n        }\n        try:\n            score = self.score_mapping[\n                json.loads(res.response.choices[0].message.content)[\"Choice\"]\n            ]\n            output[\"score_response_completeness\"] = float(score)\n            output[\"explanation_response_completeness\"] = res.response.choices[\n                0\n            ].message.content\n        except Exception:\n            logger.error(\n                f\"Error when processing payload at index {idx}: {res.error}\"\n            )\n        results.append((idx, output))\n    results = [val for _, val in sorted(results, key=lambda x: x[0])]\n\n    return results\n</code></pre>"},{"location":"operators/language/ResponseCompletenessWrtContext/","title":"ResponseCompletenessWrtContext","text":"<p>             Bases: <code>ColumnOp</code></p> <p>Grades if the response is able to answer the question asked completely with respect to the context or not.</p> <p>Attributes:</p> Name Type Description <code>col_question</code> <code>str</code> <p>(str) Column Name for the stored questions</p> <code>col_response</code> <code>str</code> <p>(str) Coloumn name for stored response</p> <code>col_context</code> <code>str</code> <p>(str) Column name for stored context</p> <code>col_out</code> <code>str</code> <p>(str) Column name for the output score</p> <code>scenario_description</code> <code>str</code> <p>Optional scenario description to incorporate in the evaluation prompt</p> <code>score_mapping</code> <code>dict</code> <p>Mapping of different grades to float scores</p> <p>Raises:</p> Type Description <code>Exception</code> <p>Raises exception for any failed evaluation attempts</p> Source code in <code>uptrain/operators/language/context_quality.py</code> <pre><code>@register_op\nclass ResponseCompletenessWrtContext(ColumnOp):\n    \"\"\"\n    Grades if the response is able to answer the question asked completely with respect to the context or not.\n\n    Attributes:\n        col_question: (str) Column Name for the stored questions\n        col_response: (str) Coloumn name for stored response\n        col_context: (str) Column name for stored context\n        col_out: (str) Column name for the output score\n        scenario_description (str): Optional scenario description to incorporate in the evaluation prompt\n        score_mapping (dict): Mapping of different grades to float scores\n\n    Raises:\n        Exception: Raises exception for any failed evaluation attempts\n\n    \"\"\"\n\n    col_question: str = \"question\"\n    col_response: str = \"response\"\n    col_context: str = \"context\"\n    col_out: str = \"score_response_completeness_wrt_context\"\n    scenario_description: t.Optional[str] = None\n    score_mapping: dict = {\"A\": 1.0, \"B\": 0.5, \"C\": 0.0}\n\n    def setup(self, settings: t.Optional[Settings] = None):\n        from uptrain.framework.remote import APIClient\n\n        assert settings is not None\n        self.settings = settings\n        if self.settings.evaluate_locally:\n            self._api_client = LLMMulticlient(settings)\n        else:\n            self._api_client = APIClient(settings)\n        return self\n\n    def run(self, data: pl.DataFrame) -&gt; TYPE_TABLE_OUTPUT:\n        data_send = polars_to_json_serializable_dict(data)\n        for row in data_send:\n            row[\"question\"] = row.pop(self.col_question)\n            row[\"response\"] = row.pop(self.col_response)\n            row[\"context\"] = row.pop(self.col_context)\n\n        try:\n            if self.settings.evaluate_locally:\n                results = self.evaluate_local(data_send)\n            else:\n                results = self._api_client.evaluate(\n                    \"response_completeness_wrt_context\",\n                    data_send,\n                    {\"scenario_description\": self.scenario_description},\n                )\n        except Exception as e:\n            logger.error(\n                f\"Failed to run evaluation for `ResponseCompletenessWrtContext`: {e}\"\n            )\n            raise e\n\n        assert results is not None\n        return {\n            \"output\": data.with_columns(\n                pl.from_dicts(results).rename(\n                    {\"score_response_completeness_wrt_context\": self.col_out}\n                )\n            )\n        }\n\n    def response_completeness_wrt_context_classify_validate_func(self, llm_output):\n        is_correct = True\n        is_correct = is_correct and (\"Choice\" in json.loads(llm_output))\n        is_correct = is_correct and json.loads(llm_output)[\"Choice\"] in [\"A\", \"B\", \"C\"]\n        return is_correct\n\n    def response_completeness_wrt_context_cot_validate_func(self, llm_output):\n        is_correct = self.response_completeness_wrt_context_classify_validate_func(\n            llm_output\n        )\n        is_correct = is_correct and (\"Reasoning\" in json.loads(llm_output))\n        return is_correct\n\n    def evaluate_local(self, data):\n        \"\"\"\n        Our methodology is based on the model grade evaluation introduced by openai evals.\n        \"\"\"\n\n        self.scenario_description, scenario_vars = parse_scenario_description(\n            self.scenario_description\n        )\n        input_payloads = []\n        if self.settings.eval_type == \"basic\":\n            few_shot_examples = RESPONSE_COMPLETENESS_WRT_CONTEXT_FEW_SHOT__CLASSIFY\n            output_format = RESPONSE_COMPLETENESS_WRT_CONTEXT_OUTPUT_FORMAT__CLASSIFY\n            validation_func = (\n                self.response_completeness_wrt_context_classify_validate_func\n            )\n            prompting_instructions = CLASSIFY\n        elif self.settings.eval_type == \"cot\":\n            few_shot_examples = RESPONSE_COMPLETENESS_WRT_CONTEXT_FEW_SHOT__COT\n            output_format = RESPONSE_COMPLETENESS_WRT_CONTEXT_OUTPUT_FORMAT__COT\n            validation_func = self.response_completeness_wrt_context_cot_validate_func\n            prompting_instructions = CHAIN_OF_THOUGHT\n        else:\n            raise ValueError(\n                f\"Invalid eval_type: {self.settings.eval_type}. Must be either 'basic' or 'cot'\"\n            )\n\n        for idx, row in enumerate(data):\n            kwargs = row\n            kwargs.update(\n                {\n                    \"output_format\": output_format,\n                    \"prompting_instructions\": prompting_instructions,\n                    \"few_shot_examples\": few_shot_examples,\n                }\n            )\n            try:\n                grading_prompt_template = (\n                    RESPONSE_COMPLETENESS_WRT_CONTEXT_PROMPT_TEMPLATE.replace(\n                        \"{scenario_description}\", self.scenario_description\n                    ).format(**kwargs)\n                )\n            except KeyError as e:\n                raise KeyError(\n                    f\"Missing required attribute(s) for scenario description: {e}\"\n                )\n            input_payloads.append(\n                self._api_client.make_payload(idx, grading_prompt_template)\n            )\n        output_payloads = self._api_client.fetch_responses(\n            input_payloads, validation_func\n        )\n\n        results = []\n        for res in output_payloads:\n            idx = res.metadata[\"index\"]\n            output = {\n                \"score_response_completeness_wrt_context\": None,\n                \"explanation_response_completeness_wrt_context\": None,\n            }\n            try:\n                score = self.score_mapping[\n                    json.loads(res.response.choices[0].message.content)[\"Choice\"]\n                ]\n                output[\"score_response_completeness_wrt_context\"] = float(score)\n                output[\n                    \"explanation_response_completeness_wrt_context\"\n                ] = res.response.choices[0].message.content\n            except Exception:\n                logger.error(\n                    f\"Error when processing payload at index {idx}: {res.error}\"\n                )\n            results.append((idx, output))\n\n        results = [val for _, val in sorted(results, key=lambda x: x[0])]\n\n        return results\n</code></pre>"},{"location":"operators/language/ResponseCompletenessWrtContext/#uptrain.operators.ResponseCompletenessWrtContext.evaluate_local","title":"<code>evaluate_local(data)</code>","text":"<p>Our methodology is based on the model grade evaluation introduced by openai evals.</p> Source code in <code>uptrain/operators/language/context_quality.py</code> <pre><code>def evaluate_local(self, data):\n    \"\"\"\n    Our methodology is based on the model grade evaluation introduced by openai evals.\n    \"\"\"\n\n    self.scenario_description, scenario_vars = parse_scenario_description(\n        self.scenario_description\n    )\n    input_payloads = []\n    if self.settings.eval_type == \"basic\":\n        few_shot_examples = RESPONSE_COMPLETENESS_WRT_CONTEXT_FEW_SHOT__CLASSIFY\n        output_format = RESPONSE_COMPLETENESS_WRT_CONTEXT_OUTPUT_FORMAT__CLASSIFY\n        validation_func = (\n            self.response_completeness_wrt_context_classify_validate_func\n        )\n        prompting_instructions = CLASSIFY\n    elif self.settings.eval_type == \"cot\":\n        few_shot_examples = RESPONSE_COMPLETENESS_WRT_CONTEXT_FEW_SHOT__COT\n        output_format = RESPONSE_COMPLETENESS_WRT_CONTEXT_OUTPUT_FORMAT__COT\n        validation_func = self.response_completeness_wrt_context_cot_validate_func\n        prompting_instructions = CHAIN_OF_THOUGHT\n    else:\n        raise ValueError(\n            f\"Invalid eval_type: {self.settings.eval_type}. Must be either 'basic' or 'cot'\"\n        )\n\n    for idx, row in enumerate(data):\n        kwargs = row\n        kwargs.update(\n            {\n                \"output_format\": output_format,\n                \"prompting_instructions\": prompting_instructions,\n                \"few_shot_examples\": few_shot_examples,\n            }\n        )\n        try:\n            grading_prompt_template = (\n                RESPONSE_COMPLETENESS_WRT_CONTEXT_PROMPT_TEMPLATE.replace(\n                    \"{scenario_description}\", self.scenario_description\n                ).format(**kwargs)\n            )\n        except KeyError as e:\n            raise KeyError(\n                f\"Missing required attribute(s) for scenario description: {e}\"\n            )\n        input_payloads.append(\n            self._api_client.make_payload(idx, grading_prompt_template)\n        )\n    output_payloads = self._api_client.fetch_responses(\n        input_payloads, validation_func\n    )\n\n    results = []\n    for res in output_payloads:\n        idx = res.metadata[\"index\"]\n        output = {\n            \"score_response_completeness_wrt_context\": None,\n            \"explanation_response_completeness_wrt_context\": None,\n        }\n        try:\n            score = self.score_mapping[\n                json.loads(res.response.choices[0].message.content)[\"Choice\"]\n            ]\n            output[\"score_response_completeness_wrt_context\"] = float(score)\n            output[\n                \"explanation_response_completeness_wrt_context\"\n            ] = res.response.choices[0].message.content\n        except Exception:\n            logger.error(\n                f\"Error when processing payload at index {idx}: {res.error}\"\n            )\n        results.append((idx, output))\n\n    results = [val for _, val in sorted(results, key=lambda x: x[0])]\n\n    return results\n</code></pre>"},{"location":"operators/language/ResponseFactualScore/","title":"ResponseFactualScore","text":"<p>             Bases: <code>ColumnOp</code></p> <p>Grade how factual the generated response was.</p> <p>Attributes:     col_question (str): Column name for the stored questions     col_context (str): Coloumn name for stored context     col_response (str): Coloumn name for the stored responses     scenario_description (str): Optional scenario description to incorporate in the evaluation prompt     score_mapping (dict): Mapping of different grades to float scores</p> <p>Raises:</p> Type Description <code>Exception</code> <p>Raises exception for any failed evaluation attempts</p> Source code in <code>uptrain/operators/language/factual_accuracy.py</code> <pre><code>@register_op\nclass ResponseFactualScore(ColumnOp):\n    \"\"\"\n    Grade how factual the generated response was.\n\n     Attributes:\n        col_question (str): Column name for the stored questions\n        col_context (str): Coloumn name for stored context\n        col_response (str): Coloumn name for the stored responses\n        scenario_description (str): Optional scenario description to incorporate in the evaluation prompt\n        score_mapping (dict): Mapping of different grades to float scores\n\n    Raises:\n        Exception: Raises exception for any failed evaluation attempts\n    \"\"\"\n\n    col_question: str = \"question\"\n    col_context: str = \"context\"\n    col_response: str = \"response\"\n    col_out: str = \"score_factual_accuracy\"\n    scenario_description: t.Optional[str] = None\n    score_mapping: dict = {\"yes\": 1.0, \"unclear\": 0.5, \"no\": 0.0}\n\n    def setup(self, settings: t.Optional[Settings] = None):\n        from uptrain.framework.remote import APIClient\n\n        assert settings is not None\n        self.settings = settings\n        if self.settings.evaluate_locally:\n            self._api_client = LLMMulticlient(settings)\n        else:\n            self._api_client = APIClient(settings)\n        return self\n\n    def run(self, data: pl.DataFrame) -&gt; TYPE_TABLE_OUTPUT:\n        data_send = polars_to_json_serializable_dict(data)\n        for row in data_send:\n            row[\"question\"] = row.pop(self.col_question)\n            row[\"response\"] = row.pop(self.col_response)\n            row[\"context\"] = row.pop(self.col_context)\n\n        try:\n            if self.settings.evaluate_locally:\n                results = self.evaluate_local(data_send)\n            else:\n                results = self._api_client.evaluate(\n                    \"factual_accuracy\",\n                    data_send,\n                    {\"scenario_description\": self.scenario_description},\n                )\n        except Exception as e:\n            logger.error(f\"Failed to run evaluation for `ResponseFactualScore`: {e}\")\n            raise e\n\n        assert results is not None\n        return {\n            \"output\": data.with_columns(\n                pl.from_dicts(results).rename({\"score_factual_accuracy\": self.col_out})\n            )\n        }\n\n    def fact_generate_validate_func(self, llm_output):\n        return isinstance(json.loads(llm_output), list)\n\n    def fact_eval_classify_validate_func(self, llm_output):\n        is_correct = True\n        is_correct = is_correct and isinstance(json.loads(llm_output), list)\n        for row in json.loads(llm_output):\n            is_correct = (\n                is_correct and min([x in row for x in [\"Judgement\", \"Fact\"]]) &gt; 0\n            )\n            is_correct = is_correct and row[\"Judgement\"].lower() in [\n                \"yes\",\n                \"no\",\n                \"unclear\",\n            ]\n        return is_correct\n\n    def fact_eval_cot_validate_func(self, llm_output):\n        is_correct = self.fact_eval_classify_validate_func(llm_output)\n        for row in json.loads(llm_output):\n            is_correct = is_correct and min([x in row for x in [\"Reasoning\"]]) &gt; 0\n        return is_correct\n\n    def evaluate_local(self, data):\n        \"\"\"\n        Our methodology is based on https://arxiv.org/abs/2305.14251\n        FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation\n        \"\"\"\n\n        # Step 0: Parse the scenario description\n        self.scenario_description, scenario_vars = parse_scenario_description(\n            self.scenario_description\n        )\n\n        # Step 1: Extract facts from the response\n        input_payloads = []\n        for idx, row in enumerate(data):\n            kwargs = row\n            kwargs.update(\n                {\n                    \"output_format\": FACT_GENERATE_OUTPUT_FORMAT,\n                    \"few_shot_examples\": FACT_GENERATE_FEW_SHOT,\n                }\n            )\n            try:\n                grading_prompt_template = FACT_GENERATE_PROMPT_TEMPLATE.replace(\n                    \"{scenario_description}\", self.scenario_description\n                ).format(**kwargs)\n            except KeyError as e:\n                raise KeyError(\n                    f\"Missing required attribute(s) for scenario description: {e}\"\n                )\n            input_payloads.append(\n                self._api_client.make_payload(idx, grading_prompt_template)\n            )\n        output_payloads = self._api_client.fetch_responses(\n            input_payloads, self.fact_generate_validate_func\n        )\n\n        fact_results = []\n        for res in output_payloads:\n            idx = res.metadata[\"index\"]\n            try:\n                facts = json.loads(res.response.choices[0].message.content)\n                fact_results.append((idx, facts))\n            except Exception:\n                logger.error(\n                    f\"Error when processing payload at index {idx}: {res.error}\"\n                )\n                fact_results.append((idx, []))\n        fact_results = [val for _, val in sorted(fact_results, key=lambda x: x[0])]\n\n        # Step 2: Verify the facts wrt context\n        input_payloads = []\n        if self.settings.eval_type == \"basic\":\n            few_shot_examples = FACT_EVAL_FEW_SHOT__CLASSIFY\n            output_format = FACT_EVALUATE_OUTPUT_FORMAT__CLASSIFY\n            validation_func = self.fact_eval_classify_validate_func\n            prompting_instructions = CLASSIFY\n        elif self.settings.eval_type == \"cot\":\n            few_shot_examples = FACT_EVAL_FEW_SHOT__COT\n            output_format = FACT_EVALUATE_OUTPUT_FORMAT__COT\n            validation_func = self.fact_eval_cot_validate_func\n            prompting_instructions = CHAIN_OF_THOUGHT\n        else:\n            raise Exception(\"Unknown Eval Type: Choose from 'basic' or 'cot'\")\n\n        for idx, row in enumerate(data):\n            kwargs = row\n            kwargs.update(\n                {\n                    \"facts\": fact_results[idx],\n                    \"output_format\": output_format,\n                    \"prompting_instructions\": prompting_instructions,\n                    \"few_shot_examples\": few_shot_examples,\n                }\n            )\n            try:\n                grading_prompt_template = FACT_EVAL_PROMPT_TEMPLATE.replace(\n                    \"{scenario_description}\", self.scenario_description\n                ).format(**kwargs)\n            except KeyError as e:\n                raise KeyError(\n                    f\"Missing required attribute(s) for scenario description: {e}\"\n                )\n            input_payloads.append(\n                self._api_client.make_payload(idx, grading_prompt_template)\n            )\n        output_payloads = self._api_client.fetch_responses(\n            input_payloads, validation_func\n        )\n\n        results = []\n        for res in output_payloads:\n            idx = res.metadata[\"index\"]\n            output = {\n                \"score_factual_accuracy\": None,\n                \"explanation_factual_accuracy\": None,\n            }\n            try:\n                judgements = [\n                    x[\"Judgement\"]\n                    for x in json.loads(res.response.choices[0].message.content)\n                ]\n                score = np.mean([self.score_mapping[x.lower()] for x in judgements])\n                output[\"score_factual_accuracy\"] = float(score)\n                output[\"explanation_factual_accuracy\"] = res.response.choices[\n                    0\n                ].message.content\n            except Exception:\n                logger.error(\n                    f\"Error when processing payload at index {idx}: {res.error}\"\n                )\n            results.append((idx, output))\n        results = [val for _, val in sorted(results, key=lambda x: x[0])]\n\n        return results\n</code></pre>"},{"location":"operators/language/ResponseFactualScore/#uptrain.operators.ResponseFactualScore.evaluate_local","title":"<code>evaluate_local(data)</code>","text":"<p>Our methodology is based on https://arxiv.org/abs/2305.14251 FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation</p> Source code in <code>uptrain/operators/language/factual_accuracy.py</code> <pre><code>def evaluate_local(self, data):\n    \"\"\"\n    Our methodology is based on https://arxiv.org/abs/2305.14251\n    FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation\n    \"\"\"\n\n    # Step 0: Parse the scenario description\n    self.scenario_description, scenario_vars = parse_scenario_description(\n        self.scenario_description\n    )\n\n    # Step 1: Extract facts from the response\n    input_payloads = []\n    for idx, row in enumerate(data):\n        kwargs = row\n        kwargs.update(\n            {\n                \"output_format\": FACT_GENERATE_OUTPUT_FORMAT,\n                \"few_shot_examples\": FACT_GENERATE_FEW_SHOT,\n            }\n        )\n        try:\n            grading_prompt_template = FACT_GENERATE_PROMPT_TEMPLATE.replace(\n                \"{scenario_description}\", self.scenario_description\n            ).format(**kwargs)\n        except KeyError as e:\n            raise KeyError(\n                f\"Missing required attribute(s) for scenario description: {e}\"\n            )\n        input_payloads.append(\n            self._api_client.make_payload(idx, grading_prompt_template)\n        )\n    output_payloads = self._api_client.fetch_responses(\n        input_payloads, self.fact_generate_validate_func\n    )\n\n    fact_results = []\n    for res in output_payloads:\n        idx = res.metadata[\"index\"]\n        try:\n            facts = json.loads(res.response.choices[0].message.content)\n            fact_results.append((idx, facts))\n        except Exception:\n            logger.error(\n                f\"Error when processing payload at index {idx}: {res.error}\"\n            )\n            fact_results.append((idx, []))\n    fact_results = [val for _, val in sorted(fact_results, key=lambda x: x[0])]\n\n    # Step 2: Verify the facts wrt context\n    input_payloads = []\n    if self.settings.eval_type == \"basic\":\n        few_shot_examples = FACT_EVAL_FEW_SHOT__CLASSIFY\n        output_format = FACT_EVALUATE_OUTPUT_FORMAT__CLASSIFY\n        validation_func = self.fact_eval_classify_validate_func\n        prompting_instructions = CLASSIFY\n    elif self.settings.eval_type == \"cot\":\n        few_shot_examples = FACT_EVAL_FEW_SHOT__COT\n        output_format = FACT_EVALUATE_OUTPUT_FORMAT__COT\n        validation_func = self.fact_eval_cot_validate_func\n        prompting_instructions = CHAIN_OF_THOUGHT\n    else:\n        raise Exception(\"Unknown Eval Type: Choose from 'basic' or 'cot'\")\n\n    for idx, row in enumerate(data):\n        kwargs = row\n        kwargs.update(\n            {\n                \"facts\": fact_results[idx],\n                \"output_format\": output_format,\n                \"prompting_instructions\": prompting_instructions,\n                \"few_shot_examples\": few_shot_examples,\n            }\n        )\n        try:\n            grading_prompt_template = FACT_EVAL_PROMPT_TEMPLATE.replace(\n                \"{scenario_description}\", self.scenario_description\n            ).format(**kwargs)\n        except KeyError as e:\n            raise KeyError(\n                f\"Missing required attribute(s) for scenario description: {e}\"\n            )\n        input_payloads.append(\n            self._api_client.make_payload(idx, grading_prompt_template)\n        )\n    output_payloads = self._api_client.fetch_responses(\n        input_payloads, validation_func\n    )\n\n    results = []\n    for res in output_payloads:\n        idx = res.metadata[\"index\"]\n        output = {\n            \"score_factual_accuracy\": None,\n            \"explanation_factual_accuracy\": None,\n        }\n        try:\n            judgements = [\n                x[\"Judgement\"]\n                for x in json.loads(res.response.choices[0].message.content)\n            ]\n            score = np.mean([self.score_mapping[x.lower()] for x in judgements])\n            output[\"score_factual_accuracy\"] = float(score)\n            output[\"explanation_factual_accuracy\"] = res.response.choices[\n                0\n            ].message.content\n        except Exception:\n            logger.error(\n                f\"Error when processing payload at index {idx}: {res.error}\"\n            )\n        results.append((idx, output))\n    results = [val for _, val in sorted(results, key=lambda x: x[0])]\n\n    return results\n</code></pre>"},{"location":"operators/language/ResponseRelevance/","title":"ResponseRelevance","text":"<p>             Bases: <code>ColumnOp</code></p> <p>Grades if the response is relevant or not.</p> <p>Attributes:</p> Name Type Description <code>col_question</code> <code>str</code> <p>(str) Column Name for the stored questions</p> <code>col_response</code> <code>str</code> <p>(str) Coloumn name for stored response</p> <code>col_out</code> <code>str</code> <p>(str) Column name for the output score</p> <code>scenario_description</code> <code>str</code> <p>Optional scenario description to incorporate in the evaluation prompt</p> <p>Raises:</p> Type Description <code>Exception</code> <p>Raises exception for any failed evaluation attempts</p> Source code in <code>uptrain/operators/language/response_quality.py</code> <pre><code>@register_op\nclass ResponseRelevance(ColumnOp):\n    \"\"\"\n    Grades if the response is relevant or not.\n\n    Attributes:\n        col_question: (str) Column Name for the stored questions\n        col_response: (str) Coloumn name for stored response\n        col_out: (str) Column name for the output score\n        scenario_description (str): Optional scenario description to incorporate in the evaluation prompt\n\n    Raises:\n        Exception: Raises exception for any failed evaluation attempts\n\n    \"\"\"\n\n    col_question: str = \"question\"\n    col_response: str = \"response\"\n    col_out: str = \"score_response_relevance\"\n    scenario_description: t.Optional[str] = None\n\n    def setup(self, settings: t.Optional[Settings] = None):\n        from uptrain.framework.remote import APIClient\n\n        assert settings is not None\n        self.settings = settings\n        if self.settings.evaluate_locally:\n            self._api_client = LLMMulticlient(settings)\n        else:\n            self._api_client = APIClient(settings)\n        return self\n\n    def run(self, data: pl.DataFrame) -&gt; TYPE_TABLE_OUTPUT:\n        data_send = polars_to_json_serializable_dict(data)\n        for row in data_send:\n            row[\"response\"] = row.pop(self.col_response)\n\n        try:\n            if self.settings.evaluate_locally:\n                results = self.evaluate_local(data_send)\n            else:\n                results = self._api_client.evaluate(\n                    \"response_relevance\",\n                    data_send,\n                    {\"scenario_description\": self.scenario_description},\n                )\n        except Exception as e:\n            logger.error(f\"Failed to run evaluation for `ResponseRelevance`: {e}\")\n            raise e\n\n        assert results is not None\n        return {\n            \"output\": data.with_columns(\n                pl.from_dicts(results).rename(\n                    {\"score_response_relevance\": self.col_out}\n                )\n            )\n        }\n\n    def response_relevance_classify_validate_func(self, llm_output):\n        pass\n\n    def response_relevance_cot_validate_func(self, llm_output):\n        pass\n\n    def evaluate_local(self, data):\n        \"\"\"\n        Our methodology is based on the model grade evaluation introduced by openai evals.\n        \"\"\"\n\n        # Works by calling evaluate_local on ResponseCompleteness and ResponseConciseness and taking the f1 score of the two\n        response_completeness = ResponseCompleteness(\n            col_question=self.col_question,\n            col_response=self.col_response,\n            scenario_description=self.scenario_description,\n        )\n        output_completeness = (\n            response_completeness.setup(settings=self.settings)\n            .run(pl.DataFrame(data))[\"output\"]\n            .to_dicts()\n        )\n\n        response_conciseness = ResponseConciseness(\n            col_response=self.col_response,\n            scenario_description=self.scenario_description,\n        )\n\n        output_conciseness = (\n            response_conciseness.setup(settings=self.settings)\n            .run(pl.DataFrame(data))[\"output\"]\n            .to_dicts()\n        )\n\n        results = []\n        for combined_row in zip(output_conciseness, output_completeness):\n            precision = combined_row[0][\"score_response_conciseness\"]\n            recall = combined_row[1][\"score_response_completeness\"]\n            output = {\n                \"score_response_relevance\": None,\n                \"explanation_response_relevance\": None,\n            }\n            if (\n                precision is not None\n                and recall is not None\n            ):\n                explanation = (\n                    \"Response Precision: \" + str(precision)\n                    + str(combined_row[0][\"explanation_response_conciseness\"])\n                    + \"\\n\"\n                    + \"Response Recall: \" + str(recall)\n                    + str(combined_row[1][\"explanation_response_completeness\"])\n                )\n                output[\"explanation_response_relevance\"] = explanation\n\n                if (\n                    precision != 0\n                    and recall != 0\n                ):\n                    output[\"score_response_relevance\"] = 2 * (\n                        (precision * recall) / (precision + recall)\n                    )\n                else:\n                    output['score_response_relevance'] = 0\n            results.append(output)\n        return results\n</code></pre>"},{"location":"operators/language/ResponseRelevance/#uptrain.operators.ResponseRelevance.evaluate_local","title":"<code>evaluate_local(data)</code>","text":"<p>Our methodology is based on the model grade evaluation introduced by openai evals.</p> Source code in <code>uptrain/operators/language/response_quality.py</code> <pre><code>def evaluate_local(self, data):\n    \"\"\"\n    Our methodology is based on the model grade evaluation introduced by openai evals.\n    \"\"\"\n\n    # Works by calling evaluate_local on ResponseCompleteness and ResponseConciseness and taking the f1 score of the two\n    response_completeness = ResponseCompleteness(\n        col_question=self.col_question,\n        col_response=self.col_response,\n        scenario_description=self.scenario_description,\n    )\n    output_completeness = (\n        response_completeness.setup(settings=self.settings)\n        .run(pl.DataFrame(data))[\"output\"]\n        .to_dicts()\n    )\n\n    response_conciseness = ResponseConciseness(\n        col_response=self.col_response,\n        scenario_description=self.scenario_description,\n    )\n\n    output_conciseness = (\n        response_conciseness.setup(settings=self.settings)\n        .run(pl.DataFrame(data))[\"output\"]\n        .to_dicts()\n    )\n\n    results = []\n    for combined_row in zip(output_conciseness, output_completeness):\n        precision = combined_row[0][\"score_response_conciseness\"]\n        recall = combined_row[1][\"score_response_completeness\"]\n        output = {\n            \"score_response_relevance\": None,\n            \"explanation_response_relevance\": None,\n        }\n        if (\n            precision is not None\n            and recall is not None\n        ):\n            explanation = (\n                \"Response Precision: \" + str(precision)\n                + str(combined_row[0][\"explanation_response_conciseness\"])\n                + \"\\n\"\n                + \"Response Recall: \" + str(recall)\n                + str(combined_row[1][\"explanation_response_completeness\"])\n            )\n            output[\"explanation_response_relevance\"] = explanation\n\n            if (\n                precision != 0\n                and recall != 0\n            ):\n                output[\"score_response_relevance\"] = 2 * (\n                    (precision * recall) / (precision + recall)\n                )\n            else:\n                output['score_response_relevance'] = 0\n        results.append(output)\n    return results\n</code></pre>"},{"location":"operators/language/RougeScore/","title":"RougeScore","text":"<p>             Bases: <code>ColumnOp</code></p> <p>Operator to compare a generated text with a source text using the Rouge score metric.</p> <p>Attributes:</p> Name Type Description <code>score_type</code> <code>Literal['precision', 'recall', 'f1']</code> <p>The type of Rouge score to calculate.</p> <code>col_in_generated</code> <code>str</code> <p>The name of the input column containing the generated text.</p> <code>col_in_source</code> <code>str</code> <p>The name of the input column containing the source text.</p> <code>col_out</code> <code>str</code> <p>The name of the output column containing the Rouge scores.</p> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary containing the Rouge scores for each pair of generated and source text.</p> Example <pre><code>import polars as pl\nfrom uptrain.operators import RougeScore\n\n# Create a DataFrame\ndf = pl.DataFrame({\n    \"text_generated\": [\"This is the generated text.\", \"Another generated sentence.\"],\n    \"text_source\": [\"This is the original source text.\", \"This is a different source text.\"]\n})\n\n# Create an instance of the RougeScore class\nrouge_op = RougeScore(score_type=\"f1\")\n\n# Calculate the Rouge-L scores\nscores = rouge_op.run(df)[\"output\"]\n\n# Print the Rouge-L scores\nprint(scores)\n</code></pre> Output <pre><code>shape: (2,)\nSeries: '_col_0' [i64]\n[\n        72\n        0\n]\n</code></pre> Source code in <code>uptrain/operators/language/rouge.py</code> <pre><code>@register_op\nclass RougeScore(ColumnOp):\n    \"\"\"\n    Operator to compare a generated text with a source text using the Rouge score metric.\n\n    Attributes:\n        score_type (Literal[\"precision\", \"recall\", \"f1\"]): The type of Rouge score to calculate.\n        col_in_generated (str): The name of the input column containing the generated text.\n        col_in_source (str): The name of the input column containing the source text.\n        col_out (str): The name of the output column containing the Rouge scores.\n\n    Returns:\n        dict: A dictionary containing the Rouge scores for each pair of generated and source text.\n\n    Example:\n        ```\n        import polars as pl\n        from uptrain.operators import RougeScore\n\n        # Create a DataFrame\n        df = pl.DataFrame({\n            \"text_generated\": [\"This is the generated text.\", \"Another generated sentence.\"],\n            \"text_source\": [\"This is the original source text.\", \"This is a different source text.\"]\n        })\n\n        # Create an instance of the RougeScore class\n        rouge_op = RougeScore(score_type=\"f1\")\n\n        # Calculate the Rouge-L scores\n        scores = rouge_op.run(df)[\"output\"]\n\n        # Print the Rouge-L scores\n        print(scores)\n        ```\n\n    Output:\n        ```\n        shape: (2,)\n        Series: '_col_0' [i64]\n        [\n                72\n                0\n        ]\n        ```\n\n    \"\"\"\n\n    score_type: t.Literal[\"precision\", \"recall\", \"f1\"]\n    col_in_generated: str = \"text_generated\"\n    col_in_source: str = \"text_source\"\n    col_out: str = \"rouge_score\"\n\n    def setup(self, settings: Settings):\n        return self\n\n    def run(self, data: pl.DataFrame) -&gt; TYPE_TABLE_OUTPUT:\n        text_generated = data.get_column(self.col_in_generated)\n        text_source = data.get_column(self.col_in_source)\n\n        results = []\n        scores = []\n        for i in range(len(text_generated)):\n            scorer = rouge_scorer.RougeScorer([\"rougeL\"])  # type: ignore\n            if text_source[i] is None or text_generated[i] is None:\n                scores.append({\"rougeL\": (0, 0, 0)})\n            else:\n                scores.append(scorer.score(text_source[i], text_generated[i]))\n\n        type_to_index = {\"precision\": 0, \"recall\": 1, \"f1\": 2}\n        if self.score_type not in type_to_index:\n            raise Exception(f\"{self.score_type} not implemented\")\n        else:\n            score_index = type_to_index[self.score_type]\n\n        results = pl.Series([int(x[\"rougeL\"][score_index] * 100) for x in scores])\n        return {\"output\": data.with_columns([results.alias(self.col_out)])}\n</code></pre>"},{"location":"operators/language/TextComparison/","title":"TextComparison","text":"<p>             Bases: <code>ColumnOp</code></p> <p>Operator to compare each text entry in a column with a list of reference texts.</p> <p>Attributes:</p> Name Type Description <code>reference_texts</code> <code>Union[list[str], str]</code> <p>List of reference text for comparison.</p> <code>col_in_text</code> <code>str</code> <p>The name of the input column containing the text data.</p> <code>col_out</code> <code>str</code> <p>The name of the output column containing the comparison results.</p> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary containing the comparison results (1 if equal, 0 otherwise).</p> Example <pre><code>import polars as pl\nfrom uptrain.operators import TextComparison\n\n# Create a DataFrame\ndf = pl.DataFrame({\n    \"text\": [\"This is a sample text.\", \"Another example sentence.\", \"Yet another sentence.\"]\n})\n\n# Set the reference text for comparison\nref_text = [\"This is a sample text.\", \"Yet another sentence.\"]\n\n# Create an instance of the TextComparison class\ncomp_op = TextComparison(reference_texts=ref_text, col_in_text=\"text\")\n\n# Compare each text entry with the reference text\ncomparison = comp_op.run(df)[\"output\"]\n\n# Print the comparison results\nprint(comparison)\n</code></pre> Output <pre><code>shape: (3,)\nSeries: '_col_0' [i64]\n[\n        1\n        0\n        1\n]\n</code></pre> Source code in <code>uptrain/operators/language/text.py</code> <pre><code>@register_op\nclass TextComparison(ColumnOp):\n    \"\"\"\n    Operator to compare each text entry in a column with a list of reference texts.\n\n    Attributes:\n        reference_texts (Union[list[str], str]): List of reference text for comparison.\n        col_in_text (str): The name of the input column containing the text data.\n        col_out (str): The name of the output column containing the comparison results.\n\n    Returns:\n        dict: A dictionary containing the comparison results (1 if equal, 0 otherwise).\n\n    Example:\n        ```\n        import polars as pl\n        from uptrain.operators import TextComparison\n\n        # Create a DataFrame\n        df = pl.DataFrame({\n            \"text\": [\"This is a sample text.\", \"Another example sentence.\", \"Yet another sentence.\"]\n        })\n\n        # Set the reference text for comparison\n        ref_text = [\"This is a sample text.\", \"Yet another sentence.\"]\n\n        # Create an instance of the TextComparison class\n        comp_op = TextComparison(reference_texts=ref_text, col_in_text=\"text\")\n\n        # Compare each text entry with the reference text\n        comparison = comp_op.run(df)[\"output\"]\n\n        # Print the comparison results\n        print(comparison)\n        ```\n\n    Output:\n        ```\n        shape: (3,)\n        Series: '_col_0' [i64]\n        [\n                1\n                0\n                1\n        ]\n        ```\n\n    \"\"\"\n\n    reference_texts: t.Union[list[str], str]\n    col_in_text: str\n    col_out: str = \"text_comparison\"\n\n    def setup(self, settings: Settings):\n        if isinstance(self.reference_texts, str):\n            self.reference_texts = [self.reference_texts]\n        return self\n\n    def run(self, data: pl.DataFrame) -&gt; TYPE_TABLE_OUTPUT:\n        results = data.get_column(self.col_in_text).apply(\n            lambda x: int(sum([x == y for y in self.reference_texts]))\n        )\n        return {\"output\": data.with_columns([results.alias(self.col_out)])}\n</code></pre>"},{"location":"operators/language/TextCompletion/","title":"TextCompletion","text":"<p>             Bases: <code>TransformOp</code></p> <p>Takes a table of prompts and LLM model to use, generates output text.</p> <p>Attributes:</p> Name Type Description <code>col_in_prompt</code> <code>str</code> <p>The name of the column containing the prompt template.</p> <code>col_in_model</code> <code>str</code> <p>The name of the column containing the model name.</p> <code>col_out_completion</code> <code>str</code> <p>The name of the column containing the generated text.</p> <code>temperature</code> <code>float</code> <p>Temperature for the LLM to generate responses.</p> <p>Returns:</p> Name Type Description <code>TYPE_TABLE_OUTPUT</code> <p>A dictionary containing the dataset with the output text.</p> Source code in <code>uptrain/operators/language/generation.py</code> <pre><code>@register_op\nclass TextCompletion(TransformOp):\n    \"\"\"\n    Takes a table of prompts and LLM model to use, generates output text.\n\n    Attributes:\n        col_in_prompt (str): The name of the column containing the prompt template.\n        col_in_model (str): The name of the column containing the model name.\n        col_out_completion (str): The name of the column containing the generated text.\n        temperature (float): Temperature for the LLM to generate responses.\n\n    Returns:\n        TYPE_TABLE_OUTPUT: A dictionary containing the dataset with the output text.\n    \"\"\"\n\n    col_in_prompt: str = \"prompt\"\n    col_in_model: str = \"model\"\n    col_out_completion: str = \"generated\"\n    temperature: float = 1.0\n    _api_client: LLMMulticlient\n\n    def setup(self, settings: Settings):\n        self._api_client = LLMMulticlient(settings=settings)\n        self._settings = settings\n        return self\n\n    def _make_payload(self, id: t.Any, text: str, model: str) -&gt; Payload:\n        if self._settings.seed is not None:\n            return Payload(\n                data={\n                    \"model\": model,\n                    \"messages\": [{\"role\": \"user\", \"content\": text}],\n                    \"temperature\": self.temperature,\n                    \"seed\": self._settings.seed,\n                },\n                metadata={\"index\": id},\n            )\n        else:\n            return Payload(\n                data={\n                    \"model\": model,\n                    \"messages\": [{\"role\": \"user\", \"content\": text}],\n                    \"temperature\": self.temperature,\n                },\n                metadata={\"index\": id},\n            )\n\n    def run(self, data: pl.DataFrame) -&gt; TYPE_TABLE_OUTPUT:\n        prompt_ser = data.get_column(self.col_in_prompt)\n        model_ser = data.get_column(self.col_in_model)\n        input_payloads = [\n            self._make_payload(idx, text, model)\n            for idx, (text, model) in enumerate(zip(prompt_ser, model_ser))\n        ]\n        output_payloads = self._api_client.fetch_responses(input_payloads)\n\n        results = []\n        for res in output_payloads:\n            assert (\n                res is not None\n            ), \"Response should not be None, we should've handled exceptions beforehand.\"\n            idx = res.metadata[\"index\"]\n            if res.error is not None:\n                logger.error(\n                    f\"Error when processing payload at index {idx}: {res.error}\"\n                )\n                results.append((idx, None))\n            else:\n                resp_text = res.response.choices[0].message.content\n                results.append((idx, resp_text))\n\n        output_text = pl.Series(\n            values=[val for _, val in sorted(results, key=lambda x: x[0])]\n        )\n        return {\n            \"output\": data.with_columns([output_text.alias(self.col_out_completion)])\n        }\n</code></pre>"},{"location":"operators/language/TextLength/","title":"TextLength","text":"<p>             Bases: <code>ColumnOp</code></p> <p>Operator to calculate the length of each text entry in a column.</p> <p>Attributes:</p> Name Type Description <code>col_in_text</code> <code>str</code> <p>The name of the input column containing the text data.</p> <code>col_out</code> <code>str</code> <p>The name of the output column containing the text lengths.</p> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary containing the calculated text lengths.</p> Example <pre><code>import polars as pl\nfrom uptrain.operators import TextLength\n\n# Create a DataFrame\ndf = pl.DataFrame({\n    \"text\": [\"This is a sample text.\", \"Another example sentence.\", \"Yet another sentence.\"]\n})\n\n# Create an instance of the TextLength class\nlength_op = TextLength(col_in_text=\"text\")\n\n# Calculate the length of each text entry\nlengths = length_op.run(df)[\"output\"]\n\n# Print the text lengths\nprint(lengths)\n</code></pre> Output <pre><code>shape: (3,)\nSeries: '_col_0' [i64]\n[\n        22\n        25\n        21\n]\n</code></pre> Source code in <code>uptrain/operators/language/text.py</code> <pre><code>@register_op\nclass TextLength(ColumnOp):\n    \"\"\"\n    Operator to calculate the length of each text entry in a column.\n\n    Attributes:\n        col_in_text (str): The name of the input column containing the text data.\n        col_out (str): The name of the output column containing the text lengths.\n\n    Returns:\n        dict: A dictionary containing the calculated text lengths.\n\n    Example:\n        ```\n        import polars as pl\n        from uptrain.operators import TextLength\n\n        # Create a DataFrame\n        df = pl.DataFrame({\n            \"text\": [\"This is a sample text.\", \"Another example sentence.\", \"Yet another sentence.\"]\n        })\n\n        # Create an instance of the TextLength class\n        length_op = TextLength(col_in_text=\"text\")\n\n        # Calculate the length of each text entry\n        lengths = length_op.run(df)[\"output\"]\n\n        # Print the text lengths\n        print(lengths)\n        ```\n\n    Output:\n        ```\n        shape: (3,)\n        Series: '_col_0' [i64]\n        [\n                22\n                25\n                21\n        ]\n        ```\n\n    \"\"\"\n\n    col_in_text: str\n    col_out: str = \"text_length\"\n\n    def setup(self, settings: Settings):\n        return self\n\n    def run(self, data: pl.DataFrame) -&gt; TYPE_TABLE_OUTPUT:\n        results = data.get_column(self.col_in_text).apply(len)\n        return {\"output\": data.with_columns([results.alias(self.col_out)])}\n</code></pre>"},{"location":"operators/language/ToneCritique/","title":"ToneCritique","text":"<p>             Bases: <code>ColumnOp</code></p> <p>Operator to assess the tone of machine generated responses.</p> <p>Attributes:</p> Name Type Description <code>llm_persona</code> <code>str</code> <p>The persona the chatbot being assessed was expected to follow</p> <code>col_response</code> <code>str</code> <p>The name of the input column containing response text</p> <code>col_out</code> <code>str</code> <p>The name of the output column containing the scores</p> <code>scenario_description</code> <code>str | None</code> <p>Optional scenario description to incorporate in the evaluation prompt</p> <code>score_mapping</code> <code>dict</code> <p>Mapping of different grades to float scores</p> <p>Raises:</p> Type Description <code>Exception</code> <p>Raises exception for any failed evaluation attempts</p> Source code in <code>uptrain/operators/language/tone.py</code> <pre><code>@register_op\nclass ToneCritique(ColumnOp):\n    \"\"\"\n    Operator to assess the tone of machine generated responses.\n\n    Attributes:\n        llm_persona (str): The persona the chatbot being assessed was expected to follow\n        col_response (str): The name of the input column containing response text\n        col_out (str): The name of the output column containing the scores\n        scenario_description (str | None): Optional scenario description to incorporate in the evaluation prompt\n        score_mapping (dict): Mapping of different grades to float scores\n\n    Raises:\n        Exception: Raises exception for any failed evaluation attempts\n\n    \"\"\"\n\n    llm_persona: str = \"helpful-chatbot\"\n    col_response: str = \"response\"\n    col_out: str = \"score_critique_tone\"\n    scenario_description: t.Optional[str] = None\n    score_mapping: dict = {\"A\": 1.0, \"B\": 0.5, \"C\": 0.0}\n\n    def setup(self, settings: t.Optional[Settings] = None):\n        from uptrain.framework.remote import APIClient\n\n        assert settings is not None\n        self.settings = settings\n        if self.settings.evaluate_locally:\n            self._api_client = LLMMulticlient(settings)\n        else:\n            self._api_client = APIClient(settings)\n        return self\n\n    def run(self, data: pl.DataFrame) -&gt; TYPE_TABLE_OUTPUT:\n        data_send = polars_to_json_serializable_dict(data)\n        for row in data_send:\n            row[\"response\"] = row.pop(self.col_response)\n\n        try:\n            if self.settings.evaluate_locally:\n                results = self.evaluate_local(data_send)\n            else:\n                results = self._api_client.evaluate(\n                    \"critique_tone\", data_send, {\"llm_persona\": self.llm_persona}\n                )\n        except Exception as e:\n            logger.error(f\"Failed to run evaluation for `ToneCritique`: {e}\")\n            raise e\n\n        assert results is not None\n        return {\n            \"output\": data.with_columns(\n                pl.from_dicts(results).rename({\"score_critique_tone\": self.col_out})\n            )\n        }\n\n    def critique_tone_classify_validate_func(self, llm_output):\n        is_correct = True\n        is_correct = is_correct and (\"Choice\" in json.loads(llm_output))\n        is_correct = is_correct and json.loads(llm_output)[\"Choice\"] in [\"A\", \"B\", \"C\"]\n        return is_correct\n\n    def critique_tone_cot_validate_func(self, llm_output):\n        is_correct = self.critique_tone_classify_validate_func(llm_output)\n        is_correct = is_correct and (\"Reasoning\" in json.loads(llm_output))\n        return is_correct\n\n    def evaluate_local(self, data):\n        \"\"\"\n        Our methodology is based on the model grade evaluation introduced by openai evals.\n        \"\"\"\n\n        self.scenario_description, scenario_vars = parse_scenario_description(\n            self.scenario_description\n        )\n        input_payloads = []\n        if self.settings.eval_type == \"basic\":\n            few_shot_examples = CRITIQUE_TONE_FEW_SHOT__CLASSIFY\n            output_format = CRITIQUE_TONE_OUTPUT_FORMAT__CLASSIFY\n            validation_func = self.critique_tone_classify_validate_func\n            prompting_instructions = CLASSIFY\n        elif self.settings.eval_type == \"cot\":\n            few_shot_examples = CRITIQUE_TONE_FEW_SHOT__COT\n            output_format = CRITIQUE_TONE_OUTPUT_FORMAT__COT\n            validation_func = self.critique_tone_cot_validate_func\n            prompting_instructions = CHAIN_OF_THOUGHT\n        else:\n            raise ValueError(\n                f\"Invalid eval_type: {self.settings.eval_type}. Must be either 'basic' or 'cot'\"\n            )\n\n        for idx, row in enumerate(data):\n            kwargs = row\n            kwargs.update(\n                {\n                    \"output_format\": output_format,\n                    \"prompting_instructions\": prompting_instructions,\n                    \"few_shot_examples\": few_shot_examples,\n                    \"llm_persona\": self.llm_persona,\n                }\n            )\n            try:\n                grading_prompt_template = CRITIQUE_TONE_PROMPT_TEMPLATE.replace(\n                    \"{scenario_description}\", self.scenario_description\n                ).format(**kwargs)\n            except KeyError as e:\n                raise KeyError(\n                    f\"Missing required attribute(s) for scenario description: {e}\"\n                )\n            input_payloads.append(\n                self._api_client.make_payload(idx, grading_prompt_template)\n            )\n        output_payloads = self._api_client.fetch_responses(\n            input_payloads, validation_func\n        )\n\n        results = []\n\n        for res in output_payloads:\n            idx = res.metadata[\"index\"]\n            output = {\"score_critique_tone\": None, \"explanation_critique_tone\": None}\n            try:\n                score = self.score_mapping[\n                    json.loads(res.response.choices[0].message.content)[\"Choice\"]\n                ]\n                output[\"score_critique_tone\"] = float(score)\n                output[\"explanation_critique_tone\"] = res.response.choices[\n                    0\n                ].message.content\n            except Exception:\n                logger.error(\n                    f\"Error when processing payload at index {idx}: {res.error}\"\n                )\n            results.append((idx, output))\n\n        results = [val for _, val in sorted(results, key=lambda x: x[0])]\n        return results\n</code></pre>"},{"location":"operators/language/ToneCritique/#uptrain.operators.ToneCritique.evaluate_local","title":"<code>evaluate_local(data)</code>","text":"<p>Our methodology is based on the model grade evaluation introduced by openai evals.</p> Source code in <code>uptrain/operators/language/tone.py</code> <pre><code>def evaluate_local(self, data):\n    \"\"\"\n    Our methodology is based on the model grade evaluation introduced by openai evals.\n    \"\"\"\n\n    self.scenario_description, scenario_vars = parse_scenario_description(\n        self.scenario_description\n    )\n    input_payloads = []\n    if self.settings.eval_type == \"basic\":\n        few_shot_examples = CRITIQUE_TONE_FEW_SHOT__CLASSIFY\n        output_format = CRITIQUE_TONE_OUTPUT_FORMAT__CLASSIFY\n        validation_func = self.critique_tone_classify_validate_func\n        prompting_instructions = CLASSIFY\n    elif self.settings.eval_type == \"cot\":\n        few_shot_examples = CRITIQUE_TONE_FEW_SHOT__COT\n        output_format = CRITIQUE_TONE_OUTPUT_FORMAT__COT\n        validation_func = self.critique_tone_cot_validate_func\n        prompting_instructions = CHAIN_OF_THOUGHT\n    else:\n        raise ValueError(\n            f\"Invalid eval_type: {self.settings.eval_type}. Must be either 'basic' or 'cot'\"\n        )\n\n    for idx, row in enumerate(data):\n        kwargs = row\n        kwargs.update(\n            {\n                \"output_format\": output_format,\n                \"prompting_instructions\": prompting_instructions,\n                \"few_shot_examples\": few_shot_examples,\n                \"llm_persona\": self.llm_persona,\n            }\n        )\n        try:\n            grading_prompt_template = CRITIQUE_TONE_PROMPT_TEMPLATE.replace(\n                \"{scenario_description}\", self.scenario_description\n            ).format(**kwargs)\n        except KeyError as e:\n            raise KeyError(\n                f\"Missing required attribute(s) for scenario description: {e}\"\n            )\n        input_payloads.append(\n            self._api_client.make_payload(idx, grading_prompt_template)\n        )\n    output_payloads = self._api_client.fetch_responses(\n        input_payloads, validation_func\n    )\n\n    results = []\n\n    for res in output_payloads:\n        idx = res.metadata[\"index\"]\n        output = {\"score_critique_tone\": None, \"explanation_critique_tone\": None}\n        try:\n            score = self.score_mapping[\n                json.loads(res.response.choices[0].message.content)[\"Choice\"]\n            ]\n            output[\"score_critique_tone\"] = float(score)\n            output[\"explanation_critique_tone\"] = res.response.choices[\n                0\n            ].message.content\n        except Exception:\n            logger.error(\n                f\"Error when processing payload at index {idx}: {res.error}\"\n            )\n        results.append((idx, output))\n\n    results = [val for _, val in sorted(results, key=lambda x: x[0])]\n    return results\n</code></pre>"},{"location":"operators/language/WordCount/","title":"WordCount","text":"<p>             Bases: <code>ColumnOp</code></p> <p>Operator to calculate the number of words in each text entry in a column.</p> <p>Attributes:</p> Name Type Description <code>col_in_text</code> <code>str</code> <p>The name of the input column containing the text data.</p> <code>col_out</code> <code>str</code> <p>The name of the output column containing the number of words.</p> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary containing the calculated number of words.</p> Example <pre><code>import polars as pl\nfrom uptrain.operators import WordCount\n\n# Create a DataFrame\ndf = pl.DataFrame({\n    \"text\": [\"This is a sample text.\", \"Another example sentence.\", \"Yet another sentence.\"]\n})\n\n# Create an instance of the WordCount class\nword_count_op = WordCount(col_in_text=\"text\")\n\n# Calculate the number of words in each text entry\nword_counts = word_count_op.run(df)[\"output\"]\n\n# Print the word counts\nprint(word_counts)\n</code></pre> Output <pre><code>shape: (3,)\nSeries: '_col_0' [i64]\n[\n        5\n        3\n        3\n]\n</code></pre> Source code in <code>uptrain/operators/language/text.py</code> <pre><code>@register_op\nclass WordCount(ColumnOp):\n    \"\"\"\n    Operator to calculate the number of words in each text entry in a column.\n\n    Attributes:\n        col_in_text (str): The name of the input column containing the text data.\n        col_out (str): The name of the output column containing the number of words.\n\n    Returns:\n        dict: A dictionary containing the calculated number of words.\n\n    Example:\n        ```\n        import polars as pl\n        from uptrain.operators import WordCount\n\n        # Create a DataFrame\n        df = pl.DataFrame({\n            \"text\": [\"This is a sample text.\", \"Another example sentence.\", \"Yet another sentence.\"]\n        })\n\n        # Create an instance of the WordCount class\n        word_count_op = WordCount(col_in_text=\"text\")\n\n        # Calculate the number of words in each text entry\n        word_counts = word_count_op.run(df)[\"output\"]\n\n        # Print the word counts\n        print(word_counts)\n        ```\n\n    Output:\n        ```\n        shape: (3,)\n        Series: '_col_0' [i64]\n        [\n                5\n                3\n                3\n        ]\n        ```\n\n    \"\"\"\n\n    col_in_text: str\n    col_out: str = \"word_count\"\n\n    def setup(self, settings: Settings):\n        return self\n\n    def run(self, data: pl.DataFrame) -&gt; TYPE_TABLE_OUTPUT:\n        results = data.get_column(self.col_in_text).apply(lambda x: len(x.split(\" \")))\n        return {\"output\": data.with_columns([results.alias(self.col_out)])}\n</code></pre>"}]}